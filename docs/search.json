[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "example_posts/new-test-post/index.html",
    "href": "example_posts/new-test-post/index.html",
    "title": "Second Post",
    "section": "",
    "text": "from source import Perceptron\np = Perceptron()\n\nI did it!!\nnot implemented\nThis is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "example_posts/new-test-post/index.html#math",
    "href": "example_posts/new-test-post/index.html#math",
    "title": "Second Post",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "Penguin Blog\n\n\n\n\n\n\ncsci415\n\n\n\nPenguin classifier\n\n\n\n\n\nFeb 16, 2024\n\n\nManuel Fors\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "example_posts/example-blog-post/index.html",
    "href": "example_posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "from source import Perceptron\np = Perceptron()\n\nI did it!!\nnot implemented\nThis is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "example_posts/example-blog-post/index.html#math",
    "href": "example_posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/penguins-blog-post/index.html",
    "href": "posts/penguins-blog-post/index.html",
    "title": "Penguin Blog",
    "section": "",
    "text": "Adelie Penguin ready to engage in his final semester"
  },
  {
    "objectID": "posts/penguins-blog-post/index.html#loading-in-data",
    "href": "posts/penguins-blog-post/index.html#loading-in-data",
    "title": "Penguin Blog",
    "section": "Loading in Data",
    "text": "Loading in Data\nWe are going to load in our training data for this project from a github url\n\nimport pandas as pd \ntrain_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\n\nNow we are going to take a look at our data and look for some potentially interesting features. My goal hear is to do some exploration through graphs and a bit of intuition to see if there are any key features that stick out to me.\n\ntrain.head()\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0809\n31\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN63A1\nYes\n11/24/08\n40.9\n16.6\n187.0\n3200.0\nFEMALE\n9.08458\n-24.54903\nNaN\n\n\n1\nPAL0809\n41\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN74A1\nYes\n11/24/08\n49.0\n19.5\n210.0\n3950.0\nMALE\n9.53262\n-24.66867\nNaN\n\n\n2\nPAL0708\n4\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN32A2\nYes\n11/27/07\n50.0\n15.2\n218.0\n5700.0\nMALE\n8.25540\n-25.40075\nNaN\n\n\n3\nPAL0708\n15\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN38A1\nYes\n12/3/07\n45.8\n14.6\n210.0\n4200.0\nFEMALE\n7.79958\n-25.62618\nNaN\n\n\n4\nPAL0809\n34\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN65A2\nYes\n11/24/08\n51.0\n18.8\n203.0\n4100.0\nMALE\n9.23196\n-24.17282\nNaN\n\n\n\n\n\n\n\nFollowing this, I made a rather arbitrary guess to take a look at the culmen depth and culmen length. I combined this with a hue to see the species as well as whether clutches were completed just to have a large range of variables potentially explored by this graph.\n\nfrom matplotlib import pyplot as plt \nimport seaborn as sns \nsns.set_theme(palette=\"bright\")\n\n#sns.scatterplot(data=train, x=\"Flipper Length (mm)\", y=\"Body Mass (g)\", hue=\"Culmen Depth (mm)\", style=\"Clutch Completion\")\n\nsns.scatterplot(data=train, x=\"Culmen Depth (mm)\", y=\"Culmen Length (mm)\", hue=\"Species\", style=\"Clutch Completion\").set_title(\"Culmen Length and Culmen Depth of Chinstrap, Gentoo, and Adelie Penguins\")\n\nText(0.5, 1.0, 'Culmen Length and Culmen Depth of Chinstrap, Gentoo, and Adelie Penguins')\n\n\n\n\n\n\n\n\n\nFollowing this, I created a graph that showed the variation in Delta 13 temperature across the three different islands. I had an intuition that Islands might be used in the key features for my function and thuse wanted to explore this graphically.\n\nsns.displot(train, x=\"Delta 13 C (o/oo)\", col=\"Island\", binwidth=.5, height=3)\n\n/home/manny/anaconda3/envs/ml-0451/lib/python3.11/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n\n\n\n\n\n\n\n\n\n\nbeautiful_table = train.groupby([\"Species\", \"Island\"]).aggregate({\"Body Mass (g)\" : ('mean') ,\"Culmen Depth (mm)\" : ('mean'), \"Flipper Length (mm)\": ('mean'), \"Delta 15 N (o/oo)\" :('mean')})\nbeautiful_table\n\n\n\n\n\n\n\n\n\nBody Mass (g)\nCulmen Depth (mm)\nFlipper Length (mm)\nDelta 15 N (o/oo)\n\n\nSpecies\nIsland\n\n\n\n\n\n\n\n\nAdelie Penguin (Pygoscelis adeliae)\nBiscoe\n3711.363636\n18.475758\n188.636364\n8.788643\n\n\nDream\n3728.888889\n18.306667\n190.133333\n8.933945\n\n\nTorgersen\n3712.804878\n18.468293\n191.195122\n8.846768\n\n\nChinstrap penguin (Pygoscelis antarctica)\nDream\n3743.421053\n18.366667\n196.000000\n9.331004\n\n\nGentoo penguin (Pygoscelis papua)\nBiscoe\n5039.948454\n14.914433\n216.752577\n8.247341\n\n\n\n\n\n\n\nAfter exploration, I can see that average Flipper Length and Delta 15 N differentiate between the species. I can also see how both the Chinstrap and Gentoo penguin only appear on one island each. While I believe these features might be key features, I am going to take a brute-force approach at finding the correct features by trying every possible combination of approaches."
  },
  {
    "objectID": "posts/penguins-blog-post/index.html#organizing-data",
    "href": "posts/penguins-blog-post/index.html#organizing-data",
    "title": "Penguin Blog",
    "section": "Organizing Data",
    "text": "Organizing Data\nFollowing this graphical exploration, I need to use the LabelEncoder. The LabelEncoder is going to set increasingly large numbers starting from 0 to replace each label. For all other qualitative columns, they are are turned into 1’s and 0’s so that we can run our models on it.\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df) #here is where we set all non-integer values to integers\n  return df, y\n\nX_train, y_train = prepare_data(train)"
  },
  {
    "objectID": "posts/penguins-blog-post/index.html#lets-use-a-more-comprehensive-search-for-choosing-features",
    "href": "posts/penguins-blog-post/index.html#lets-use-a-more-comprehensive-search-for-choosing-features",
    "title": "Penguin Blog",
    "section": "Lets use a more comprehensive search for choosing features!",
    "text": "Lets use a more comprehensive search for choosing features!\nBecause our dataset is relatively smaller, we can use an exhaustive search for all the features contained in the dataset. This means we are going to look at every combination of qualitative columns with quantitative columns and then create a model on this. Following this we calculate an estimated score which we add to a dictionary of ratings.\n\nfrom itertools import combinations\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nimport warnings\nwarnings.filterwarnings('ignore')\n\nLR = LogisticRegression()\n\n# these are not actually all the columns: you'll \n# need to add any of the other ones you want to search for\nall_qual_cols = [\"Clutch Completion\", \"Sex\", \"Island\"]\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)','Body Mass (g)', 'Delta 15 N (o/oo)', 'Delta 13 C (o/oo)']\nratings = {}\nfor qual in all_qual_cols: \n  qual_cols = [col for col in X_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 2):\n    # you could train models and score them here, keeping the list of \n    cols = qual_cols + list(pair) \n    LR.fit(X_train[cols], y_train)\n    #cv_scores_LR = cross_val_score(LR, X_train, y_train, cv=3)\n\n    estimated_score = LR.score(X_train[cols], y_train) \n    #estimated_score = cv_scores_LR.mean()\n    ratings[\" \".join(cols)] = estimated_score \n\nWe now have a dictionary of items that should tell us our highest scored combination\n\ntop_result = max(ratings, key=ratings.get)\ntop_result \n\n'Sex_FEMALE Sex_MALE Culmen Length (mm) Culmen Depth (mm)'\n\n\nWe are going to move our qualitative values to the end of our list because of our decision regions function we have later on\n\n#top_result= [\"Culmen Length (mm)\", \"Culmen Depth (mm)\", \"Clutch Completion_No\", \"Clutch Completion_Yes\"]\ntop_result= [\"Culmen Length (mm)\", \"Culmen Depth (mm)\", \"Sex_FEMALE\", \"Sex_MALE\"]\n\n\nLR.fit(X_train[top_result], y_train)\nLR.score(X_train[top_result], y_train)\n\n0.99609375"
  },
  {
    "objectID": "posts/penguins-blog-post/index.html#plotting-decisions-regions",
    "href": "posts/penguins-blog-post/index.html#plotting-decisions-regions",
    "title": "Penguin Blog",
    "section": "Plotting Decisions Regions",
    "text": "Plotting Decisions Regions\nWow! Our rating is pretty high. Lets visualize what our model is doing by plotting Decisions Regions\n\nfrom matplotlib import pyplot as plt\nimport numpy as np\n\n\n\nfrom matplotlib.patches import Patch\n\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (7, 3))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1], \n            title = qual_features[i])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n      plt.tight_layout()\n\n\nprint(X_train[top_result], y_train)\n\nplot_regions(LR, X_train[top_result], y_train)\n\n     Culmen Length (mm)  Culmen Depth (mm)  Sex_FEMALE  Sex_MALE\n0                  40.9               16.6        True     False\n1                  49.0               19.5       False      True\n2                  50.0               15.2       False      True\n3                  45.8               14.6        True     False\n4                  51.0               18.8       False      True\n..                  ...                ...         ...       ...\n270                51.1               16.5       False      True\n271                35.9               16.6        True     False\n272                39.5               17.8        True     False\n273                36.7               19.3        True     False\n274                42.4               17.3        True     False\n\n[256 rows x 4 columns] [1 1 2 2 1 0 0 1 2 1 0 1 0 1 1 2 0 2 2 2 2 0 0 0 2 1 0 0 0 0 0 0 1 2 0 0 2\n 2 1 1 2 2 1 0 0 2 2 1 2 2 1 2 0 0 2 2 0 1 2 2 1 2 1 2 2 2 0 0 0 2 2 2 0 1\n 2 2 2 0 0 2 0 0 2 0 0 0 1 0 0 1 0 0 0 1 0 0 2 2 0 0 2 0 2 1 0 2 2 1 2 2 2\n 0 2 0 0 0 1 0 2 2 0 2 2 1 2 0 0 1 2 2 1 0 2 0 1 2 0 0 2 0 2 1 0 0 2 1 0 2\n 0 2 0 1 0 0 0 2 2 2 0 0 2 0 2 1 1 0 1 2 0 0 1 1 0 0 1 0 0 2 1 2 1 0 0 0 1\n 0 2 2 2 2 1 1 1 2 2 2 0 1 0 0 1 0 0 0 0 0 1 2 2 2 0 2 2 1 0 2 0 0 2 0 2 0\n 2 0 2 2 2 2 0 2 1 0 2 1 1 0 2 1 0 0 0 1 0 1 0 0 2 1 0 0 0 2 0 0 0 1]"
  },
  {
    "objectID": "posts/penguins-blog-post/index.html#cross-validation",
    "href": "posts/penguins-blog-post/index.html#cross-validation",
    "title": "Penguin Blog",
    "section": "Cross Validation",
    "text": "Cross Validation\nWe want to calculate how our model will do on unseen data, and a useful way to simulate this is by performing cross-validation. We can take parts of the train data from being used, and then testing on those witheld parts, we can determine our accuracy.\n\nfrom sklearn.model_selection import cross_val_score\n\ncv_scores_LR = cross_val_score(LR, X_train, y_train, cv=5)\ncv_scores_LR.mean()\n\n1.0"
  },
  {
    "objectID": "posts/penguins-blog-post/index.html#we-did-it",
    "href": "posts/penguins-blog-post/index.html#we-did-it",
    "title": "Penguin Blog",
    "section": "We did it!",
    "text": "We did it!\n\nNext Steps\nThis would involve testing other models such as Decision Classfier Trees, and we could run the cross validation step within our brute-force attempt instead of after."
  }
]