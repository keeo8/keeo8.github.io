[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This blog is to showcase the work I do in class as well as my thought process behind coding."
  },
  {
    "objectID": "about.html#about-this-blog",
    "href": "about.html#about-this-blog",
    "title": "About",
    "section": "",
    "text": "This blog is to showcase the work I do in class as well as my thought process behind coding."
  },
  {
    "objectID": "about.html#about-me",
    "href": "about.html#about-me",
    "title": "About",
    "section": "About me",
    "text": "About me\nI am a senior Computer Science major at Middlebury College. I have experience in Python, C, OCaml, and Java, as well as the domain-specific language “NetLogo”. I also like linguistics and art! Enjoy reading my blog and thank you for stopping by!"
  },
  {
    "objectID": "posts/svm_breakdown/index.html",
    "href": "posts/svm_breakdown/index.html",
    "title": "Explaining Support Vector Machines",
    "section": "",
    "text": "As I handle less and less cash in my day to day life, I have been wondering how many fake bills I have potentially held over my lifetime. Would I have been able to tell what makes them different? Surely if this cash had the wrong president on the front or a weird font I might have been able to tell, but are there more minute distinctions between really good forgeries and normal bills? Today, I will be analyzing the various measurements of a bill: height, length, and margins. Through these variables, I am going to determine whether or not a dollar is real fake. In order to accomplish this, I will use Support Vector Machines and the dataset of fake and real money\n\n\nSupport Vector Machines (SVM) is an algorithm which can be used for both regression and classification. This approach divides points into classes using a hyperplane. This line is calculated based on how far away points would lie from the line. On both sides of the line, the distance of each point is maximized. From there, we arrive to the term “Support Vector”. Support Vectors are the closest points to the hyperplane, and their distances are maximized.\nWhen trying to separate the data into two classes, one might think that SVM will fail when there are outliers in the data. However, a built-in feature in SVM is a “slack variable”. This is the distance between where the outlier lies and where it should be. From this, SVM can safely ignore the outlier, creating its line (hyperplane).\nData must separable by a line for vanilla SVM to work. However, if it is not linearly separable, we can still do SVM. In order to do so, we would add an extra dimension to our data via some function.\n\n\n\nThat dataset has two defined classes: real and fake. This means that with supervised learning, we can train a model to make predictions using our classes as labels. I chose SVM because of its strength in classification problems as well as its ability to limit outliers. With a dataset as small as a few thousand rows, this means we avoid overfitting to some observed noise or outliers.\n\n\n\nWe are going to clean up our data, graphically explore our data, divide into a training and test split, train our SVM model, and then finally, make predictions and gauge our accuracy.\n\n\n\n\n\n\nlibrary(caret)\n\nLoading required package: ggplot2\n\n\nLoading required package: lattice\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ lubridate 1.9.3     ✔ tibble    3.2.1\n✔ purrr     1.0.2     ✔ tidyr     1.3.1\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n✖ purrr::lift()   masks caret::lift()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(e1071)\nlibrary(ggplot2)\n\nbills &lt;- read_csv2(\"databases/fake_bills.csv\")\n\nℹ Using \"','\" as decimal and \"'.'\" as grouping mark. Use `read_delim()` for more control.\nRows: 1500 Columns: 7── Column specification ────────────────────────────────────────────────────────\nDelimiter: \";\"\nnum (6): diagonal, height_left, height_right, margin_low, margin_up, length\nlgl (1): is_genuine\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nAn interesting graph emerges:\n\nbills |&gt;\n  ggplot() + \n  ggtitle(\"Relationship between Margin Heights and class of Bill\") +\n  geom_point(aes(x = margin_up, y = margin_low, color = is_genuine))\n\nWarning: Removed 37 rows containing missing values (`geom_point()`).\n\n\n\n\n\n\n\n\n\nEach of these clusters of data appears separable by a line!\n\n\n\n\nclean_bills &lt;- bills |&gt;\n                drop_na()\ntable(clean_bills$is_genuine)\n\n\nFALSE  TRUE \n  492   971 \n\n\nFor the variable we are classifying, is_genuine, I noticed that there are only 492 false bills, but 971 true bills. Thus, I select a random 500 true bills so that there is the same amount for both. I was able to find this out because of the table() function.\nI also decide to organize our data into a 80/20 training-test split.\n\n# Organize data into subsets of true and false\nreal_bills &lt;- clean_bills |&gt; \n    filter(is_genuine == TRUE) %&gt;%\n    sample_n(size = 500, replace = FALSE)\n\nfake_bills &lt;- bills |&gt;\n    filter(is_genuine == FALSE)   \n\n# Create test data \ntrain_fake_bills &lt;- fake_bills |&gt; \n    sample_n(size = 393, replace = FALSE)\n\ntrain_real_bills &lt;- real_bills |&gt; \n    sample_n(size = 393, replace = FALSE)\n\n# Create training data\ntest_real_bills &lt;- anti_join(real_bills, train_real_bills)\n\nJoining with `by = join_by(is_genuine, diagonal, height_left, height_right,\nmargin_low, margin_up, length)`\n\ntest_fake_bills &lt;- anti_join(fake_bills, train_fake_bills)\n\nJoining with `by = join_by(is_genuine, diagonal, height_left, height_right,\nmargin_low, margin_up, length)`\n\n# Combine our sets to make the full sets of each\ntrain_set &lt;- bind_rows(train_fake_bills, train_real_bills)\ntest_set &lt;- bind_rows(test_fake_bills, test_real_bills)\n\n#Drop na values \ntrain_set &lt;- train_set |&gt; \n            drop_na()\n\ntest_set &lt;- test_set |&gt; \n            drop_na()\n\n\n\n\nWith our training and testing sets created, we can now start using SVM. The most common package is e1071 which comes with a host of other useful functions. We begin with training our svm model on our data.\n\nmodel &lt;- svm(is_genuine ~ .,\n             data = train_set, kernel = \"linear\", cost = .1, scale = TRUE, \n             type = \"C-classification\")\n\nLet’s break down this function:\n\nkernel This decides how the data should be separated. If there are linear patterns, use linear; if there are polynomial pattnerns, use polynomial. There are also other kernels such as radial bias and sigmoid with their own specific use functions.\ncost: This determines how large the margin is from our hyperplane we make, to the the support vectors. The larger the cost, the smaller the margin allowed. The smaller the C, the larger the margin is.\ntype: Because SVM can be used for regression and classification, we specify that we want to use C-classification. This classification itself is the default setting in R.\n\n\n\n\nAfter training, it is now finally time to predict on our test_set\n\n  y_pred &lt;- predict(model, test_set)\n  confusionMatrix(table(y_pred, test_set$is_genuine)) \n\nConfusion Matrix and Statistics\n\n       \ny_pred  FALSE TRUE\n  FALSE    83    6\n  TRUE     23  101\n                                          \n               Accuracy : 0.8638          \n                 95% CI : (0.8104, 0.9069)\n    No Information Rate : 0.5023          \n    P-Value [Acc &gt; NIR] : &lt; 2.2e-16       \n                                          \n                  Kappa : 0.7275          \n                                          \n Mcnemar's Test P-Value : 0.002967        \n                                          \n            Sensitivity : 0.7830          \n            Specificity : 0.9439          \n         Pos Pred Value : 0.9326          \n         Neg Pred Value : 0.8145          \n             Prevalence : 0.4977          \n         Detection Rate : 0.3897          \n   Detection Prevalence : 0.4178          \n      Balanced Accuracy : 0.8635          \n                                          \n       'Positive' Class : FALSE           \n                                          \n\n\nOur accuracy is about 84% which is quite good! We also see our true positive were 78% correct and that our true negatives were 92%.\n\n\n\nDepending on our usecase, this accuracy could be quite good. For example, if a cashier has to scan cash to check for forgery every time they are handed money, it would beneficial (and hopefully a good outlook on humans) to calculate with high accuracy that a dollar is not a fake. However, with a true positive of 78%, this could conversely negatively affect those dealing with money, as 22% of the time, the model would be unfairly assuming that the money is fake."
  },
  {
    "objectID": "posts/svm_breakdown/index.html#algorithm-of-choice-support-vector-machines",
    "href": "posts/svm_breakdown/index.html#algorithm-of-choice-support-vector-machines",
    "title": "Explaining Support Vector Machines",
    "section": "",
    "text": "Support Vector Machines (SVM) is an algorithm which can be used for both regression and classification. This approach divides points into classes using a hyperplane. This line is calculated based on how far away points would lie from the line. On both sides of the line, the distance of each point is maximized. From there, we arrive to the term “Support Vector”. Support Vectors are the closest points to the hyperplane, and their distances are maximized.\nWhen trying to separate the data into two classes, one might think that SVM will fail when there are outliers in the data. However, a built-in feature in SVM is a “slack variable”. This is the distance between where the outlier lies and where it should be. From this, SVM can safely ignore the outlier, creating its line (hyperplane).\nData must separable by a line for vanilla SVM to work. However, if it is not linearly separable, we can still do SVM. In order to do so, we would add an extra dimension to our data via some function."
  },
  {
    "objectID": "posts/svm_breakdown/index.html#why-svm",
    "href": "posts/svm_breakdown/index.html#why-svm",
    "title": "Explaining Support Vector Machines",
    "section": "",
    "text": "That dataset has two defined classes: real and fake. This means that with supervised learning, we can train a model to make predictions using our classes as labels. I chose SVM because of its strength in classification problems as well as its ability to limit outliers. With a dataset as small as a few thousand rows, this means we avoid overfitting to some observed noise or outliers."
  },
  {
    "objectID": "posts/svm_breakdown/index.html#methods",
    "href": "posts/svm_breakdown/index.html#methods",
    "title": "Explaining Support Vector Machines",
    "section": "",
    "text": "We are going to clean up our data, graphically explore our data, divide into a training and test split, train our SVM model, and then finally, make predictions and gauge our accuracy."
  },
  {
    "objectID": "posts/svm_breakdown/index.html#code",
    "href": "posts/svm_breakdown/index.html#code",
    "title": "Explaining Support Vector Machines",
    "section": "",
    "text": "library(caret)\n\nLoading required package: ggplot2\n\n\nLoading required package: lattice\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ lubridate 1.9.3     ✔ tibble    3.2.1\n✔ purrr     1.0.2     ✔ tidyr     1.3.1\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n✖ purrr::lift()   masks caret::lift()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(e1071)\nlibrary(ggplot2)\n\nbills &lt;- read_csv2(\"databases/fake_bills.csv\")\n\nℹ Using \"','\" as decimal and \"'.'\" as grouping mark. Use `read_delim()` for more control.\nRows: 1500 Columns: 7── Column specification ────────────────────────────────────────────────────────\nDelimiter: \";\"\nnum (6): diagonal, height_left, height_right, margin_low, margin_up, length\nlgl (1): is_genuine\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nAn interesting graph emerges:\n\nbills |&gt;\n  ggplot() + \n  ggtitle(\"Relationship between Margin Heights and class of Bill\") +\n  geom_point(aes(x = margin_up, y = margin_low, color = is_genuine))\n\nWarning: Removed 37 rows containing missing values (`geom_point()`).\n\n\n\n\n\n\n\n\n\nEach of these clusters of data appears separable by a line!\n\n\n\n\nclean_bills &lt;- bills |&gt;\n                drop_na()\ntable(clean_bills$is_genuine)\n\n\nFALSE  TRUE \n  492   971 \n\n\nFor the variable we are classifying, is_genuine, I noticed that there are only 492 false bills, but 971 true bills. Thus, I select a random 500 true bills so that there is the same amount for both. I was able to find this out because of the table() function.\nI also decide to organize our data into a 80/20 training-test split.\n\n# Organize data into subsets of true and false\nreal_bills &lt;- clean_bills |&gt; \n    filter(is_genuine == TRUE) %&gt;%\n    sample_n(size = 500, replace = FALSE)\n\nfake_bills &lt;- bills |&gt;\n    filter(is_genuine == FALSE)   \n\n# Create test data \ntrain_fake_bills &lt;- fake_bills |&gt; \n    sample_n(size = 393, replace = FALSE)\n\ntrain_real_bills &lt;- real_bills |&gt; \n    sample_n(size = 393, replace = FALSE)\n\n# Create training data\ntest_real_bills &lt;- anti_join(real_bills, train_real_bills)\n\nJoining with `by = join_by(is_genuine, diagonal, height_left, height_right,\nmargin_low, margin_up, length)`\n\ntest_fake_bills &lt;- anti_join(fake_bills, train_fake_bills)\n\nJoining with `by = join_by(is_genuine, diagonal, height_left, height_right,\nmargin_low, margin_up, length)`\n\n# Combine our sets to make the full sets of each\ntrain_set &lt;- bind_rows(train_fake_bills, train_real_bills)\ntest_set &lt;- bind_rows(test_fake_bills, test_real_bills)\n\n#Drop na values \ntrain_set &lt;- train_set |&gt; \n            drop_na()\n\ntest_set &lt;- test_set |&gt; \n            drop_na()\n\n\n\n\nWith our training and testing sets created, we can now start using SVM. The most common package is e1071 which comes with a host of other useful functions. We begin with training our svm model on our data.\n\nmodel &lt;- svm(is_genuine ~ .,\n             data = train_set, kernel = \"linear\", cost = .1, scale = TRUE, \n             type = \"C-classification\")\n\nLet’s break down this function:\n\nkernel This decides how the data should be separated. If there are linear patterns, use linear; if there are polynomial pattnerns, use polynomial. There are also other kernels such as radial bias and sigmoid with their own specific use functions.\ncost: This determines how large the margin is from our hyperplane we make, to the the support vectors. The larger the cost, the smaller the margin allowed. The smaller the C, the larger the margin is.\ntype: Because SVM can be used for regression and classification, we specify that we want to use C-classification. This classification itself is the default setting in R.\n\n\n\n\nAfter training, it is now finally time to predict on our test_set\n\n  y_pred &lt;- predict(model, test_set)\n  confusionMatrix(table(y_pred, test_set$is_genuine)) \n\nConfusion Matrix and Statistics\n\n       \ny_pred  FALSE TRUE\n  FALSE    83    6\n  TRUE     23  101\n                                          \n               Accuracy : 0.8638          \n                 95% CI : (0.8104, 0.9069)\n    No Information Rate : 0.5023          \n    P-Value [Acc &gt; NIR] : &lt; 2.2e-16       \n                                          \n                  Kappa : 0.7275          \n                                          \n Mcnemar's Test P-Value : 0.002967        \n                                          \n            Sensitivity : 0.7830          \n            Specificity : 0.9439          \n         Pos Pred Value : 0.9326          \n         Neg Pred Value : 0.8145          \n             Prevalence : 0.4977          \n         Detection Rate : 0.3897          \n   Detection Prevalence : 0.4178          \n      Balanced Accuracy : 0.8635          \n                                          \n       'Positive' Class : FALSE           \n                                          \n\n\nOur accuracy is about 84% which is quite good! We also see our true positive were 78% correct and that our true negatives were 92%.\n\n\n\nDepending on our usecase, this accuracy could be quite good. For example, if a cashier has to scan cash to check for forgery every time they are handed money, it would beneficial (and hopefully a good outlook on humans) to calculate with high accuracy that a dollar is not a fake. However, with a true positive of 78%, this could conversely negatively affect those dealing with money, as 22% of the time, the model would be unfairly assuming that the money is fake."
  },
  {
    "objectID": "posts/WiDs-blog-post/index.html",
    "href": "posts/WiDs-blog-post/index.html",
    "title": "WiDS Conference",
    "section": "",
    "text": "On March 4th, I attended the WiDS conference here in Middlebury. The room was filled with Middlebury Students eager to hear lectures, talk to peers, and eat some delicious snacks. Looking around the room, there is a 60/40 split of women and men. To a passerby, it might seem like Computer Science is a deeply diverse. To Computer Science Majors at Middlebury, one can notice that the class is roughly a 50/50 split of men. I learned from 4 different Professors on how to use data, and what it means to be a women in Data Science. I read the report “Solving the Equation: The Variables for Women’s Success in Engineering and Computing” written by Corbett and Hill (2015) to supplement all the work I did in this essay and from there I learned that women were the original data scientists that were then systematically excluded from the same profession over-time.\n\n\nTo answer this question, let us first reflect on the history of women in Computer Science, Engineering, and Math to understand the situation women face now. Leading up to the 1950s, the “first computing programmer”, Ada lovelace figured out that one could feed a series of instructions to an theoretical machine to perform calculations. Women were among the first programmers of the ENIAC during World War 2. This led traditionally male engineering programs to start allowing the admission of women to these institutions.\n\n\n\n\n\nImage 1. A picture of Grace Hopper, the creator of the first compiler! Courtesy of Getty Images\n\n\nComputing in the 1960s was still viewed as a “low-status clerical” job, which revolved around administrative and “secretary”-style work. However, companies began to use aptitude tests that male stereotypes heavily benefited from. A strong personality began to develop around Computer Science, leading to the stereotype of “antisocial, mathematically inclined males” being the face of Computer Science. While prompting a drop in representation of women in CS, there existed another significant drop in the 1980s.\nThe personal computer became a “toy” for young men to get heavy interaction with in the home. This connected gaming culture to the home, which was a culture that traditionally excluded women. As Gamer-culture has become entwined with those those interested in Computer Science departments, it is important to notice what actually happens at these companies that revolve around gaming. What you can see can be regarded as “frat boy behavior”, in which these large video-game companies, which heavily influence those who get into computer science, are creating unsafe and harmful situations in which women who want to work in industry are forced to face. This feeds into an unfortunate positive feedback loop, where as more women feel uncomfortable and leave these positions, men then fill in the gaps. What are the reprecussions of diversity dropping in tech and related fields?\nWith the growing amount of dependency our world has on the work of tech companies, algorithms continue to be implemented to affect lives in a major way. As can be clearly inferred, missing half the population can lead to drastic consequences, and is as unsustainable as fossil fuel usage. With greater diversity of thought in the workplace, we can make better, more informed decisions. Moreover, it leads to lower quality to life for women in the workforce. Engineering and CS tend to offer job benefits that are rarer in other fields. Workplace flexibility exploded during COVID, but engineering roles have maintained that working from home is a viable path. There are reports that Job satisfaction is higher in engineering jobs, and while I believe often times applies more to men, these companies can most definitely offer this satisfaction for women in these companies, if they fix their current issues that push women away. Perhaps most importantly, the engineering and computing fields have almost entirely closed the gender pay gap, with women being paid 90 cents for every dollar made by a man.\n\n\n\nThese disciplines continue to disenfranchise women with work-life balance issues, sense of belonging, in-group favoritism, and microinequities, so is there any hope? I think that is a perfectly logical question, to which I of course say yes!! It is important to remember that people all around the world are trying to resolve this imbalance in Math, Engineering, and CS. Programs like Girls who code seek to provide opportunities for more women to get access to coding and eventually becoming engineers. Huge conferences like the Grace Hopper Conference try to connect female and non-binary students from around the world to network and connect with female professionals in the Engineering and Data Science fields. However, how does a college or university get to push women towards these disciplines? Fostering a strong bond between engineers-to-be and working professionals at the same university is where events like WiDS comes in.\n\n\n\nIt was exciting to be seated among so many people in STEM at Middlebury. The room was much more diverse than the average Middlebury classroom, and we all munched on small snacks as we waited for the presentations to begin.\n\n\n\n\n\nImage 2. A picture of Professor Amy Yuen\n\n\nThe first presenter was Professor Amy Yuen. She is a political science teacher at Middlebury that focuses her research on the intersection between game theory, large statistical (SC) and asked two questsion: Why do members continue to run for spots and is the SC democratic? Through these questions she wanted to answer whose issues are discussed on the international stage. Her research showed two key things: 1. There are Permanent Members whose main job is to fight crisis and maintain. 2. Elected Members are more likely to fight for current issues. Within these Elected Members, there are some who invest lots of money to get reelected. Ultimately, this answered that members who continue to run are trying to fight for current issues and that the SC is not democratic as votes from certain countries carry more weight i.e. Permanent Members.\n\n\n\n\n\nImage 3. A picture of Professor Sarah Brown\n\n\nThe second presenter was Professor Sarah Brown. She is a Computer Science professor at the University of Rhode Island. Her research focuses on Machine Learning as a multidisciplinary field. As a keynote speaker, Professor Brown did not fail to deliver. With a BS, MS, and PhD, she appears to be an extremely STEM and hard science person. However, her presentation sought to explain what makes up Machine learning. It is made up of three domains: Computer Science, Statistics, and Domain-Expertise. While CS students often also understand statistics, they have to reach out to domain-experts for their expertise. What Professor Brown presented was how Computer Scientists and Data Scientists can add specific “keys” to their toolbox to unlock this knowledge.\nShe presented three keys: “Contextualize Data”, “Disciplines Are Communities”, and “Meet People Where They Are”. Professor Brown explains that data is a primary source. Treat this like Social studies class and use context to supplement the trends you are seeing, and you will see underlying properties emerge. After seeing these patterns, it is important to know when to make and deploy a model. As a community and a discipline, we must define fairness, and know how that influences so that we can properly determine how to use our models. But how do we ensure that people do their due diligence and try to ensure fairness? This is where we meet people where they are. You have to show that unbiased algorithms affect all of us, and that all of us need to fight for fairness in algorithms.\n\n\n\n\n\nImage 4. A picture of Professor Jessica L’Roe\n\n\nThe third presenter was Professor Jessica L’Roe. She is a professor of Geoegraphy at Middlebury that studies the relationship with humans in forest landscape in East Africa and the Amazon. Professor L’Roe brought a refreshing look at how data can be used in non-traditional settings, and more importantly how data is collected. She presented quite a bit of work, but her research on “Landscape Changes around Kibale National Park” was particularly interesting. She harped on the importance of both qualitative and quantitative data. A piece of data that was particularly interesting to me was collecting data about what benefits people felt from the forest near them. Within her talk, her overall question appeared to be “How do you apply stats to work?” and she challenged this by showcasing her various usages of data.\n\n\n\n\n\nImage 5. A picture of Professor Laura Biester\n\n\nThe fourth presenter was Professor Laura Biester. She is a professor in our very own Computer science department at Middlebury. Her research focuses on natural language processing and computational social science. Her talk focused on a pretty impressive technological feat of searching all the comments in reddit and finding examples of people using “identity claims” of depression. These were instances where people stated that they were diagnosed with depression. The question posed was to discover if there were benefits to sharing one’s diagnosis on an anonymous forumn like Reddit. The research focused on the difference in the way they spoke before and following their identity claim. The research ultimately showed that they displayed more positive emotions after their identity claim.\n\n\n\nFrom this blog, I learned how to speak about a very real issue in Computer Science. Women continue to choose other fields not because of some arbitrary disinterest but because of a slew of behavior within the CS world that is both intentionally and unintentionally excludes women. These same causes also disproportionately underrepresented minority women to be significantly less likely to pursue fields related to engineering. I identified with some of the struggles faced by women as a Latino man interested in the CS world, particularly the inherent in-group favoritism and a sense of belonging. I also understood that there were varying degrees with which I identified with the very serious problems that women face in these disciplines. Going to the conference gave me a very unique opportunity to listen to female professionals adjacent to the Engineering disciplines clearly outline how they used data in their every day work. Following this meeting I talked to my partner a bit about this blog post and she told me possibly my biggest takeaway from the reading and essay I have synthesized. Female Professionals, regardless of the field, thread a delicate line between displaying femininity and masculinity. Particularly in the CS world, they are competing in a predominantly male field, and thus “masculine” traits are cherished. However, women are still expected to be somewhat feminine, otherwise they are also seen in a negative light. Having to experience this sort of dysphoria in the workplace was incredibly eye-opening to me, and without this blog post I might not have learned that.\nFrom the presenters, I particularly enjoyed the key-note speaker. Professor Brown did an incredible job connecting soft skills to Computer Science. I resonated heavily with the way she detailed how our past experiences in non-STEM areas should influence how we are thinking about CS. Moreover, she provided a unique viewpoint being an extremely decorated POC female professor and gave liberal arts CS majors a sense of tranquility as we apply to opportunities in the real world.\n\n\n\n\n“https://www.aauw.org/app/uploads/2020/03/Solving-the-Equation-report-nsa.pdf” by Corbett and Hill 2015\n“https://www.middlebury.edu/college/academics/computer-science/faculty-and-staff” (for photos)"
  },
  {
    "objectID": "posts/WiDs-blog-post/index.html#abstract",
    "href": "posts/WiDs-blog-post/index.html#abstract",
    "title": "WiDS Conference",
    "section": "",
    "text": "On March 4th, I attended the WiDS conference here in Middlebury. The room was filled with Middlebury Students eager to hear lectures, talk to peers, and eat some delicious snacks. Looking around the room, there is a 60/40 split of women and men. To a passerby, it might seem like Computer Science is a deeply diverse. To Computer Science Majors at Middlebury, one can notice that the class is roughly a 50/50 split of men. I learned from 4 different Professors on how to use data, and what it means to be a women in Data Science. I read the report “Solving the Equation: The Variables for Women’s Success in Engineering and Computing” written by Corbett and Hill (2015) to supplement all the work I did in this essay and from there I learned that women were the original data scientists that were then systematically excluded from the same profession over-time.\n\n\nTo answer this question, let us first reflect on the history of women in Computer Science, Engineering, and Math to understand the situation women face now. Leading up to the 1950s, the “first computing programmer”, Ada lovelace figured out that one could feed a series of instructions to an theoretical machine to perform calculations. Women were among the first programmers of the ENIAC during World War 2. This led traditionally male engineering programs to start allowing the admission of women to these institutions.\n\n\n\n\n\nImage 1. A picture of Grace Hopper, the creator of the first compiler! Courtesy of Getty Images\n\n\nComputing in the 1960s was still viewed as a “low-status clerical” job, which revolved around administrative and “secretary”-style work. However, companies began to use aptitude tests that male stereotypes heavily benefited from. A strong personality began to develop around Computer Science, leading to the stereotype of “antisocial, mathematically inclined males” being the face of Computer Science. While prompting a drop in representation of women in CS, there existed another significant drop in the 1980s.\nThe personal computer became a “toy” for young men to get heavy interaction with in the home. This connected gaming culture to the home, which was a culture that traditionally excluded women. As Gamer-culture has become entwined with those those interested in Computer Science departments, it is important to notice what actually happens at these companies that revolve around gaming. What you can see can be regarded as “frat boy behavior”, in which these large video-game companies, which heavily influence those who get into computer science, are creating unsafe and harmful situations in which women who want to work in industry are forced to face. This feeds into an unfortunate positive feedback loop, where as more women feel uncomfortable and leave these positions, men then fill in the gaps. What are the reprecussions of diversity dropping in tech and related fields?\nWith the growing amount of dependency our world has on the work of tech companies, algorithms continue to be implemented to affect lives in a major way. As can be clearly inferred, missing half the population can lead to drastic consequences, and is as unsustainable as fossil fuel usage. With greater diversity of thought in the workplace, we can make better, more informed decisions. Moreover, it leads to lower quality to life for women in the workforce. Engineering and CS tend to offer job benefits that are rarer in other fields. Workplace flexibility exploded during COVID, but engineering roles have maintained that working from home is a viable path. There are reports that Job satisfaction is higher in engineering jobs, and while I believe often times applies more to men, these companies can most definitely offer this satisfaction for women in these companies, if they fix their current issues that push women away. Perhaps most importantly, the engineering and computing fields have almost entirely closed the gender pay gap, with women being paid 90 cents for every dollar made by a man.\n\n\n\nThese disciplines continue to disenfranchise women with work-life balance issues, sense of belonging, in-group favoritism, and microinequities, so is there any hope? I think that is a perfectly logical question, to which I of course say yes!! It is important to remember that people all around the world are trying to resolve this imbalance in Math, Engineering, and CS. Programs like Girls who code seek to provide opportunities for more women to get access to coding and eventually becoming engineers. Huge conferences like the Grace Hopper Conference try to connect female and non-binary students from around the world to network and connect with female professionals in the Engineering and Data Science fields. However, how does a college or university get to push women towards these disciplines? Fostering a strong bond between engineers-to-be and working professionals at the same university is where events like WiDS comes in.\n\n\n\nIt was exciting to be seated among so many people in STEM at Middlebury. The room was much more diverse than the average Middlebury classroom, and we all munched on small snacks as we waited for the presentations to begin.\n\n\n\n\n\nImage 2. A picture of Professor Amy Yuen\n\n\nThe first presenter was Professor Amy Yuen. She is a political science teacher at Middlebury that focuses her research on the intersection between game theory, large statistical (SC) and asked two questsion: Why do members continue to run for spots and is the SC democratic? Through these questions she wanted to answer whose issues are discussed on the international stage. Her research showed two key things: 1. There are Permanent Members whose main job is to fight crisis and maintain. 2. Elected Members are more likely to fight for current issues. Within these Elected Members, there are some who invest lots of money to get reelected. Ultimately, this answered that members who continue to run are trying to fight for current issues and that the SC is not democratic as votes from certain countries carry more weight i.e. Permanent Members.\n\n\n\n\n\nImage 3. A picture of Professor Sarah Brown\n\n\nThe second presenter was Professor Sarah Brown. She is a Computer Science professor at the University of Rhode Island. Her research focuses on Machine Learning as a multidisciplinary field. As a keynote speaker, Professor Brown did not fail to deliver. With a BS, MS, and PhD, she appears to be an extremely STEM and hard science person. However, her presentation sought to explain what makes up Machine learning. It is made up of three domains: Computer Science, Statistics, and Domain-Expertise. While CS students often also understand statistics, they have to reach out to domain-experts for their expertise. What Professor Brown presented was how Computer Scientists and Data Scientists can add specific “keys” to their toolbox to unlock this knowledge.\nShe presented three keys: “Contextualize Data”, “Disciplines Are Communities”, and “Meet People Where They Are”. Professor Brown explains that data is a primary source. Treat this like Social studies class and use context to supplement the trends you are seeing, and you will see underlying properties emerge. After seeing these patterns, it is important to know when to make and deploy a model. As a community and a discipline, we must define fairness, and know how that influences so that we can properly determine how to use our models. But how do we ensure that people do their due diligence and try to ensure fairness? This is where we meet people where they are. You have to show that unbiased algorithms affect all of us, and that all of us need to fight for fairness in algorithms.\n\n\n\n\n\nImage 4. A picture of Professor Jessica L’Roe\n\n\nThe third presenter was Professor Jessica L’Roe. She is a professor of Geoegraphy at Middlebury that studies the relationship with humans in forest landscape in East Africa and the Amazon. Professor L’Roe brought a refreshing look at how data can be used in non-traditional settings, and more importantly how data is collected. She presented quite a bit of work, but her research on “Landscape Changes around Kibale National Park” was particularly interesting. She harped on the importance of both qualitative and quantitative data. A piece of data that was particularly interesting to me was collecting data about what benefits people felt from the forest near them. Within her talk, her overall question appeared to be “How do you apply stats to work?” and she challenged this by showcasing her various usages of data.\n\n\n\n\n\nImage 5. A picture of Professor Laura Biester\n\n\nThe fourth presenter was Professor Laura Biester. She is a professor in our very own Computer science department at Middlebury. Her research focuses on natural language processing and computational social science. Her talk focused on a pretty impressive technological feat of searching all the comments in reddit and finding examples of people using “identity claims” of depression. These were instances where people stated that they were diagnosed with depression. The question posed was to discover if there were benefits to sharing one’s diagnosis on an anonymous forumn like Reddit. The research focused on the difference in the way they spoke before and following their identity claim. The research ultimately showed that they displayed more positive emotions after their identity claim.\n\n\n\nFrom this blog, I learned how to speak about a very real issue in Computer Science. Women continue to choose other fields not because of some arbitrary disinterest but because of a slew of behavior within the CS world that is both intentionally and unintentionally excludes women. These same causes also disproportionately underrepresented minority women to be significantly less likely to pursue fields related to engineering. I identified with some of the struggles faced by women as a Latino man interested in the CS world, particularly the inherent in-group favoritism and a sense of belonging. I also understood that there were varying degrees with which I identified with the very serious problems that women face in these disciplines. Going to the conference gave me a very unique opportunity to listen to female professionals adjacent to the Engineering disciplines clearly outline how they used data in their every day work. Following this meeting I talked to my partner a bit about this blog post and she told me possibly my biggest takeaway from the reading and essay I have synthesized. Female Professionals, regardless of the field, thread a delicate line between displaying femininity and masculinity. Particularly in the CS world, they are competing in a predominantly male field, and thus “masculine” traits are cherished. However, women are still expected to be somewhat feminine, otherwise they are also seen in a negative light. Having to experience this sort of dysphoria in the workplace was incredibly eye-opening to me, and without this blog post I might not have learned that.\nFrom the presenters, I particularly enjoyed the key-note speaker. Professor Brown did an incredible job connecting soft skills to Computer Science. I resonated heavily with the way she detailed how our past experiences in non-STEM areas should influence how we are thinking about CS. Moreover, she provided a unique viewpoint being an extremely decorated POC female professor and gave liberal arts CS majors a sense of tranquility as we apply to opportunities in the real world.\n\n\n\n\n“https://www.aauw.org/app/uploads/2020/03/Solving-the-Equation-report-nsa.pdf” by Corbett and Hill 2015\n“https://www.middlebury.edu/college/academics/computer-science/faculty-and-staff” (for photos)"
  },
  {
    "objectID": "posts/logistic-reg-post/index.html",
    "href": "posts/logistic-reg-post/index.html",
    "title": "Logistic Regression",
    "section": "",
    "text": "Gradient Descent remains a modern way to minimize loss in a Machine Learning Algorithm. As a result, it is important to explore the different ways this can be improved. Through implementation of a Logistic Regression using Gradient Descent with momentum, I proved graphically how the efficacy of models with momentum superpassess that of those without. When using Gradient Descent, it is better to use momentum as it converges faster. I also show the issue of utilizing more dimensions than there are points. This leads to overfitting, and results in poor accuracy. When you have more dimensions than points, it is best to lower your dimensions by attempting to combine sparse dimensions or by removal. The source of my code lies here"
  },
  {
    "objectID": "posts/logistic-reg-post/index.html#abstract",
    "href": "posts/logistic-reg-post/index.html#abstract",
    "title": "Logistic Regression",
    "section": "",
    "text": "Gradient Descent remains a modern way to minimize loss in a Machine Learning Algorithm. As a result, it is important to explore the different ways this can be improved. Through implementation of a Logistic Regression using Gradient Descent with momentum, I proved graphically how the efficacy of models with momentum superpassess that of those without. When using Gradient Descent, it is better to use momentum as it converges faster. I also show the issue of utilizing more dimensions than there are points. This leads to overfitting, and results in poor accuracy. When you have more dimensions than points, it is best to lower your dimensions by attempting to combine sparse dimensions or by removal. The source of my code lies here"
  },
  {
    "objectID": "posts/logistic-reg-post/index.html#part-a-initialize-packages-and-data",
    "href": "posts/logistic-reg-post/index.html#part-a-initialize-packages-and-data",
    "title": "Logistic Regression",
    "section": "Part A: Initialize packages and data",
    "text": "Part A: Initialize packages and data\n\n%load_ext autoreload\n%autoreload 2\n\nimport torch \nimport numpy as np\nfrom logistic import LogisticRegression, GradientDescentOptimizer\n\ndef classification_data(n_points = 300, noise = 0.2, p_dims = 2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    y = 1.0*y\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n    \n    return X, y\n\nX, y = classification_data(noise = 0.5)"
  },
  {
    "objectID": "posts/logistic-reg-post/index.html#part-b-experiments",
    "href": "posts/logistic-reg-post/index.html#part-b-experiments",
    "title": "Logistic Regression",
    "section": "Part B: Experiments",
    "text": "Part B: Experiments\n\nVanilla Gradient Descent\nAs in all of our following models, we are going to utilize Gradient Descent. This means that we are going to continously attempt to reach a local maximum in gradual steps. We will eventually converge to a value and our loss will start to plateau at a given value.\n\nLR = LogisticRegression() \nopt = GradientDescentOptimizer(LR)\n\nvanilla_loss_vec = []\nfor _ in range(10000):\n    loss = LR.loss(X, y) \n    vanilla_loss_vec.append(loss)\n    opt.step(X, y, alpha = 0.04, beta = 0)\n\nWe first perform our vanilla gradient descent, which means \\(p_{dim} = 2\\text{, }α = 0.03\\text{, and }β = 0\\) we can see from our line that it converges to the correct weight vector around 6000 iterations\n\nfrom matplotlib import pyplot as plt\nplt.plot(vanilla_loss_vec, color = \"black\")\nplt.title(\"Loss Gradient Descent without Momentum over iterations\")\nlabs = plt.gca().set(xlabel = \"Number of gradient descent iterations\", ylabel = \"Loss (binary cross entropy)\")\n\n\n\n\n\n\n\n\n\n\nUsing momentum\nSomething that gradient descent falls victim to, is thinking it discovered a global minimum, when in fact it has found a local minimum. The idea of momentum is that the our algorithm should converge at a faster rate than vanilla gradient descent. It also means that it should be able to overcome areas that appears to be global minima, by “speeding” through those areas with assisted momentum.\nWe then perform our gradient descent with momentum, which means \\(p_{dim} = 2\\text{, }α = 0.03\\text{, and }β = 0.9\\) we can see an extremely sharp drop off from our loss which converges to the correct weight vector around a few hundred iterations\n\nLR = LogisticRegression() \nopt = GradientDescentOptimizer(LR)\n\nmomentum_loss_vec = []\nfor _ in range(10000):\n    loss = LR.loss(X, y) \n    momentum_loss_vec.append(loss)\n    opt.step(X, y, alpha = 0.03, beta = .9)\n\n\nplt.plot(momentum_loss_vec, color = \"black\")\nplt.title(\"Loss Gradient Descent with Momentum over iterations\")\nlabs = plt.gca().set(xlabel = \"Number of gradient descent iterations\", ylabel = \"Loss (binary cross entropy)\")\n\n\n\n\n\n\n\n\n\n\nOverfitting\nThe curse of all scientists trying to make predictions with models is the potential to overfit. If we have more dimensions than we have points, then the data is highly sparse, meaning that there are many dimensions with little or no points. With higher dimensionality, the model will more likely to fit to noise. Thus, we are going to simulate how this can happen with more dimensions than points.\n\nCreating our data\nIn order to model what happens when we have more dimensions than we have points, let’s call our classification_data function twice so that we can save training data and test dating. That way, we can see what overfitting on our training data causes our predictions to look like on our test data.\nWe give them the same parameters \\[\\text{n\\_points = 50, noise = 0.5, p\\_dims = 100}\\]\n\nX_train, y_train = classification_data(n_points = 50, noise = 0.5, p_dims = 100)\nX_test, y_test = classification_data(n_points = 50, noise = 0.5, p_dims = 100)\n\n\nfrom sklearn.metrics import accuracy_score \n\nLR = LogisticRegression() \nopt = GradientDescentOptimizer(LR)\n\noverfit_loss_vec = []\nfor _ in range(10000):\n    loss = LR.loss(X_train, y_train) \n    overfit_loss_vec.append(loss)\n    opt.step(X_train, y_train, alpha = 0.001, beta = .8)\n\ny_pred_train = LR.predict(X_train)\ny_pred_test = LR.predict(X_test)\n\n\ntrain_prediction = accuracy_score(y_train, y_pred_train)\ntest_prediction = accuracy_score(y_test, y_pred_test)\n\nprint(\"Training Accuracy: %5.2f Test Accuracy: %5.2f\" % (train_prediction, test_prediction))\n\nTraining Accuracy:  1.00 Test Accuracy:  0.76\n\n\nOur final accuracy is \\(76\\%\\) for our test data, even though our training data has a final accuracy of \\(100\\%\\)!"
  },
  {
    "objectID": "posts/logistic-reg-post/index.html#discussion",
    "href": "posts/logistic-reg-post/index.html#discussion",
    "title": "Logistic Regression",
    "section": "Discussion",
    "text": "Discussion\nWe have gone quite a few places, so its important to sum up what we have done as well as what we have discovered. We implemented Logistic Regression with Gradient Descent with Momentum. Using our Logistic Regression, we conducted three experiments. The first experiment was our Vanilla Gradient Descent. This meant we didn’t utilize momentum, and it resulted in a convergence at around 6000 iterations. Using the same parameters, we conducted our second experiment Gradient Descent with Momentum. By using momentum, we were able to converge at a faster rate. This was expected as momentum helps our Gradient Descent more quickly reach a local minimum."
  },
  {
    "objectID": "posts/perceptron-post/index.html",
    "href": "posts/perceptron-post/index.html",
    "title": "Perceptron Blog",
    "section": "",
    "text": "We are going to load in our algorithm from our perceptron.py!\n\nimport torch \nimport numpy as np\ntorch.manual_seed(1234)\n\ndef perceptron_data(n_points = 300, noise = 0.2, p_dims = 2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n\n    # convert y from {0, 1} to {-1, 1}\n    y = 2*y - 1\n\n    return X, y\n\nX, y = perceptron_data(n_points = 300, noise = 0.2)\n\n\n%load_ext autoreload\n%autoreload 2\n\ntorch.manual_seed(1234)\nX, y = perceptron_data(n_points = 50, noise = 0.3)\n\nfrom perceptron import Perceptron, PerceptronOptimizer\n# instantiate a model and an optimizer\np = Perceptron() \nopt = PerceptronOptimizer(p)\n\nloss = 1.0\n\n# for keeping track of loss values\nloss_vec = []\n\nn = X.size()[0]\n\nwhile loss &gt; 0: # dangerous -- only terminates if data is linearly separable\n    \n    # not part of the update: just for tracking our progress    \n    loss = p.loss(X, y) \n    loss_vec.append(loss)\n    \n    # pick a random data point\n    i = torch.randint(n, size = (1,))\n    x_i = X[[i],:]\n    y_i = y[i]\n    \n    # perform a perceptron update using the random data point\n    opt.step(x_i, y_i)\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n\n\nRuntimeError: mat1 and mat2 shapes cannot be multiplied (50x3 and 1x3)"
  },
  {
    "objectID": "posts/perceptron-post/index.html#starting",
    "href": "posts/perceptron-post/index.html#starting",
    "title": "Perceptron Blog",
    "section": "",
    "text": "We are going to load in our algorithm from our perceptron.py!\n\nimport torch \nimport numpy as np\ntorch.manual_seed(1234)\n\ndef perceptron_data(n_points = 300, noise = 0.2, p_dims = 2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n\n    # convert y from {0, 1} to {-1, 1}\n    y = 2*y - 1\n\n    return X, y\n\nX, y = perceptron_data(n_points = 300, noise = 0.2)\n\n\n%load_ext autoreload\n%autoreload 2\n\ntorch.manual_seed(1234)\nX, y = perceptron_data(n_points = 50, noise = 0.3)\n\nfrom perceptron import Perceptron, PerceptronOptimizer\n# instantiate a model and an optimizer\np = Perceptron() \nopt = PerceptronOptimizer(p)\n\nloss = 1.0\n\n# for keeping track of loss values\nloss_vec = []\n\nn = X.size()[0]\n\nwhile loss &gt; 0: # dangerous -- only terminates if data is linearly separable\n    \n    # not part of the update: just for tracking our progress    \n    loss = p.loss(X, y) \n    loss_vec.append(loss)\n    \n    # pick a random data point\n    i = torch.randint(n, size = (1,))\n    x_i = X[[i],:]\n    y_i = y[i]\n    \n    # perform a perceptron update using the random data point\n    opt.step(x_i, y_i)\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n\n\nRuntimeError: mat1 and mat2 shapes cannot be multiplied (50x3 and 1x3)"
  },
  {
    "objectID": "posts/final-project-post/final_project.html",
    "href": "posts/final-project-post/final_project.html",
    "title": "Project Blog Post",
    "section": "",
    "text": "Our project takes on the challenge of predicting population density in regions lacking data. Leveraging landcover image data and tract geometry, our approach involves computing zonal statistics and employing machine learning models. With this problem in mind, we employ satellite data from Connecticut due to its completeness and its potential to also be applied to other Northeastern States within the US. We create Linear Regression models and Spatial Autoregression models with our zonal statistics and tract data. We gauge their efficacy based on their mean-squared error and \\(R^2\\) value. Through this, we find that Linear Regression with No Penalty works best out of our Linear Regression models and our Endogenous Spatial Autoregression model works better than the Exogenous model. Furthermore, we conclude that Spatial Autoregression is more effective at predicting population density than Linear Regression. In regions without adequate census data, the Exogenous model would improve estimations of population density by taking into account the landcover of a given region and its neighbors. Our code and datasets are available through our Github\n\n\n\nIn countries such as the US, there is a large and accurate amount of census data. However there are many ountries in areas where the resources for gathering census data is lesser. This is potentially due to geographic inaccessibility, political conflict, administrative failiure, and as mentioned previously, a lack of resources. Thus, we want a way to predict human populations around the world with the data of the land itself, satellite imagery. With this imaging, the geography is divided into classes which we can then use as variables for our model. Research into this topic has stagnated to a degree, however Tian et al. (2005) produced a hallmark paper which tested the effectivity of modeling population with land cover data. It found that a similar model could have “feasible” and can have “high accuracy”. They utilized Linear Regression, and also manually broke down China into even 1km by 1km cells. Because of availablity of census data, we instead used census tracts, but we continued with the idea of utilizing Linear Regression. With some exploratory graphs of Connecticut, we discovered there might be a Spatial Pattern within our data. In order to take this into account during modeling, we started researching into machine learning algorithms with a spatial component. We came across a paper by Liu, Kounadi, and Zurita-Milla (2022), which concluded that models with a spatial component, such as spatial lag, garner better results than those without. They used spatial lag, and eigvenvectors spatial filtering to predict things beyond our datasets such as soil types. Thus, we sought to create Linear Regression Models and Spatial Autoregressive models, and compare the them to see which is more effective in predicting population density based on land cover.\n\n\n\nNASA in a webinar session called “Humanitarian Applications Using NASA Earth Observations” presented how satellite remote-sensing data could be useful in monitoring humanitarian conditions at refugee settlements. Human settlements could be detected through remote sensing images and therefore could be used to predict the population in a region. This talk alerted us that we still lack necessary population data in many parts of the world, but also demonstrated how remote sensing could be a powerful tool in tackling this problem and solving lack of population data in different countries. Thus, we decide to investigate the connection between remote sensing land cover data and population density in a context with better data coverage.\nThis type of model would be most beneficial by governments and government organizations. These users would most likely be hospital contractors, policy makers, emergency services providers such as ambulances and firefighers, and sociologists. Population census data is crucial for policy makers as it assists in city management so that the equitable distribution of resources can be better calculated.\nThe implications extend beyond helping users. Real people would be affected by this technology. Those who are workers in fields such as emergency service work, or school teachers who might have been over-worked previously may be relieved by the building of new hospitals and schools to compensate for population changes. However, the negative effects are also extremely real.\nImagining that this model expanded beyond the barriers of Connecticut and is being used in countries with much lower census data such as Brazil, there might be a calculation for a forestry company to continue harvesting wood from the Amazon, but they do not want to affect populations. Our algorithm calculates there are very few people in the area, as there is very dense land cover in the Amazon. This company starts to cut down trees and discovers that they are in an area of Indigenous peoples. A minority group that is already negatively affected continues to be disenfranchised. The issue of undercalculating the population density in an area can also affect the amount of resources a policymaker might provide to a region with a much greater population and lacking resources. This would also continue to negatively impact an already negatively impacted area.\nUltimately, the world would be a more equitable and sustainable place if this type of technology could assist countries lacking population data. The positive aspects of providing data where there is none provides the potential for great resource partioning, and better understanding of a countries population.\n\n\n\n\n\nWith this project being the entire state of Connecticut, we utilized landcover data, population, shape files for graphing, and synthesized data which combined our various data sets into manageable datasets suitable for modeling.\nThe bread and butter of our data stems from a 1-meter resolution landcover imagery covering the entire state of Connecticut. Derived from NAIP, the data has already been processed such that every pixel represents a certain class of landcover.\nAt over 800 MB, the dataset is too large to share via GitHub, and is downloadable by clicking on the first option at this link. This landcover dataset was one of the most complete datsets we could find, which is why we wanted to use it for our modelling.\nOur other data sources are the geometries and population data on the Census tract level for the state of Connecticut. We downloaded tract geometries directly into our Jupyter Notebook final_project.ipynb using the Pygris package, and we downloaded the population data from Social Explorer, storing it at data/population.csv.\n\n\n\nFirst, we clean and prepare our data for the model. We start by combining our Tract Geometry of CT with the Population Data of CT to form a new dataset. We utilize both the CT Landcover Data and the Tracts Data in a calculation of Zonal Statistics. This means we calculate the proportion of pixels within each tract that are of a given landcover class. This then is saved as a combined dataset which we then continue to clean by imputing values, performing more advanced Zonal Statistics, and dropping any NA Columns. From there, we are left with data ready to be used in a model.\nThe flowchart below more elegantly outlines this process\n\n\n\n\n\nflowchart LR\n  A(Population Data) --&gt; B(Tracts Data)\n  C(Tracts Geometry Data) --&gt; B(Tracts Data)\n  B --&gt; D{Zonal Statistics}\n  E(CT Landcover Data) --&gt; D{Zonal Statistics}\n  D{Zonal Statistics} --&gt; F(Combined Data)\n  F(Combined Data) --&gt; |Impute Data| G[Ready for Model]\n  F --&gt; |Additional Landcover Statistics| G[Ready for Model]\n  F --&gt; |Drop Uncommon Landcover| G[Cleaned Data]\n\n\n\n\n\n\nWe then implement three types of Linear Regression:\n\nLinear Regression with No Penalty Term\nLinear Regression with \\(\\ell_1\\) Regularization (Lasso Regression)\nLinear Regression with \\(\\ell_1\\) Regularization (Ridge Regression)\n\nBy utilizing the \\(R^2\\) and Mean Squared Error, we quantified the success of each of our models against one another as well as comparing them to sci-kit learn’s own implementations of each of these Linear Regression Models.\nFollowing Linear Regression, we then wanted to implement two types of Spatial AutoRegression:\n\nEndogenous Spatial Autoregression\nExogenous Spatial Autoregression\n\nAs our data can be plotted on a map of Connecticut, we felt it would be amiss to not explore Spatial Autogression. Through this style of model, we can take into account the spatial aspect of each tract when we are predicting. We chose both Endogenous and Exogenous Models. Endogenous Models take into account the neighboring tract population densities of a given tract. Exogenous Models take into account the zonal statistics of a given tract’s neighbors.\nWe merge our data with shape file and calculate the spatial lag of a each tract’s neighbors. The spatial lag is this case is the average population density of a given tracts of land. We also calculate the average landcover types of a given’s tracts neighbors.\nIn total, we create 8 models which we compare in order to determine the best way to predict population density with landcover data\n\n\n\n\n\nflowchart \nA[Cleaned Data] --&gt; B{No Penalty LR}\nA --&gt; C{Lasso LR}\nB --&gt; K{ours}\nB --&gt; L{sci-kit learn}\nC --&gt; G{ours}\nC --&gt; H{sci-kit learn}\nA --&gt; D{Ridge LR}\nD --&gt; I{ours}\nD --&gt; J{sci-kit learn}\nA --&gt; |Spatial Lag Pop Density| E{Endogenous}\nA --&gt; |Spatial Lag Landcover| F{Exogenous}"
  },
  {
    "objectID": "posts/final-project-post/final_project.html#abstract",
    "href": "posts/final-project-post/final_project.html#abstract",
    "title": "Project Blog Post",
    "section": "",
    "text": "Our project takes on the challenge of predicting population density in regions lacking data. Leveraging landcover image data and tract geometry, our approach involves computing zonal statistics and employing machine learning models. With this problem in mind, we employ satellite data from Connecticut due to its completeness and its potential to also be applied to other Northeastern States within the US. We create Linear Regression models and Spatial Autoregression models with our zonal statistics and tract data. We gauge their efficacy based on their mean-squared error and \\(R^2\\) value. Through this, we find that Linear Regression with No Penalty works best out of our Linear Regression models and our Endogenous Spatial Autoregression model works better than the Exogenous model. Furthermore, we conclude that Spatial Autoregression is more effective at predicting population density than Linear Regression. In regions without adequate census data, the Exogenous model would improve estimations of population density by taking into account the landcover of a given region and its neighbors. Our code and datasets are available through our Github"
  },
  {
    "objectID": "posts/final-project-post/final_project.html#introduction",
    "href": "posts/final-project-post/final_project.html#introduction",
    "title": "Project Blog Post",
    "section": "",
    "text": "In countries such as the US, there is a large and accurate amount of census data. However there are many ountries in areas where the resources for gathering census data is lesser. This is potentially due to geographic inaccessibility, political conflict, administrative failiure, and as mentioned previously, a lack of resources. Thus, we want a way to predict human populations around the world with the data of the land itself, satellite imagery. With this imaging, the geography is divided into classes which we can then use as variables for our model. Research into this topic has stagnated to a degree, however Tian et al. (2005) produced a hallmark paper which tested the effectivity of modeling population with land cover data. It found that a similar model could have “feasible” and can have “high accuracy”. They utilized Linear Regression, and also manually broke down China into even 1km by 1km cells. Because of availablity of census data, we instead used census tracts, but we continued with the idea of utilizing Linear Regression. With some exploratory graphs of Connecticut, we discovered there might be a Spatial Pattern within our data. In order to take this into account during modeling, we started researching into machine learning algorithms with a spatial component. We came across a paper by Liu, Kounadi, and Zurita-Milla (2022), which concluded that models with a spatial component, such as spatial lag, garner better results than those without. They used spatial lag, and eigvenvectors spatial filtering to predict things beyond our datasets such as soil types. Thus, we sought to create Linear Regression Models and Spatial Autoregressive models, and compare the them to see which is more effective in predicting population density based on land cover."
  },
  {
    "objectID": "posts/final-project-post/final_project.html#values-statement",
    "href": "posts/final-project-post/final_project.html#values-statement",
    "title": "Project Blog Post",
    "section": "",
    "text": "NASA in a webinar session called “Humanitarian Applications Using NASA Earth Observations” presented how satellite remote-sensing data could be useful in monitoring humanitarian conditions at refugee settlements. Human settlements could be detected through remote sensing images and therefore could be used to predict the population in a region. This talk alerted us that we still lack necessary population data in many parts of the world, but also demonstrated how remote sensing could be a powerful tool in tackling this problem and solving lack of population data in different countries. Thus, we decide to investigate the connection between remote sensing land cover data and population density in a context with better data coverage.\nThis type of model would be most beneficial by governments and government organizations. These users would most likely be hospital contractors, policy makers, emergency services providers such as ambulances and firefighers, and sociologists. Population census data is crucial for policy makers as it assists in city management so that the equitable distribution of resources can be better calculated.\nThe implications extend beyond helping users. Real people would be affected by this technology. Those who are workers in fields such as emergency service work, or school teachers who might have been over-worked previously may be relieved by the building of new hospitals and schools to compensate for population changes. However, the negative effects are also extremely real.\nImagining that this model expanded beyond the barriers of Connecticut and is being used in countries with much lower census data such as Brazil, there might be a calculation for a forestry company to continue harvesting wood from the Amazon, but they do not want to affect populations. Our algorithm calculates there are very few people in the area, as there is very dense land cover in the Amazon. This company starts to cut down trees and discovers that they are in an area of Indigenous peoples. A minority group that is already negatively affected continues to be disenfranchised. The issue of undercalculating the population density in an area can also affect the amount of resources a policymaker might provide to a region with a much greater population and lacking resources. This would also continue to negatively impact an already negatively impacted area.\nUltimately, the world would be a more equitable and sustainable place if this type of technology could assist countries lacking population data. The positive aspects of providing data where there is none provides the potential for great resource partioning, and better understanding of a countries population."
  },
  {
    "objectID": "posts/final-project-post/final_project.html#materials-and-methods",
    "href": "posts/final-project-post/final_project.html#materials-and-methods",
    "title": "Project Blog Post",
    "section": "",
    "text": "With this project being the entire state of Connecticut, we utilized landcover data, population, shape files for graphing, and synthesized data which combined our various data sets into manageable datasets suitable for modeling.\nThe bread and butter of our data stems from a 1-meter resolution landcover imagery covering the entire state of Connecticut. Derived from NAIP, the data has already been processed such that every pixel represents a certain class of landcover.\nAt over 800 MB, the dataset is too large to share via GitHub, and is downloadable by clicking on the first option at this link. This landcover dataset was one of the most complete datsets we could find, which is why we wanted to use it for our modelling.\nOur other data sources are the geometries and population data on the Census tract level for the state of Connecticut. We downloaded tract geometries directly into our Jupyter Notebook final_project.ipynb using the Pygris package, and we downloaded the population data from Social Explorer, storing it at data/population.csv.\n\n\n\nFirst, we clean and prepare our data for the model. We start by combining our Tract Geometry of CT with the Population Data of CT to form a new dataset. We utilize both the CT Landcover Data and the Tracts Data in a calculation of Zonal Statistics. This means we calculate the proportion of pixels within each tract that are of a given landcover class. This then is saved as a combined dataset which we then continue to clean by imputing values, performing more advanced Zonal Statistics, and dropping any NA Columns. From there, we are left with data ready to be used in a model.\nThe flowchart below more elegantly outlines this process\n\n\n\n\n\nflowchart LR\n  A(Population Data) --&gt; B(Tracts Data)\n  C(Tracts Geometry Data) --&gt; B(Tracts Data)\n  B --&gt; D{Zonal Statistics}\n  E(CT Landcover Data) --&gt; D{Zonal Statistics}\n  D{Zonal Statistics} --&gt; F(Combined Data)\n  F(Combined Data) --&gt; |Impute Data| G[Ready for Model]\n  F --&gt; |Additional Landcover Statistics| G[Ready for Model]\n  F --&gt; |Drop Uncommon Landcover| G[Cleaned Data]\n\n\n\n\n\n\nWe then implement three types of Linear Regression:\n\nLinear Regression with No Penalty Term\nLinear Regression with \\(\\ell_1\\) Regularization (Lasso Regression)\nLinear Regression with \\(\\ell_1\\) Regularization (Ridge Regression)\n\nBy utilizing the \\(R^2\\) and Mean Squared Error, we quantified the success of each of our models against one another as well as comparing them to sci-kit learn’s own implementations of each of these Linear Regression Models.\nFollowing Linear Regression, we then wanted to implement two types of Spatial AutoRegression:\n\nEndogenous Spatial Autoregression\nExogenous Spatial Autoregression\n\nAs our data can be plotted on a map of Connecticut, we felt it would be amiss to not explore Spatial Autogression. Through this style of model, we can take into account the spatial aspect of each tract when we are predicting. We chose both Endogenous and Exogenous Models. Endogenous Models take into account the neighboring tract population densities of a given tract. Exogenous Models take into account the zonal statistics of a given tract’s neighbors.\nWe merge our data with shape file and calculate the spatial lag of a each tract’s neighbors. The spatial lag is this case is the average population density of a given tracts of land. We also calculate the average landcover types of a given’s tracts neighbors.\nIn total, we create 8 models which we compare in order to determine the best way to predict population density with landcover data\n\n\n\n\n\nflowchart \nA[Cleaned Data] --&gt; B{No Penalty LR}\nA --&gt; C{Lasso LR}\nB --&gt; K{ours}\nB --&gt; L{sci-kit learn}\nC --&gt; G{ours}\nC --&gt; H{sci-kit learn}\nA --&gt; D{Ridge LR}\nD --&gt; I{ours}\nD --&gt; J{sci-kit learn}\nA --&gt; |Spatial Lag Pop Density| E{Endogenous}\nA --&gt; |Spatial Lag Landcover| F{Exogenous}"
  },
  {
    "objectID": "posts/final-project-post/final_project.html#acquire-tract-geometries",
    "href": "posts/final-project-post/final_project.html#acquire-tract-geometries",
    "title": "Project Blog Post",
    "section": "Acquire Tract Geometries",
    "text": "Acquire Tract Geometries\nAs a test of concept, lets utilize the pygris library to access the CT tracts information and then let’s do a simple plot to ensure it’s correct.\n\n# Download geometry\nct_tracts = tracts(state = \"CT\", cb = True, cache = True, year = 2016)\n\n# Display geometry\nfig, ax = plt.subplots()\nct_tracts.plot(ax = ax)\nplt.title(\"Tracts Cartographic Boundaries\");\n\nUsing FIPS code '09' for input 'CT'"
  },
  {
    "objectID": "posts/final-project-post/final_project.html#calculate-population-density",
    "href": "posts/final-project-post/final_project.html#calculate-population-density",
    "title": "Project Blog Post",
    "section": "Calculate Population Density",
    "text": "Calculate Population Density\nBefore we begin our journey into zonal statistics and eventually creating a predictive model, we first want to understand what the population density looks like in Connecticut. We have some general hypotheses that the areas around New Haven and Hartford are going to have higher amounts of population, and we also expect to see some small pockets of communities around Connecticut.\n\n# Import tracts population data\npop = pd.read_csv(\"../data/population.csv\")\n\n# Convert data type so join key matches\nct_tracts[\"Geo_TRACT\"] = ct_tracts[\"TRACTCE\"].astype(int)\n\n# Join attributes to geometry\ntracts = ct_tracts.merge(pop, how = \"inner\", on='Geo_TRACT')\n\n# Project tracts\ntracts = tracts.to_crs(\"EPSG:3857\")\n\n# Calculate area in KM^2\ntracts[\"Area\"] = tracts.area/1000**2\n\n# Calculate population density\ntracts[\"PopDensity\"] = tracts[\"SE_A00001_001\"]/tracts[\"Area\"]\n\n# Create map\ntracts.plot(\"PopDensity\", legend = True);"
  },
  {
    "objectID": "posts/final-project-post/final_project.html#first-steps",
    "href": "posts/final-project-post/final_project.html#first-steps",
    "title": "Project Blog Post",
    "section": "First steps",
    "text": "First steps\nHere we open our path to our file, and more importantly, we set up our data to be used in zonal statistics. .read turns our data into a Numpy Array. Following this we are going to .transform our data, which means we are going to take the pixel locations of our coordinates (row col) and map them to our spatial coordinates (x, y). These coordinate values are relative to the CRS (Coordinate Reference System) which we defined earlier as “EPSG:2234”\n\n%%script echo skipping\n#the data can be accessed from https://coastalimagery.blob.core.windows.net/ccap-landcover/CCAP_bulk_download/High_Resolution_Land_Cover/Phase_2_Expanded_Categories/Legacy_Land_Cover_pre_2024/CONUS/ct_2016_ccap_hires_landcover_20200915.zip\nraster_path = '../data/ct_2016_ccap_hires_landcover_20200915.tif'\nlandcover = rasterio.open(raster_path)\narr = landcover.read(1)\naffine = landcover.transform\n\nskipping"
  },
  {
    "objectID": "posts/final-project-post/final_project.html#performing-zonal-statistics",
    "href": "posts/final-project-post/final_project.html#performing-zonal-statistics",
    "title": "Project Blog Post",
    "section": "Performing Zonal statistics",
    "text": "Performing Zonal statistics\nIt’s as simple as importing rasterstats. We have handled the important data manipulation, and now it’s basically plug and play! One function to note is .to_crs which takes in given coordinate reference system and transforms all the points in our dataframe to match that system.\nThe rasterstats library is very good at getting information from rasters, and we can in fact gain more information by using categorical = True. This allows to see the amount of each type of pixel at a given tract.\n\n%%script echo skipping\ndf_new = zonal_stats(zone, arr, affine=affine, categorical = True)\n\nskipping\n\n\nTaking a look at our dataframe, we can confirm that each column is a type of pixel and each row is a tract\n\n%%script echo skipping\ndf_categorical = pd.DataFrame(df_new)\ndf_categorical\n\nskipping"
  },
  {
    "objectID": "posts/final-project-post/final_project.html#visualizing-zonal-stats",
    "href": "posts/final-project-post/final_project.html#visualizing-zonal-stats",
    "title": "Project Blog Post",
    "section": "Visualizing Zonal Stats",
    "text": "Visualizing Zonal Stats\nNow that we have information on the amount of each pixel at a given tract, we can find the most common pixel per tract by using the function .idxmax() which will through each row and find the column with the largest value.\n\n%%script echo skipping\ndf_categorical['max_type'] = df_categorical.idxmax(axis=1)\ncombined_df = pd.concat([tracts, df_categorical], axis=1)\ncombined_df['max_type'] = combined_df['max_type'].astype(str)\n\nskipping\n\n\n\n%%script echo skipping\ncombined_df.plot(\"max_type\", legend = True);\n\nskipping\n\n\n\nSaving this data\nThese statistics took quite a while to run, and it may be beneficial to save this data as a csv to continue running statistics in the future\n\n%%script echo skipping\n\ncombined_df.to_csv('../data/combined_data.csv', index=False)\n\nskipping"
  },
  {
    "objectID": "posts/final-project-post/final_project.html#data-preparation-1",
    "href": "posts/final-project-post/final_project.html#data-preparation-1",
    "title": "Project Blog Post",
    "section": "Data Preparation",
    "text": "Data Preparation\nFirst, we import our data.\n\n# Import and display data\ndata = pd.read_csv(\"../data/combined_data.csv\")\ndata.head()\n\n\n\n\n\n\n\n\nSTATEFP\nCOUNTYFP\nTRACTCE\nAFFGEOID\nGEOID\nNAME\nLSAD\nALAND\nAWATER\ngeometry\n...\n18\n19\n20\n21\n22\n7\n6\n0\n23\nmax_type\n\n\n\n\n0\n9\n1\n11000\n1400000US09001011000\n9001011000\n110.0\nCT\n4473567\n3841130\nPOLYGON ((-8191739.173321358 5013468.769836016...\n...\n136572.0\n423692.0\n142589.0\n1378858.0\nNaN\nNaN\nNaN\nNaN\nNaN\n2\n\n\n1\n9\n1\n20800\n1400000US09001020800\n9001020800\n208.0\nCT\n2315472\n0\nPOLYGON ((-8187432.3302968815 5025136.84023609...\n...\nNaN\nNaN\n27939.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n11\n\n\n2\n9\n1\n21400\n1400000US09001021400\n9001021400\n214.0\nCT\n1640443\n0\nPOLYGON ((-8189589.702028457 5021116.993618919...\n...\nNaN\nNaN\n13728.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n2\n\n\n3\n9\n1\n22200\n1400000US09001022200\n9001022200\n222.0\nCT\n1442382\n117063\nPOLYGON ((-8186995.178656538 5019223.193891366...\n...\nNaN\n20584.0\n80161.0\n99956.0\nNaN\nNaN\nNaN\nNaN\nNaN\n2\n\n\n4\n9\n1\n43100\n1400000US09001043100\n9001043100\n431.0\nCT\n6652660\n58522\nPOLYGON ((-8178763.436270848 5029936.759394648...\n...\nNaN\nNaN\n9940.0\n68655.0\n486.0\nNaN\nNaN\nNaN\nNaN\n11\n\n\n\n\n5 rows × 87 columns\n\n\n\nLooks like there is some missing data in tracts that contain no pixels of a certain class. Let’s impute 0 for all NaN values.\n\n# Impute 0 for missing data\nprint(\"Before imputation, there were\", pd.isnull(data.iloc[:,68:-1]).sum().sum(), \"NaN values.\")\ndata[pd.isnull(data.iloc[:,68:-1])] = 0\nprint(\"After imputation, there are\", pd.isnull(data.iloc[:,68:-1]).sum().sum(), \"NaN values.\")\ndata.head()\n\nBefore imputation, there were 5774 NaN values.\nAfter imputation, there are 0 NaN values.\n\n\n\n\n\n\n\n\n\nSTATEFP\nCOUNTYFP\nTRACTCE\nAFFGEOID\nGEOID\nNAME\nLSAD\nALAND\nAWATER\ngeometry\n...\n18\n19\n20\n21\n22\n7\n6\n0\n23\nmax_type\n\n\n\n\n0\n9\n1\n11000\n1400000US09001011000\n9001011000\n110.0\nCT\n4473567\n3841130\nPOLYGON ((-8191739.173321358 5013468.769836016...\n...\n136572.0\n423692.0\n142589.0\n1378858.0\n0.0\n0.0\n0.0\n0.0\n0.0\n2\n\n\n1\n9\n1\n20800\n1400000US09001020800\n9001020800\n208.0\nCT\n2315472\n0\nPOLYGON ((-8187432.3302968815 5025136.84023609...\n...\n0.0\n0.0\n27939.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n11\n\n\n2\n9\n1\n21400\n1400000US09001021400\n9001021400\n214.0\nCT\n1640443\n0\nPOLYGON ((-8189589.702028457 5021116.993618919...\n...\n0.0\n0.0\n13728.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n2\n\n\n3\n9\n1\n22200\n1400000US09001022200\n9001022200\n222.0\nCT\n1442382\n117063\nPOLYGON ((-8186995.178656538 5019223.193891366...\n...\n0.0\n20584.0\n80161.0\n99956.0\n0.0\n0.0\n0.0\n0.0\n0.0\n2\n\n\n4\n9\n1\n43100\n1400000US09001043100\n9001043100\n431.0\nCT\n6652660\n58522\nPOLYGON ((-8178763.436270848 5029936.759394648...\n...\n0.0\n0.0\n9940.0\n68655.0\n486.0\n0.0\n0.0\n0.0\n0.0\n11\n\n\n\n\n5 rows × 87 columns\n\n\n\nNow that we have complete data, we can calculate the proportion of pixels belonging to each class.\n\n# Calculate total number of pixels in each tract\ndata[\"sum\"] = data.iloc[:,68:-1].sum(axis = 1)\n\n# Calculate proportion of pixels belonging to each class\ndata.iloc[:,68:-2] = data.iloc[:,68:-2].div(data['sum'], axis=0)\n\n# View data\ndata.head()\n\n\n\n\n\n\n\n\nSTATEFP\nCOUNTYFP\nTRACTCE\nAFFGEOID\nGEOID\nNAME\nLSAD\nALAND\nAWATER\ngeometry\n...\n19\n20\n21\n22\n7\n6\n0\n23\nmax_type\nsum\n\n\n\n\n0\n9\n1\n11000\n1400000US09001011000\n9001011000\n110.0\nCT\n4473567\n3841130\nPOLYGON ((-8191739.173321358 5013468.769836016...\n...\n0.069327\n0.023331\n0.225616\n0.000000\n0.0\n0.0\n0.0\n0.0\n2\n6111530.0\n\n\n1\n9\n1\n20800\n1400000US09001020800\n9001020800\n208.0\nCT\n2315472\n0\nPOLYGON ((-8187432.3302968815 5025136.84023609...\n...\n0.000000\n0.012054\n0.000000\n0.000000\n0.0\n0.0\n0.0\n0.0\n11\n2317904.0\n\n\n2\n9\n1\n21400\n1400000US09001021400\n9001021400\n214.0\nCT\n1640443\n0\nPOLYGON ((-8189589.702028457 5021116.993618919...\n...\n0.000000\n0.008350\n0.000000\n0.000000\n0.0\n0.0\n0.0\n0.0\n2\n1644135.0\n\n\n3\n9\n1\n22200\n1400000US09001022200\n9001022200\n222.0\nCT\n1442382\n117063\nPOLYGON ((-8186995.178656538 5019223.193891366...\n...\n0.013289\n0.051753\n0.064533\n0.000000\n0.0\n0.0\n0.0\n0.0\n2\n1548918.0\n\n\n4\n9\n1\n43100\n1400000US09001043100\n9001043100\n431.0\nCT\n6652660\n58522\nPOLYGON ((-8178763.436270848 5029936.759394648...\n...\n0.000000\n0.001484\n0.010249\n0.000073\n0.0\n0.0\n0.0\n0.0\n11\n6698858.0\n\n\n\n\n5 rows × 88 columns\n\n\n\n\n# Separate predictors and outcome\nX = data.iloc[:,68:-2]\ny = data[\"PopDensity\"]\n\nWe had an issue where our results were not quite matching those of scikit-learn and we discovered that this was due to a way we set up our dataset. Since we have calculated the proportion of pixels in each tract belonging to each landcover class, the landcovers sum to 1 in every row. Since we create an additional column of ones in order to calculate a y-intercept for linear regression with gradient descent, this means that our y-intercept column is equal to the sum of our other columns. In other words, the constant column is linearly dependent on our other predictor columns. To address this issue, we drop some columns that seem unimportant. Specifically, these columns are mostly zero, meaning that they are not very common in Connecticut anyway.\n\n# Drop some landcovers to address issue of linear combination \nX = X[['2', '5', '11', '12', '8', '13', '14', '15', '20', '21']]"
  },
  {
    "objectID": "posts/final-project-post/final_project.html#linear-regression-with-no-penalty-term",
    "href": "posts/final-project-post/final_project.html#linear-regression-with-no-penalty-term",
    "title": "Project Blog Post",
    "section": "Linear Regression with No Penalty Term",
    "text": "Linear Regression with No Penalty Term\n\nSci-kit Learn\n\nTrain Model\nFirst, we fit a linear regression model with scikit-learn. We do this simply to verify against our own implementation of linear regression.\n\n# Fit model\n# Doing this just for the purpose of seeing what it looks like\n# We can use the results from this package to verify that our implementation is working properly\n\n#Train and test split creation\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nLR_s = LinearRegression() \n\nm = LR_s.fit(X_train, y_train)\n\nLinear regression seeks to minimize the mean squared error, so we report the mean square error from scikit-learn’s model here.\n\n# MSE\nmean_squared_error(y_train, LR_s.predict(X_train))\n\n227396.12768129486\n\n\nLet’s check the \\(R^2\\) value of our model. Recall that \\(R^2\\) is also known as the coefficient of determination, and it represents the proportion of variation in one’s outcome variable that is explained by one’s model.\n\n# R^2 value\nm.score(X_train, y_train)\n\n0.7723901708932351\n\n\nWith an \\(R^2\\) value of roughly \\(0.772\\), our ordinary least squares regression model accounts for about \\(77.2\\)% of the variation of the population densities in Connecticut’s tracts.\nLet’s inspect the y-intercept and coefficients to verify that our coefficients seem logical.\n\n# Y-intercept\nprint(\"Intercept:\", m.intercept_)\n\n# Min and max population density\nprint(\"Population Density Min:\", y_train.min())\nprint(\"Population Density Max:\", y_train.max())\n\nIntercept: 983.2395073145441\nPopulation Density Min: 0.0\nPopulation Density Max: 6084.305602883675\n\n\nSince our predictions are proportions of pixels in a tract of a given landcover, it is impossible for all of our predictors to be zero. Basically this means that no tract will be in the situation where all variables are equal to zero, leaving the y-intercept as its population density. However, in theory, in the absence of any landcover pixels, the population density would be \\(983\\) people per square kilometer. With y_train ranging from 0 to 6084, this seems somewhat reasonable.\n\n# Variable coefficients\nm.coef_\n\narray([  3409.40801231,  -2942.65854175,   -917.38563842,  -4525.6598175 ,\n          668.32452458,  -2125.96537456,  -1746.52921947,  -1576.35637606,\n       -13652.09857612,  -1417.12360532])\n\n\n\n# Columns\nX.columns\n\nIndex(['2', '5', '11', '12', '8', '13', '14', '15', '20', '21'], dtype='object')\n\n\nMost of these coefficients are negative, indicating that as the proportion of pixels representing a given landcover type increases, the population density of the tract decreases. The only positive values are the coefficient of 2, which represents developed impervious landcover, and the coefficient of 8, which represents grassland/herbaceous landcover. We definitely anticipated a positive coefficient for 2, as impervious developed surfaces like buildings and roads are a major marker of human presence. The documentation indicates that while this landcover cannot be used for tilling, it can be used for grazing, so perhaps the positive coefficient is indicative of population density associated with farming. Also, Connecticut is generally forested in rural areas, so grassy areas are likely in suburbia or near urban areas. The magnitude of 2 is much larger than 8, however, indicating that developed impervious landcover is the most important factor increasing population density.\nThe negative coefficients correspond to developed open space, mixed forest, shrub, palustrine forested wetland, palustrine scrub/shrub wetland, palustrine emergent wetland, barren land, and open water. With the exception of developed open space, these landcovers are generally not associated with population density. And developed open space does not necessitate people living in that location – people could live in one tract and commute to a tract with developed open space for recreational purposes, for example. Thus it makes sense that increased values of these variables contribute to less population density.\n\n\nTest Model\nNow that we have evaluated the basic interpretation of our model on our training data, let us check the performance of our model on our testing data. First, we calculate our predictions.\n\n# Create predictions (on test data)\npreds = LR_s.predict(X_test)\n\nLet us inspect the mean square error of our model on the testing data.\n\n# MSE\nmean_squared_error(y_test, preds)\n\n373799.85511504946\n\n\nAt \\(373,800\\), the mean squared error of our model on the testing data is much larger than the mean squared error on the training data, which was \\(227,396\\). This makes sense as our model was fit specifically to the tendencies of the training data.\nTo evaluate the explanatory power of our model, let’s also calculate the \\(R^2\\) value on our testing data.\n\n# Test R^2 value\nr2_score(y_test, preds)\n\n0.7086666350845903\n\n\nAs one might anticipate, the \\(R^2\\) value of the testing data is lower than the training data. However, at \\(0.709\\), the \\(R^2\\) of the testing data is only \\(0.064\\) lower than the \\(R^2\\) of the training data. In other words, our model explains \\(6.4\\)% less of the variation of the population density in our testing data. This is not a negligible amount, but we are still relatively satisfied with a model that explains over \\(70\\)% of the variation in population density.\n\n\n\nOur Implementation\nWe implemented ordinary linear regression with gradient descent in linear_regression.py. Let us train the model using our implementation and verify that our results roughly match those of scikit-learn.\n\nTrain Model\nFirst, we need to convert our training and testing data to the torch.tensor format to match the expected input of our model. We also add a column of ones at the end of the X training and testing data for the purposes of training our y-intercept.\n\n# convert to torch tensors\n# add column of ones for y-intercept\nX_train_torch = torch.cat((torch.tensor(X_train.values), torch.ones((X_train.shape[0], 1))), 1)\ny_train_torch = torch.tensor(y_train.values)\nX_test_torch = torch.cat((torch.tensor(X_test.values), torch.ones((X_test.shape[0], 1))), 1)\ny_test_torch = torch.tensor(y_test.values)\n\nNow that we have our data in the appropriate format, we can train our model.\n\n# fit linear regression model\nLR = LinearRegress()\nopt = GradientDescentOptimizer(LR)\n\n# initialize vector to record loss values\nloss_vec = []\n\n# fit model\nfor i in range(500000): \n    # update model\n    opt.step(X_train_torch, y_train_torch, alpha = 0.01)\n\n    # calculate and record loss\n    loss = LR.loss(X_train_torch, y_train_torch) \n    loss_vec.append(loss)\n\nLet’s inspect the evolution of our loss function (mean squared error) to verify that our model has converged to a solution.\n\n# plot the changes in loss \nplt.plot(loss_vec, color = \"slategrey\")\nlabs = plt.gca().set(xlabel = \"Iteration\", ylabel = \"Loss\")\nplt.title(\"Loss Function Throughout Model Training\");\n\n# MSE\nprint(\"Mean squared error after training:\", LR.mse(X_train_torch, y_train_torch).item())\n\nMean squared error after training: 227396.1319467965\n\n\n\n\n\n\n\n\n\nGreat! After \\(500,000\\) iterations, our mean squared error is \\(227,396.132\\), which is essentially equivalent to the mean squared error of \\(227,396.128\\) found by scikit-learn.\nLet’s inspect the y-intercept and coefficients to verify that they are similar to scikit-learn’s solution.\n\n# Y-intercept\nLR.w[-1]\n\ntensor(983.0291, dtype=torch.float64)\n\n\nThis y-intercept is also similar to the figure of \\(983.2395\\) reported by scikit-learn.\n\n# Variable coefficients\nprint(\"Coefficients:\", LR.w[:-1])\n\n# Differences in signs\nprint(\"Differences in sign:\", (torch.tensor(m.coef_)*LR.w[:-1]&lt; 0).sum().item())\n\n# Maximum difference in coefficient\nprint(\"Maximum coefficient difference:\", torch.abs((torch.tensor(m.coef_)-LR.w[:-1])).max().item())\n\nCoefficients: tensor([  3409.6334,  -2942.3605,   -917.2350,  -4527.3370,    669.5791,\n         -2126.6514,  -1721.1996,  -1578.8776, -13651.8922,  -1416.8399],\n       dtype=torch.float64)\nDifferences in sign: 0\nMaximum coefficient difference: 25.329603726808955\n\n\nOur coefficients are very similar to those from scikit-learn’s solution! All coefficients have the same sign and the maximum difference between a coefficient in our two models is \\(25\\). Considering the magnitude of the coefficients, this difference is relatively small. Thus the interpretation of our model matches the interpretation of scikit-learn’s model, making us confident that we have implemented linear regression correctly.\n\n# Compute R^2 score\nLR.r2(X_train_torch, y_train_torch)\n\ntensor(0.7724, dtype=torch.float64)\n\n\nOur \\(R^2\\) value is the same as scikit-learn’s.\n\n\nTest Model\nNow we inspect our model’s performance on the testing data.\n\n# MSE\nLR.mse(X_test_torch, y_test_torch)\n\ntensor(373801.8165, dtype=torch.float64)\n\n\nAt \\(373,802\\), our implementation’s testing MSE is very similar to scikit-learn’s \\(373,800\\), indicating similar performance. Once again, this is substantially larger than the training MSE, indicating that our model did not generalize perfectly.\n\n# R^2 value\nLR.r2(X_test_torch, y_test_torch)\n\ntensor(0.7087, dtype=torch.float64)\n\n\nScikit-learn’s testing \\(R^2\\) value was also \\(0.7087\\)! Overall, it appears that we have succesfully implemented linear regression in a manner that achieves similar results to scikit-learn."
  },
  {
    "objectID": "posts/final-project-post/final_project.html#linear-regression-with-ell_1-regularization",
    "href": "posts/final-project-post/final_project.html#linear-regression-with-ell_1-regularization",
    "title": "Project Blog Post",
    "section": "Linear Regression with \\(\\ell_1\\) Regularization",
    "text": "Linear Regression with \\(\\ell_1\\) Regularization\n\nSci-kit Learn\n\nTrain Model\nFirst, we fit the model with scikit-learn Lasso and inspect the resulting model. As before, we do this simply to verify against our own implementation.\n\n# Fit model\nLR_s_l1 = Lasso(alpha = 1)\n\nm = LR_s_l1.fit(X_train, y_train)\n\n# Report results\nprint(\"MSE:\", mean_squared_error(y_train, LR_s_l1.predict(X_train)),\n      \"\\nR^2:\", m.score(X_train, y_train),\n      \"\\nY-intercept:\", m.intercept_,\n      \"\\nCoefficients:\\n\", m.coef_)\n\nMSE: 236944.40360579122 \nR^2: 0.7628329217280919 \nY-intercept: -191.72480712350455 \nCoefficients:\n [ 4358.88007237 -1696.29039686   224.06934601    -0.\n     0.            -0.            -0.            -0.\n -6028.66936275  -279.44578393]\n\n\n\n# Columns\nX.columns\n\nIndex(['2', '5', '11', '12', '8', '13', '14', '15', '20', '21'], dtype='object')\n\n\nThe training MSE is slightly larger and the training \\(R^2\\) is slightly smaller than linear regression with no regularizer, which makes sense as we have applied a penalty to help prevent overfitting. The y-intercept is closer to \\(0\\), and many of the coefficients are equal to exactly \\(0\\), making them more interpretable: some coefficients simply do not matter! In this model, landcover 2 (developed impervious) again has a positive coefficient, and with a large magnitude, it remains the main driver in high population density. There is one other variable, 11 (mixed forest), which has a positive coefficient. Interestingly, it was negative in the other model, leading to confusion in its interpretation. But with a somewhat small magnitude, this variable overall has a minor impact on population density, only changing the population density by 224 people per square kilometer as its value increases from 0 to 1. With the \\(\\ell_1\\) regularizer, the landcovers of shrub, grassland/herbaceous, palustrine forested wetland, palustrine scrub/shrub wetland, and palustrine emergent wetland are now equal to zero. These coefficients must not have been that important to the model, as our regularizer made them have zero impact on population density. Variables with negative coefficients are developed open space, barren land, and open water, probably for the same reasons that they were negative earlier.\n\n\nTest Model\nNext, we discover whether the \\(\\ell_1\\) regularizer actually made the model generalize better to the testing data.\n\n# Create predictions (on test data)\npreds = LR_s_l1.predict(X_test)\n\n# Report results\nprint(\"MSE:\", mean_squared_error(y_test, preds),\n      \"\\nR^2:\", r2_score(y_test, preds))\n\nMSE: 390639.95954704983 \nR^2: 0.6955417389066836\n\n\nOur new MSE of \\(390,639\\) is actually larger than the MSE of \\(373,800\\) with no regularizer, indicating that the \\(\\ell_1\\) regularizer did not help our model generalize to the testing data. Furthermore, the \\(R^2\\) value was larger in the previous model, meaning that the model with no regularizer explained more variation in the outcome variable.\n\n\n\nOur Implementation\nLet’s fit linear regression with the \\(\\ell_1\\) norm with our own implementation and verify that our results match those of scikit-learn. Note that scikit-learn uses an algorithm known as coordinate descent to find their solution, but we learned about gradient descent in this class. Coordinate descent is better suited for lasso regression because it allows some coefficients to equal exactly zero. Gradient descent with the \\(\\ell_1\\) norm makes some coefficients much smaller, but does not cause any of them to equal exactly zero. To mimick their results, in our implementation we set our coefficients equal to zero if they are below a selected threshold. We allow our model \\(5000\\) iterations to begin learning the coefficients before applying this threshold.\n\nTrain Model\n\n# fit linear regression model\nLR_l1 = LinearRegress(penalty = \"l1\", lam = 1) # 1 in scikit-learn\nopt_l1 = GradientDescentOptimizer(LR_l1)\n\n# initialize vector to record loss values\nloss_vec_l1 = []\n\n# fit model\nfor i in range(50000):\n    # update model\n    opt_l1.step(X_train_torch, y_train_torch, alpha = 0.001)\n\n    # set coefs equal to zero after model has had enough learning time\n    if i &gt; 5000:\n        LR_l1.w[torch.abs(LR_l1.w) &lt; 500] = 0\n\n    # calculate and record loss\n    loss = LR_l1.loss(X_train_torch, y_train_torch) \n    loss_vec_l1.append(loss)\n\n# plot the changes in loss \nplt.plot(loss_vec_l1, color = \"slategrey\")\nlabs = plt.gca().set(xlabel = \"Iteration\", ylabel = \"Loss\")\nplt.title(\"Loss Function Throughout Model Training\");\n\n\n\n\n\n\n\n\nIt appears that our model converged to a solution with a similar loss function value! Note that the small upwards blip in the loss function occured at iteration \\(5000\\) when we began allowing our model to set some coefficients equal to zero. Let us inspect our results and compare them to scikit-learn’s output.\n\n# Report results\nprint(\"MSE:\", LR_l1.mse(X_train_torch, y_train_torch).item(),\n      \"\\nR^2:\", LR_l1.r2(X_train_torch, y_train_torch),\n      \"\\nY-intercept:\", LR_l1.w[-1],\n      \"\\nCoefficients:\\n\", LR_l1.w[:-1],\n      \"\\nDifferences in sign:\", (torch.tensor(m.coef_)*LR_l1.w[:-1]&lt; 0).sum().item(),\n      \"\\nMaximum coefficient difference:\", torch.abs((torch.tensor(m.coef_)-LR_l1.w[:-1])).max().item())\n\nMSE: 233736.21710488532 \nR^2: tensor(0.7660, dtype=torch.float64) \nY-intercept: tensor(0., dtype=torch.float64) \nCoefficients:\n tensor([ 4257.6646, -1977.7857,     0.0000,     0.0000,     0.0000,     0.0000,\n            0.0000,     0.0000, -7781.5773,  -551.3119], dtype=torch.float64) \nDifferences in sign: 0 \nMaximum coefficient difference: 1752.9079262978257\n\n\nOur model’s MSE of \\(233,736\\) is slightly smaller than scikit-learn’s MSE of \\(236,944\\) and our model’s \\(R^2\\) of \\(0.7660\\) is slightly larger than scikit-learn’s \\(R^2\\) of \\(0.7628\\), indicating that our linear regression model with the \\(\\ell_1\\) norm performed marginally better than theirs. This difference could have occured due to differences in the optimizer and the number of training iterations. Additionally, these MSE and \\(R^2\\) metrics are both slightly worse than what our implementation achieved with no regularizer, which makes sense as we are attempting to prevent overfitting.\nOne should note that our workaround for setting coefficients equal to zero is not ideal for several reasons. First, we hard-coded a certain threshold for choosing coefficients to set equal to zero, as well as a certain number of iterations at which to begin checking for these low-magnitude coefficients. Most users probably do not want to decide on such a threshold. Second, our method did not exactly replicate the output from scikit-learn. Adjusting our parameters to exactly reproduce the coefficients set to zero proved difficult, and the best we were able to do involved setting the y-intercept and landcover 11 equal to zero, while they were nonzero in scikit-learn’s solution. Landcover 11 represents mixed forest and was the one coefficient with a somewhat counterintuitive value in scikit-learn’s model, so in terms of interpretation, our new model still makes sense. All coefficients have the same sign as scikit-learn’s model with similar magnitudes, making us confident that our model is successfully describing the situation, despite the minor discrepancies.\n\n\nTest Model\n\n# Report results\nprint(\"MSE:\", LR_l1.mse(X_test_torch, y_test_torch),\n      \"\\nR^2:\", LR_l1.r2(X_test_torch, y_test_torch))\n\nMSE: tensor(384765.6761, dtype=torch.float64) \nR^2: tensor(0.7001, dtype=torch.float64)\n\n\nThese values are pretty similar to the ones we have seen already. At \\(384,766\\), our implementation’s MSE is less than scikit-learn’s \\(390,640\\), and at \\(0.7001\\), our implementation’s \\(R^2\\) is slightly more than scikit-learn’s \\(0.6955\\). This means that our model generalized slightly better to the testing data, in addition to performing better on the training data. Again, this can likely be explained by differences in the optimization method and the number of training iterations.\nFurthermore, this MSE is slightly larger than the \\(373,802\\) figure returned by our implementation of linear regression with no penalty term, and this \\(R^2\\) is slighly smaller than the \\(0.7087\\) figure, indicating that linear regression with the \\(\\ell_1\\) penalty did not generalize better to the testing data."
  },
  {
    "objectID": "posts/final-project-post/final_project.html#linear-regression-with-ell_2-regularization",
    "href": "posts/final-project-post/final_project.html#linear-regression-with-ell_2-regularization",
    "title": "Project Blog Post",
    "section": "Linear Regression with \\(\\ell_2\\) Regularization",
    "text": "Linear Regression with \\(\\ell_2\\) Regularization\n\nSci-kit Learn\n\nTrain Model\nFirst, we fit the model with scikit-learn Ridge and inspect the resulting model. As before, we do this to assess the validity of our own implementation.\n\n# Fit model\nLR_s_l2 = Ridge(alpha = .1)\n\nm = LR_s_l2.fit(X_train, y_train)\n\n# Report results\nprint(\"MSE:\", mean_squared_error(y_train, LR_s_l2.predict(X_train)),\n      \"\\nR^2:\", m.score(X_train, y_train),\n      \"\\nY-intercept:\", m.intercept_,\n      \"\\nCoefficients:\\n\", m.coef_)\n\nMSE: 235049.69085940512 \nR^2: 0.7647294150800621 \nY-intercept: 69.8570955883946 \nCoefficients:\n [ 4146.87047622 -2058.81157194     6.57006522 -1039.66258053\n   107.03266863  -815.93549227  -127.78253829  -231.19197573\n -6438.07336424  -692.08973348]\n\n\n\n# Columns\nX.columns\n\nIndex(['2', '5', '11', '12', '8', '13', '14', '15', '20', '21'], dtype='object')\n\n\nThe training MSE is larger and the training \\(R^2\\) is smaller than scikit-learn’s linear regression with no regularizer. We anticipated this would be true in comparison to the no regularizer model as the penalty term helps prevent overfitting. It appears that with our chosen parameters, lasso regression performed better than ridge regression in terms of both MSE and \\(R^2\\), but this will change depending on the selected value for parameters.\nThe y-intercept and all coefficients except for landcover 2 are smaller than they were under linear regression without regularization, indicating that the regularization method has been successful in decreasing the magnitude of our coefficients. None of the coefficients are equal to exactly zero, but that is to be expected when working with the \\(\\ell_2\\) penalty.\nThe sign of every coefficient in this model is the same as in the original linear regression model except for landcover 11 (mixed forest), which is now positive and was also positive under lasso regression. However, the magnitude of this coefficient is really small; at 6.57, a location’s population density only changes by 6.57 people per square kilometer as the proportion of pixels represented by mixed forest increases from 0 to 1.\n\n\nTest Model\n\n# Create predictions (on test data)\npreds = LR_s_l2.predict(X_test)\n\n# Report results\nprint(\"MSE:\", mean_squared_error(y_test, preds),\n      \"\\nR^2:\", r2_score(y_test, preds))\n\nMSE: 387635.2059385676 \nR^2: 0.6978835936921313\n\n\nOn the testing data, our MSE of \\(387,635\\) is similar to the result of \\(390,640\\) with the \\(\\ell_1\\) regularizer but larger than the MSE of \\(373,800\\) with no regularizer, indicating that the \\(\\ell_2\\) regularizer also did not help our model generalize to the testing data better than unregularized linear regression. The \\(R^2\\) value was also larger in linear regression, meaning that the model without regularization explained more variation in the outcome variable.\n\n\n\nOur Implementation\n\nTrain Model\nLet’s fit linear regression with the \\(\\ell_1\\) norm with our own implementation and verify that our results are reasonably similar to those of scikit-learn.\n\n# fit linear regression model\nLR_l2 = LinearRegress(penalty = \"l2\", lam = .1/X_train_torch.shape[0]) \nopt_l2 = GradientDescentOptimizer(LR_l2)\n\n# initialize vector to record loss values\nloss_vec_l2 = []\n\n# fit model\nfor i in range(1000000): \n    # update model\n    opt_l2.step(X_train_torch, y_train_torch, alpha = 0.00001)\n\n    # calculate and record loss\n    loss = LR_l2.loss(X_train_torch, y_train_torch) \n    loss_vec_l2.append(loss)\n\n# plot the changes in loss \nplt.plot(loss_vec_l2, color = \"slategrey\")\nlabs = plt.gca().set(xlabel = \"Iteration\", ylabel = \"Loss\")\nplt.title(\"Loss Function Throughout Model Training\");\n\n\n\n\n\n\n\n\n\nm.coef_\n\narray([ 4146.87047622, -2058.81157194,     6.57006522, -1039.66258053,\n         107.03266863,  -815.93549227,  -127.78253829,  -231.19197573,\n       -6438.07336424,  -692.08973348])\n\n\n\n# Report results\nprint(\"MSE:\", LR_l2.mse(X_train_torch, y_train_torch).item(),\n      \"\\nR^2:\", LR_l2.r2(X_train_torch, y_train_torch),\n      \"\\nY-intercept:\", LR_l2.w[-1],\n      \"\\nCoefficients:\\n\", LR_l2.w[:-1],\n      \"\\nDifferences in sign:\", (torch.tensor(m.coef_)*LR_l2.w[:-1]&lt; 0).sum().item(),\n      \"\\nMaximum coefficient difference:\", torch.abs((torch.tensor(m.coef_)-LR_l2.w[:-1])).max().item())\n\nMSE: 246031.0025521462 \nR^2: tensor(0.7537, dtype=torch.float64) \nY-intercept: tensor(-258.6768, dtype=torch.float64) \nCoefficients:\n tensor([ 4417.6162, -1821.4942,   373.5831,  -372.9745,  -251.1601,  -436.8005,\n          -43.2542,  -109.9865, -2181.9550,  -541.2076], dtype=torch.float64) \nDifferences in sign: 1 \nMaximum coefficient difference: 4256.118320893194\n\n\n\nX.columns\n\nIndex(['2', '5', '11', '12', '8', '13', '14', '15', '20', '21'], dtype='object')\n\n\nFirst of all, our implementation does not generate identical output to scikit-learn’s implementation. In order to make our implementation converge to a solution, we needed to make \\(\\lambda\\) far smaller than in their implementation. This may have occurred because we are using the optimization technique of gradient descent, but scikit-learn has implemented a number of more complex techniques and automatically detects which one to use depending on the dataset it receives as input. It is also possible that they implemented their loss function as the sum of squared error rather than the mean squared error. If this is the case, then dividing our \\(\\lambda\\) by the number of observations should theoretically produce identical results. In the code above, we opt for this implementation; however, it should be noted that we have not confirmed whether scikit-learn actually uses the sum of squares in their loss function. Even with this modification, our model has converged to a different solution than theirs, for reasons we have not uncovered.\nAlthough our results are different, they are not drastically different. Our MSE is \\(246,030\\) rather than \\(235,050\\) and our \\(R^2\\) is \\(0.7537\\) rather than \\(0.7647\\), differences that are not ideal but also not terrible. All coefficients have the same sign as their solution except for 8 (grassland/herbaceous). In all prior models, the coefficient of landcover 8 has been positive or zero, but in this model, it is negative! This confuses the interpretation of landcover 8, but with only one discrepancy it does not necessarily ring alarm bells. Perhaps if we had the time to confirm scikit-learn’s loss function and implement the same optimization method we would achive more similar results.\n\n\nTest Model\n\n# Report results\nprint(\"MSE:\", LR_l2.mse(X_test_torch, y_test_torch),\n      \"\\nR^2:\", LR_l2.r2(X_test_torch, y_test_torch))\n\nMSE: tensor(399693.2117, dtype=torch.float64) \nR^2: tensor(0.6885, dtype=torch.float64)\n\n\nAt \\(399,692\\), our implementation’s MSE is more than scikit-learn’s \\(387,635\\) as well as all prior results. And at \\(0.6885\\), our implementation’s \\(R^2\\) is less than scikit-learn’s \\(0.6979\\) and all other results. We could achieve better results by modifying our parameter values, but we were unable to identically reproduce the output of scikit-learn. Overall, our results indicate that for this problem, regularization does not lead to improved performance on the testing data, although it may facilitate interpretation of coefficients."
  },
  {
    "objectID": "posts/final-project-post/final_project.html#discussion-of-linear-regression",
    "href": "posts/final-project-post/final_project.html#discussion-of-linear-regression",
    "title": "Project Blog Post",
    "section": "Discussion of Linear Regression",
    "text": "Discussion of Linear Regression\nIn linear regression, a major assumption is that all observations are independent of each other. However, when working with spatial data, nearby observations are often similar, such that observations are not independent if they are in close proximity to each other. In order to determine whether our model suffers from such spatial dependence, we will fit a linear regression model on the entire dataset and produce a map of our model’s residuals. We opt for linear regression without regularization due to its higher performance in the work above.\n\n# convert to torch tensors\n# add column of ones for y-intercept\nX_torch = torch.cat((torch.tensor(X.values), torch.ones((X.shape[0], 1))), 1)\ny_torch = torch.tensor(y.values)\n\n# fit linear regression model\nLR_full = LinearRegress()\nopt_full = GradientDescentOptimizer(LR_full)\n\n# fit model\nfor i in range(500000): \n    # update model\n    opt_full.step(X_torch, y_torch, alpha = 0.01)\n\n    # calculate and record loss\n    loss = LR_full.loss(X_torch, y_torch) \n\n\n# calculate residuals\nresid = (y_torch - LR_full.pred(X_torch))\n\n# add residual column to tracts\ntracts[\"resid\"] = resid\n\n# specify that color ramp should be centered at 0\ndivnorm = TwoSlopeNorm(vmin=-3000, vcenter=0., vmax = 3000)\n\n# create map\nresid_map = tracts.plot(\"resid\", legend = True, cmap = \"seismic\", norm = divnorm, figsize = (8,8))\nplt.title(\"Residual Map\");\n\n\n\n\n\n\n\n\nIn an ideal scenario with spatially independent observations, the values of residuals would be distributed randomly throughout the map. However, with clear clusters of red and blue, our model visually appears to be making similar errors in nearby places. In other words, our residuals suffer from spatial autocorrelation. This may occur because the population density in one tract influences the population density in another tract; similarly, the landcover in one tract may influence the population density in a neighboring tract. Fortunately, there exists an entire field of spatial statistics dedicated to addressing issues of spatial autocorrelation. In the following section, we will employ one technique, known as spatial lag regression, in order to account for spatial dependence and hopefully improve our results. Before continuing to our section on spatial autoregression, we first perform cross-validation on linear regression and report the average root mean squared error (RMSE) in order to compare our results to our autoregressive results. We will opt for scikit-learn’s linear regression class since it is faster and achieves identical results to ours.\n\n# define model\nLR = LinearRegression() \n\n# define scoring function\n# this is just required in order to use scikit-learn's cross_val_score function\n# basically they multiply the MSE by -1, so we need to account for that afterwards\nmse_score = make_scorer(mean_squared_error, greater_is_better = False)\n\n# cross validation\ncv_scores_LR = cross_val_score(estimator = LR, X = X, y = y, scoring = mse_score, cv = 4)\n\n# compute average RMSE\nnp.sqrt(-1*cv_scores_LR).mean()\n\n503.0545511056982\n\n\nWith regular linear regression, we have achieved an average cross-validation RMSE of \\(503\\) people per square kilometer. Let’s see if accounting for space can improve our results!"
  },
  {
    "objectID": "posts/final-project-post/final_project.html#data-processing-and-exploration",
    "href": "posts/final-project-post/final_project.html#data-processing-and-exploration",
    "title": "Project Blog Post",
    "section": "Data Processing and Exploration",
    "text": "Data Processing and Exploration\nIn this spatial autoregression model, we adopt queen criterion to construct spatial continuity weight matrix. The queen criterion defines neighbors as spatial units sharing a common edge or a common vertex. This means that in our model, we will add the features and characteristics of the neighboring tracts as part of the prediction variables.\nTo find the weight matrix, we need to introduce geometry to our dataset. Here, I am merging the csv file to a shapefile and convert the merged data to a GeoDataFrame format. Later, I calculate the queen spatial continuity matrix using the libpysal pacakge. Using the spatial weight continuity matrix, we can then calculate the spatial lag data of population density, which is the mean population density of the neighboring tracts.\n\n# import shapefile\n# need separate shapefile because the one form pygris didn't cooperate with the weights matrix functions\ndata = pd.read_csv(\"../data/combined_data.csv\")\ngdf = gpd.read_file('../data/tl_2016_09_tract.shp')\n\n# create merge columns\ngdf['TRACTCE'] = gdf['TRACTCE'].astype(int)\ndata['TRACTCE'] = data['TRACTCE'].astype(int)\n\n# merge csv with shapfile using TRACTCE\nmerged_gdf = gdf.merge(data, on='TRACTCE', how='left')\n\n# make merged_gdf into geo dataframe\nmerged_gdf = gpd.GeoDataFrame(merged_gdf)\n\n# drop out all rows that have no population density\nmerged_gdf = merged_gdf.dropna(subset=['PopDensity'], axis=0)\n\n# clean tracts that have truncated data on population density\nmerged_gdf = merged_gdf[merged_gdf['PopDensity'] != 0]\nmerged_gdf = merged_gdf[merged_gdf['TRACTCE'] != 194202]\n\n# define the geometry_x column to be the geometry feature \nmerged_gdf.set_geometry(\"geometry_x\", inplace=True)\n\n# calculate Queen's neighbor weights for each tracts\nw = lp.weights.Queen.from_dataframe(merged_gdf)\nw.transform = 'R'\n\n# compute spatial lag of population density\nmerged_gdf['spatial_lag_PopDens'] = lp.weights.lag_spatial(w, merged_gdf['PopDensity'])\n\n# calculate the mean pop density of each tract's neighbors\n#merged_gdf['avg_neighbor_density'] = merged_gdf.groupby('TRACTCE')['spatial_lag'].transform('mean')\nmerged_gdf['PopDensity'] = merged_gdf['PopDensity'].astype(float)\n\n# download merged_gdf to csv file\nmerged_gdf.to_csv('../data/merged_gdf.csv', index=False)\n\n/tmp/ipykernel_18572/2255761594.py:27: FutureWarning: `use_index` defaults to False but will default to True in future. Set True/False directly to control this behavior and silence this warning\n  w = lp.weights.Queen.from_dataframe(merged_gdf)\n\n\nNext, we want to perform a spatial autocorrelation evaluation using Global Moran’s I index. This evaluation assesses the spatial distribution characteristic of the entire region. We plot the scatter plot between the population denisty and mean population denisty of tract’s neighbors. The Global Moran’s I index, if we do not delve into its mathematical details, is the slope of the best fit line between these two numbers. In our case, we calculated the Moran’s I index to be 0.6. Together with the distribution of the scatter plot, we believe that population density of the neighboring tracts are dependent. We also want to inspect the spatial association at a local scale. The color of each tract is based on its own population density and the population density of its surrounding tracts.\nMoran’s Scatterplot has four categories: High-High, High-Low, Low-High, Low-Low. High/low before the dash means whether the tract has a populuation density that is higher/lower than the mean overall population density. High/low after the dash means whether the tract’s neighbors population denisty is above/below the average population density. After categorization, we map the tracts to inspect the distribution of the tracts’ categories. We find that High-High tracts are usually in urban areas, Low-High tracts are usually suburbs, High-Low tracts are typically towns in the rural area, and Low-Low are rural tracts. Therefore, we believe that by taking into account the characteristics of the target tract’s neighboring tract, we are able to predict population density better than ordinary least square regression.\n\n# read data\nmerged_csv_moran = pd.read_csv(\"../data/merged_gdf.csv\", usecols=['PopDensity', 'spatial_lag_PopDens', \"Geo_NAME\"]).dropna()\n\n# Extract x and y columns from the DataFrame\nx = merged_csv_moran['PopDensity'].values.reshape(-1, 1)  # Reshape to make it a 2D array for scikit-learn\ny = merged_csv_moran['spatial_lag_PopDens'].values\n\n# Calculate the average for 'spatial_lag_PopDens' and 'PopDensity'\np = merged_csv_moran['spatial_lag_PopDens'].mean()\nq = merged_csv_moran['PopDensity'].mean()\n\n# Categorize the rows based on conditions\nmerged_csv_moran['category'] = 0  # Initialize category column\nmerged_csv_moran.loc[(merged_csv_moran['spatial_lag_PopDens'] &gt;= p) & (merged_csv_moran['PopDensity'] &gt;= q), 'category'] = 'High-High'\nmerged_csv_moran.loc[(merged_csv_moran['spatial_lag_PopDens'] &gt;= p) & (merged_csv_moran['PopDensity'] &lt; q), 'category'] = 'Low-High'\nmerged_csv_moran.loc[(merged_csv_moran['spatial_lag_PopDens'] &lt; p) & (merged_csv_moran['PopDensity'] &gt;= q), 'category'] = 'High-Low'\nmerged_csv_moran.loc[(merged_csv_moran['spatial_lag_PopDens'] &lt; p) & (merged_csv_moran['PopDensity'] &lt; q), 'category'] = 'Low-Low'\n\n# Calculate the average for 'spatial_lag_PopDens' and 'PopDensity'\np = merged_csv_moran['spatial_lag_PopDens'].mean()\nq = merged_csv_moran['PopDensity'].mean()\n\n# Define custom colors for categories\ncolors = {'High-High': '#F47E3E', 'Low-Low': '#0FA3B1', 'Low-High': '#D9E5D6', 'High-Low': '#DCC156'}\n\n# Create a scatter plot of x vs y\nscatter = plt.scatter(x, y, color=merged_csv_moran['category'].map(colors))\n\n# Fit a linear regression model\nmodel = LinearRegression()\nmodel.fit(x, y)\n\n# Get the slope and intercept of the fitted line\nslope = model.coef_[0]\nintercept = model.intercept_\n\n# Plot the fitted line\nplt.plot(x, model.predict(x), color='red', label=f'Linear Regression (y = {slope:.2f}x + {intercept:.2f})')\n\n# Add labels and title\nplt.xlabel('Population Density')\nplt.ylabel('Spatial Lag Density')\nplt.title(\"Moran's I = 0.60\")\n\n# Create legend entries manually\nlegend_patches = [\n    Patch(color=color, label=label) for label, color in colors.items()\n]\n\n# Add the legend with custom entries and regression equation\nplt.legend(handles=legend_patches + [scatter, plt.Line2D([0], [0], color='red', label=f'(y = {slope:.2f}x + {intercept:.2f})')])\n\n# Draw horizontal and vertical dashed line at y = p\nplt.axhline(y=p, color='gray', linestyle='--')\nplt.axvline(x=q, color='gray', linestyle='--')\n\n# Show plot\nplt.show()\n\n/tmp/ipykernel_18572/4162410205.py:14: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value 'High-High' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  merged_csv_moran.loc[(merged_csv_moran['spatial_lag_PopDens'] &gt;= p) & (merged_csv_moran['PopDensity'] &gt;= q), 'category'] = 'High-High'\n/tmp/ipykernel_18572/4162410205.py:51: MatplotlibDeprecationWarning: An artist whose label starts with an underscore was passed to legend(); such artists will no longer be ignored in the future.  To suppress this warning, explicitly filter out such artists, e.g. with `[art for art in artists if not art.get_label().startswith('_')]`.\n  plt.legend(handles=legend_patches + [scatter, plt.Line2D([0], [0], color='red', label=f'(y = {slope:.2f}x + {intercept:.2f})')])\n\n\n\n\n\n\n\n\n\n\n# Calculate the average for 'spatial_lag_PopDens' and 'PopDensity'\np = merged_gdf['spatial_lag_PopDens'].mean()\nq = merged_gdf['PopDensity'].mean()\n\n# Categorize the rows based on conditions\nmerged_gdf['category'] = 0  # Initialize category column\nmerged_gdf.loc[(merged_gdf['spatial_lag_PopDens'] &gt;= p) & (merged_gdf['PopDensity'] &gt;= q), 'category'] = 'High-High'\nmerged_gdf.loc[(merged_gdf['spatial_lag_PopDens'] &gt;= p) & (merged_gdf['PopDensity'] &lt; q), 'category'] = 'Low-High'\nmerged_gdf.loc[(merged_gdf['spatial_lag_PopDens'] &lt; p) & (merged_gdf['PopDensity'] &gt;= q), 'category'] = 'High-Low'\nmerged_gdf.loc[(merged_gdf['spatial_lag_PopDens'] &lt; p) & (merged_gdf['PopDensity'] &lt; q), 'category'] = 'Low-Low'\n\n# Define custom colors for categories\ncolors = {'High-High': '#F47E3E', 'Low-Low': '#0FA3B1', 'Low-High': '#D9E5D6', 'High-Low': '#DCC156'}\n\n# Plot the map using custom colors\nfig, ax = plt.subplots(figsize=(10, 10))\nmerged_gdf.plot(column='category', ax=ax, color=merged_gdf['category'].map(colors), legend=True)\nplt.title('Map of Moran Scatterplot Quadrants')\nplt.show()\n\n/tmp/ipykernel_18572/3545356962.py:7: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value 'High-High' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  merged_gdf.loc[(merged_gdf['spatial_lag_PopDens'] &gt;= p) & (merged_gdf['PopDensity'] &gt;= q), 'category'] = 'High-High'\n/tmp/ipykernel_18572/3545356962.py:17: UserWarning: Only specify one of 'column' or 'color'. Using 'color'.\n  merged_gdf.plot(column='category', ax=ax, color=merged_gdf['category'].map(colors), legend=True)\n\n\n\n\n\n\n\n\n\nInstead of using all possible land cover types, we are going to use land cover types that are more common among all tracts in CT for density prediction. The land cover types we selected are the same as the ones in linear regression section.\n\n# All landcover types\nall_landcover = ['2', '5', '8', '11', '12', '13', '14', '15', '17', '18', '19', '20', '21', '22', '7', '6', '0', '23']\nall_landcover_pct = ['2pct', '5pct', '8pct', '11pct', '12pct', '13pct', '14pct', '15pct', '17pct', '18pct', '19pct', '20pct', '21pct', '22pct', '7pct', '6pct', '0pct', '23pct']\n\n# Select landcover types\nlandcover_types = ['2', '5', '11', '12', '8', '13', '14', '15', '20', '21'] #, '22', '7', '8', '13', '14', '15', '20', '21'\nlandcover_pct = ['2pct', '5pct', '11pct', '12pct', '8pct', '13pct', '14pct', '15pct', '20pct', '21pct'] # , '22pct', '7pct', '8pct', '13pct', '14pct', '15pct', '20pct', '21pct'\n\n# Merge them into our data\nmerged_gdf['sum'] = merged_gdf[all_landcover].sum(axis=1)\nmerged_gdf[all_landcover_pct] = merged_gdf[all_landcover].div(merged_gdf['sum'], axis=0).multiply(100).astype(float)\n\n# Download merged_gdf to csv file optionally \n#merged_gdf.to_csv('merged_gdf_saved.csv', index=False)"
  },
  {
    "objectID": "posts/final-project-post/final_project.html#spatial-lag-regression",
    "href": "posts/final-project-post/final_project.html#spatial-lag-regression",
    "title": "Project Blog Post",
    "section": "Spatial Lag Regression",
    "text": "Spatial Lag Regression\n\nEndogenous vs. Exogenous: What’s the Difference?\nThere are two types of spatially lagged regression models. The first one is spatially lagged endogenous regression model. The endogenous model includes the spatial lagged value of the target variable as one of the explanatory variables for regression. In our case, the population density of a tract’s neighbor is part of the variables we use to predict the population density of the tract.\nThe second type of spatially lagged regression model is spatially lagged exogenous regression model. Instead of taking into account the population density, our target variable, of the neighboring tracts, the exogenous model considers the explanatory variables of the tract’s surroundings. In our case, the spatially lagged exogenous model adds neighbors’ land type information to the model. We will calculate the spatial lagged value of each land cover type for all tracts and include them as part of the predictor variables.\nWe first fit both models to the entirety of CT and map their residuals on each tract. First, we fit the endogenous model.\n\n# Endogenous model: consider spatial lag population denisty\npredictor = landcover_pct + ['spatial_lag_PopDens']\n\n# Get explanatory variables and target variable\nX_merged_gdf = merged_gdf[predictor].values\ny_merged_gdf = merged_gdf['PopDensity'].values.reshape(-1, 1)\n\n# Create, fit, and predict with Linear Regression\nmodel = LinearRegression()\nmodel.fit(X_merged_gdf, y_merged_gdf)\ny_pred = model.predict(X_merged_gdf)\n\n# Calculate residuals \nresiduals = y_merged_gdf - y_pred\nmerged_gdf['residuals'] = residuals\n\n# Remove Spatial lag so that our Exogenous model does not take this into account\nmerged_gdf.drop(columns=['spatial_lag_PopDens'], inplace=True)\n\nNext, we fit the exogenous model.\n\n# Exogenous model: consider\nexo_predictor = landcover_pct + ['lag_2pct', 'lag_5pct', 'lag_11pct', 'lag_12pct', 'lag_8pct', 'lag_13pct', 'lag_14pct', 'lag_15pct', 'lag_20pct', 'lag_21pct'] \n\nfor i in range(len(landcover_pct)):\n        merged_gdf['lag_' + landcover_pct[i]] = lp.weights.lag_spatial(w, merged_gdf[landcover_pct[i]])\n\n# Get explanatory variables and target variable\nX_merged_gdf_exo = merged_gdf[exo_predictor].values\ny_merged_gdf_exo = merged_gdf['PopDensity'].values.reshape(-1, 1)\n\n#Create, fit, and predict with Linear Regression\nmodel_exo = LinearRegression()\nmodel_exo.fit(X_merged_gdf_exo, y_merged_gdf_exo)\ny_pred_exo = model_exo.predict(X_merged_gdf_exo)\n\n#Calculate Residuals and make new column\nresiduals_exo = y_merged_gdf_exo - y_pred_exo\nmerged_gdf['residuals_exo'] = residuals_exo\n\nNow, we visualize the map of residuals for both models.\n\n# Define the colors for the custom colormap\ncolors = [(0, 'brown'), (0.5, 'white'), (1, 'green')]  # Position 0 is brown, position 0.5 is white, position 1 is green\n\n# Create the colormap\ncmap = LinearSegmentedColormap.from_list('custom_cmap', colors)\n\n# Determine the range of residuals to be used for normalization\nresiduals_max = max(abs(merged_gdf['residuals_exo'].max()), abs(merged_gdf['residuals'].max()))\nvmax = residuals_max * 0.75  # Adjust the factor as needed\n\n# Create a normalization object\nnorm = Normalize(vmin=-vmax, vmax=vmax)\n\n# First graph\nfig, axes = plt.subplots(1, 2, figsize=(20, 10))  # Create a figure with 1 row and 2 columns\n\n# Graph 1 - Exogenous variables\nmerged_gdf.plot(column='residuals_exo', cmap=cmap, legend=True, ax=axes[0], vmax=vmax, norm=norm)\naxes[0].set_title('Spatial Distribution of Residuals (Exogenous)')\naxes[0].set_xlabel('Longitude')\naxes[0].set_ylabel('Latitude')\n\n# Graph 2 - Spatial lag of PopDensity\nmerged_gdf.plot(column='residuals', cmap=cmap, legend=True, ax=axes[1], vmax=vmax, norm=norm)\naxes[1].set_title('Spatial Distribution of Residuals (Endogenous)')\naxes[1].set_xlabel('Longitude')\naxes[1].set_ylabel('Latitude')\n\nplt.show()\n\n\n\n\n\n\n\n\n\nnum_bins = 50\nhist_range = (0, 2000)\n\n# Create subplots with two columns\nfig, axs = plt.subplots(1, 2, figsize=(12, 6))\n\n# Plot the first histogram\naxs[0].hist(merged_gdf['residuals_exo'], bins=num_bins, range=hist_range, color='green')\naxs[0].set_xlabel('Absolute Residual')\naxs[0].set_ylabel('Number of Rows')\naxs[0].set_title('Distribution of Absolute Residuals (exogenous)')\n\n# Plot the second histogram\naxs[1].hist(merged_gdf['residuals'], bins=num_bins, range=hist_range, color='green')\naxs[1].set_xlabel('Absolute Residual')\naxs[1].set_ylabel('Number of Rows')\naxs[1].set_title('Distribution of Absolute Residuals (endogenous)')\n\n# Adjust layout\nplt.tight_layout()\n\n# Show plots\nplt.show()\n\n\n\n\n\n\n\n\nThe exogenous spatial lag residual map is on the left and the endogenous spatial lag residual map is on the right. Qualitatively assessing the these two maps, we see both models tend to underestimate the population density in urban areas. It is reasonable as land cover data is only two dimensional and does not account for the vertical height of the buildings. We also see slightly differences in prediction of rural areas between Exogenous and Endogenous. Endogenous is more accurate in rural areas as the landcover is not a sole factor of prediction, and it instead takes into account the population density of the tracts around it. Exogenous is slightly more inaccurate in these regions for the lack of this parameter. Both models tend to have a better performance at predicting density in less populated areas (Low-Low tracts).\nContinuing to explore, we created two residual histograms. We noted that they are similar to our map of CT and do not present any new pattern.\nTo explore a bit deeper into our dataset, we will look at a snapshot of our map around Hartford and its neighboring cities.\n\n# Create a normalization object\nnorm = Normalize(vmin=-2000, vmax=2000)\n\n# First graph\nfig, axes = plt.subplots(1, 2, figsize=(20, 10))  # Create a figure with 1 row and 2 columns\n\n# Graph 2 - Exogenous variables\nmerged_gdf.plot(column='residuals_exo', cmap=cmap, legend=True, ax=axes[0], vmax=vmax, norm=norm)\naxes[0].set_title('Spatial Distribution of Residuals Near Hartford (Exogenous)')\naxes[0].set_xlabel('Longitude')\naxes[0].set_ylabel('Latitude')\n\n# Graph 1 - Spatial lag of PopDensity\nmerged_gdf.plot(column='residuals', cmap=cmap, legend=True, ax=axes[1], vmax=vmax, norm=norm)\naxes[1].set_title('Spatial Distribution of Residuals Near Hartford (Endogenous)')\naxes[1].set_xlabel('Longitude')\naxes[1].set_ylabel('Latitude')\n\naxes[0].set_ylim([41.6, 41.8])\naxes[0].set_xlim([-72.9, -72.5])\naxes[1].set_ylim([41.6, 41.8])\naxes[1].set_xlim([-72.9, -72.5])\n\nplt.show()\n\n\n\n\n\n\n\n\nOne area we were particularly curious about was Hartford as it is a large urban hub in the central of Connecticut. We noticed that we were grossly underestimating densely populated areas, which makes sense as they are relatively large outliers from the rest of the relatively lower population and spread out suburban areas of Connecticut. However, we were better at calculating more densely populated areas with the Endogenous model. We hypothesize this is due to the fact that Endogenous Models inherently take into account the population densities of neighboring tracts. Thus, there is a greater likelihood that the model will “self-correct” by knowing the population of its neighbors."
  },
  {
    "objectID": "posts/final-project-post/final_project.html#training-and-testing-cross-validation",
    "href": "posts/final-project-post/final_project.html#training-and-testing-cross-validation",
    "title": "Project Blog Post",
    "section": "Training and Testing Cross Validation",
    "text": "Training and Testing Cross Validation\nFor training and testing, we need to separate the data into two. Due to the spatial dependence of tracts, we cannot randomly select tracts from the dataset and assign them to either training or testing data because neighboring tracts will not be in the same dataset. Therefore, to minimize the rupture of spatial relations, we decide to separate training and testing data by neighboring counties to ensure that all tracts in training and testiing data are countinuous. Later, we perform for loops on each set of training and testing data and calculate their mean RMSE for each training and testing set for both endogenous and exogenous model.\n\nmerged_csv = pd.read_csv(\"../data/merged_gdf.csv\")\n\n# Extract the county name from the the Geo_NAME column. \nmerged_gdf['County'] = merged_gdf['Geo_NAME'].str.split(',').str[1].str.strip().str.replace(' ', '')\nmerged_gdf = merged_gdf.dropna(subset=['County'])\n\n\n# Spatially lagged endogenous regressor\nodd_counties = ['NewLondonCounty', 'NewHavenCounty', 'LitchfieldCounty', 'TollandCounty']\neven_counties = ['MiddlesexCounty', 'FairfieldCounty','HartfordCounty', 'WindhamCounty']\n\nrmse = []\n\nfor i in range(4):\n    # Splitting training and testing counties\n    train_1 = merged_gdf[(merged_gdf['County'] != odd_counties[i]) & (merged_gdf['County'] != even_counties[i])]\n    test_1 = merged_gdf[(merged_gdf['County'] == odd_counties[i]) | (merged_gdf['County'] == even_counties[i])]\n\n    # Queen weight matrix for each train and test\n    train_1_w = lp.weights.Queen.from_dataframe(train_1)\n    test_1_w = lp.weights.Queen.from_dataframe(test_1)\n    \n    # Regularize the weights\n    train_1_w.transform = 'R'\n    test_1_w.transform = 'R'\n    \n    # Calculate the spatial lag pop density\n    train_1['spatial_lag_PopDens'] = lp.weights.lag_spatial(train_1_w, train_1['PopDensity'])\n    test_1['spatial_lag_PopDens'] = lp.weights.lag_spatial(test_1_w, test_1['PopDensity'])\n    \n    y_train = np.array(train_1['PopDensity']).reshape((-1,1))\n    x_train = np.array(train_1[predictor])\n\n    y_test = np.array(test_1['PopDensity'])\n    x_test = np.array(test_1[predictor])\n\n    # Fit linear regression model using scikit-learn \n    model = LinearRegression()\n    model.fit(x_train, y_train)\n\n    # Predict on test data\n    y_pred_test = model.predict(x_test)\n\n    # Calculate RMSE\n    test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))\n\n    rmse.append(test_rmse)\n\n\nnp.mean(rmse)\n\n382.27553702535505\n\n\nThe average root mean square error of the spatially lagged endogenous regression model is 382.28. The endogenous model is more advantageous when we have a relatively higher coverage of census data and we need to predict the population density of small region surrounded by regions with good census.\nNext, we do training and testing cross validation for the exogenous spatial lagged model.\n\n# Spatially lagged exogenous regressors\n\nrmse_exo = []\n\n# Set loops for each set of different counties\nfor i in range(4):\n\n    train_1 = merged_gdf[(merged_gdf['County'] != odd_counties[i]) & (merged_gdf['County'] != even_counties[i])]\n    test_1 = merged_gdf[(merged_gdf['County'] == odd_counties[i]) | (merged_gdf['County'] == even_counties[i])]\n\n    train_1_w = lp.weights.Queen.from_dataframe(train_1)\n    test_1_w = lp.weights.Queen.from_dataframe(test_1)\n\n    train_1_w.transform = 'R'\n    test_1_w.transform = 'R'\n\n    # Calculate spatial lag \n    for j in range(len(landcover_pct)):\n        train_1['lag_' + landcover_pct[j]] = lp.weights.lag_spatial(train_1_w, train_1[landcover_pct[j]])\n        test_1['lag_' + landcover_pct[j]] = lp.weights.lag_spatial(test_1_w, test_1[landcover_pct[j]])\n    \n    # Extract training and test data \n    y_train = np.array(train_1['PopDensity']).reshape((-1,1))\n    x_train = np.array(train_1[exo_predictor])\n\n    y_test = np.array(test_1['PopDensity'])\n    x_test = np.array(test_1[exo_predictor])\n\n    # Fit linear regression model using scikit-learn \n    model = LinearRegression()\n    model.fit(x_train, y_train)\n\n    # Predict on test data\n    y_pred_test = model.predict(x_test)\n\n    # Calculate RMSE\n    test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))\n\n    rmse_exo.append(test_rmse)\n\n\nnp.mean(rmse_exo)\n\n391.66561692553\n\n\nThe average RMSE of the spatially lagged endogenous regression model cross validation is 391.67, which is slightly larger than the RMSE of the endogenous model. The exogenous model is more applicable to scenarios when we have good satellite data but sparse census data.\nComparing our Spatial Autoregression to our Linear regression, it is clear that our Spatial Regression yields better results."
  },
  {
    "objectID": "posts/final-project-post/final_project.html#concluding-discussion",
    "href": "posts/final-project-post/final_project.html#concluding-discussion",
    "title": "Project Blog Post",
    "section": "Concluding Discussion",
    "text": "Concluding Discussion\nThrough this project, we were able to implement three different forms of Linear Regression, as well as create Spatial Autoregression models, and we determined the efficacy of each of these models both mathematically and graphically. Our results were relatively similar to Tian et al. (2005) in that they underpredicted the population density in densely populated urban areas more frequently than other plots of land, and over-predicted population density in rural areas. Overall, we accomplished a few key things with project. Through our models, we were able to predict population density with only landcover with relatively strong accuracy. We successfully compared different machine learning models and concluded that Spatial Autoregression was more accurate than Linear Regression. With more time, we would have liked to implement Poisson Regression and performed analysis at the block group level instead of tract level. With more computational power, we would have liked to calculate a larger dataset, representing a larger spatial region. Overall, we are proud of our work!"
  },
  {
    "objectID": "posts/final-project-post/final_project.html#group-contributions-statement",
    "href": "posts/final-project-post/final_project.html#group-contributions-statement",
    "title": "Project Blog Post",
    "section": "Group Contributions Statement",
    "text": "Group Contributions Statement\nLiam helped with data acquisition and preparation, wrote our implementation of linear regression with gradient descent in linear_regression.py, and compared the output of our class with that of scikit-learn. Alex acquired landcover data from Conus, and shapefile data of Connecticut. He then implemented zonal statistics with Manny. Alex explained the differences in Spatial Autoregression methods, trained the models, and utilized cross validation. Manny created visualizations of our models to represent graphically the residuals of each model. He proof-read all of our code, making corrections, rewriting descriptions, and ensuring cohesive and consistent writing styles. He also contributed to code in the Spatial Auto Regression section."
  },
  {
    "objectID": "posts/final-project-post/final_project.html#personal-reflection",
    "href": "posts/final-project-post/final_project.html#personal-reflection",
    "title": "Project Blog Post",
    "section": "Personal Reflection",
    "text": "Personal Reflection\nI learned a lot from this process. From a non-technical standpoint, I was working with two really smart mathematicians who have taken a few computer science classes and it was really cool to learn how they operated. They coded differently, commented differently, and got into different problems than I got into while running code. Through this, i learned how to work with non-computer scientists and we were able to work of their domain-specific knowledge of geography with my more computer-science-oriented expertise to create a well-manicured project. From a technical standpoint, a lot of this project involved understanding GIS and key components of this such as tracts, shapefiles, and zonal statistics. I spent extra time understanding these materials. In addition, I also learned about a different type of machine learning model which called Spatial Autoregression which was super cool. I thought I achieved a good amount, I became much more involved in understanding the theoretical and mathematical aspects of this project. I was not able to implement Poisson Regression, but that was okay as I feel with more time I would have been able to do this.\nWith the information I have learned in this project, I really want to do a similar project in Taiwan. I am going to be there for the next 18 months working as a English Tutor and also doing some CS internship there and I am really curious about how this would apply to a place like Taiwan, as it is densely covered in forests. I can see myself continuing to explore GIS applications of Computer Science."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Collection of Blogs",
    "section": "",
    "text": "Logistic Regression\n\n\n\n\n\n\ncsci415\n\n\n\nImplementation of Logistic Regression using Gradient Descent with Momentum\n\n\n\n\n\nJun 17, 2024\n\n\nManuel Fors\n\n\n\n\n\n\n\n\n\n\n\n\nExplaining Support Vector Machines\n\n\n\n\n\n\nR\n\n\n\nI give a break introduction of how Support Vector Machines (SVM) can be used in machine learning\n\n\n\n\n\nMay 28, 2024\n\n\nManuel Fors\n\n\n\n\n\n\n\n\n\n\n\n\nProject Blog Post\n\n\n\n\n\n\ncsci415\n\n\n\nPredicting Population Density with Land Cover\n\n\n\n\n\nMay 17, 2024\n\n\nManuel Fors, Liam Smith, Yide (Alex) Xu\n\n\n\n\n\n\n\n\n\n\n\n\nPenguin Blog\n\n\n\n\n\n\ncsci415\n\n\n\nGenre Classifier\n\n\n\n\n\nMay 15, 2024\n\n\nManuel Fors\n\n\n\n\n\n\n\n\n\n\n\n\nWhose Costs?\n\n\n\n\n\n\ncsci415\n\n\n\nCredit risk-prediction\n\n\n\n\n\nApr 24, 2024\n\n\nManuel Fors\n\n\n\n\n\n\n\n\n\n\n\n\nTest analysis of Phishing and Non-phishing emails\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\nApr 24, 2024\n\n\nManuel Fors\n\n\n\n\n\n\n\n\n\n\n\n\nWiDS Conference\n\n\n\n\n\n\ncsci415\n\n\n\nEssay on attending WiDS at Middlebury\n\n\n\n\n\nApr 3, 2024\n\n\nManuel Fors\n\n\n\n\n\n\n\n\n\n\n\n\nPerceptron Blog\n\n\n\n\n\n\ncsci415\n\n\n\nThis is an implementation of the Perceptron Algorithm\n\n\n\n\n\nMar 22, 2024\n\n\nManuel Fors\n\n\n\n\n\n\n\n\n\n\n\n\nReplication Study Blog\n\n\n\n\n\n\ncsci415\n\n\n\nThis focuses on a study done on medical algorithms that assign scores to individuals and contains a racial bias.\n\n\n\n\n\nMar 21, 2024\n\n\nManuel Fors\n\n\n\n\n\n\n\n\n\n\n\n\nPenguin Blog\n\n\n\n\n\n\ncsci415\n\n\n\nPenguin classifier\n\n\n\n\n\nFeb 16, 2024\n\n\nManuel Fors\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/penguins-blog-post/index.html",
    "href": "posts/penguins-blog-post/index.html",
    "title": "Penguin Blog",
    "section": "",
    "text": "Adelie Penguin ready to engage in his final semester"
  },
  {
    "objectID": "posts/penguins-blog-post/index.html#loading-in-data",
    "href": "posts/penguins-blog-post/index.html#loading-in-data",
    "title": "Penguin Blog",
    "section": "Loading in Data",
    "text": "Loading in Data\nWe are going to load in our training data for this project from a github url\n\nimport pandas as pd \ntrain_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\n\nNow we are going to take a look at our data and look for some potentially interesting features. My goal hear is to do some exploration through graphs and a bit of intuition to see if there are any key features that stick out to me.\n\ntrain.head()\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0809\n31\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN63A1\nYes\n11/24/08\n40.9\n16.6\n187.0\n3200.0\nFEMALE\n9.08458\n-24.54903\nNaN\n\n\n1\nPAL0809\n41\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN74A1\nYes\n11/24/08\n49.0\n19.5\n210.0\n3950.0\nMALE\n9.53262\n-24.66867\nNaN\n\n\n2\nPAL0708\n4\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN32A2\nYes\n11/27/07\n50.0\n15.2\n218.0\n5700.0\nMALE\n8.25540\n-25.40075\nNaN\n\n\n3\nPAL0708\n15\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN38A1\nYes\n12/3/07\n45.8\n14.6\n210.0\n4200.0\nFEMALE\n7.79958\n-25.62618\nNaN\n\n\n4\nPAL0809\n34\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN65A2\nYes\n11/24/08\n51.0\n18.8\n203.0\n4100.0\nMALE\n9.23196\n-24.17282\nNaN\n\n\n\n\n\n\n\nFollowing this, I made a rather arbitrary guess to take a look at the culmen depth and culmen length. I combined this with a hue to see the species as well as whether clutches were completed just to have a large range of variables potentially explored by this graph.\n\nfrom matplotlib import pyplot as plt \nimport seaborn as sns \nsns.set_theme(palette=\"bright\")\n\n#sns.scatterplot(data=train, x=\"Flipper Length (mm)\", y=\"Body Mass (g)\", hue=\"Culmen Depth (mm)\", style=\"Clutch Completion\")\n\nsns.scatterplot(data=train, x=\"Culmen Depth (mm)\", y=\"Culmen Length (mm)\", hue=\"Species\", style=\"Clutch Completion\").set_title(\"Culmen Length and Culmen Depth of Chinstrap, Gentoo, and Adelie Penguins\")\n\nText(0.5, 1.0, 'Culmen Length and Culmen Depth of Chinstrap, Gentoo, and Adelie Penguins')\n\n\n\n\n\n\n\n\n\nFollowing this, I created a graph that showed the variation in Delta 13 temperature across the three different islands. I had an intuition that Islands might be used in the key features for my function and thuse wanted to explore this graphically.\n\nsns.displot(train, x=\"Delta 13 C (o/oo)\", col=\"Island\", binwidth=.5, height=3)\n\n/home/manny/anaconda3/envs/ml-0451/lib/python3.11/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n\n\n\n\n\n\n\n\n\n\nbeautiful_table = train.groupby([\"Species\", \"Island\"]).aggregate({\"Body Mass (g)\" : ('mean') ,\"Culmen Depth (mm)\" : ('mean'), \"Flipper Length (mm)\": ('mean'), \"Delta 15 N (o/oo)\" :('mean')})\nbeautiful_table\n\n\n\n\n\n\n\n\n\nBody Mass (g)\nCulmen Depth (mm)\nFlipper Length (mm)\nDelta 15 N (o/oo)\n\n\nSpecies\nIsland\n\n\n\n\n\n\n\n\nAdelie Penguin (Pygoscelis adeliae)\nBiscoe\n3711.363636\n18.475758\n188.636364\n8.788643\n\n\nDream\n3728.888889\n18.306667\n190.133333\n8.933945\n\n\nTorgersen\n3712.804878\n18.468293\n191.195122\n8.846768\n\n\nChinstrap penguin (Pygoscelis antarctica)\nDream\n3743.421053\n18.366667\n196.000000\n9.331004\n\n\nGentoo penguin (Pygoscelis papua)\nBiscoe\n5039.948454\n14.914433\n216.752577\n8.247341\n\n\n\n\n\n\n\nAfter exploration, I can see that average Flipper Length and Delta 15 N differentiate between the species. I can also see how both the Chinstrap and Gentoo penguin only appear on one island each. While I believe these features might be key features, I am going to take a brute-force approach at finding the correct features by trying every possible combination of approaches."
  },
  {
    "objectID": "posts/penguins-blog-post/index.html#organizing-data",
    "href": "posts/penguins-blog-post/index.html#organizing-data",
    "title": "Penguin Blog",
    "section": "Organizing Data",
    "text": "Organizing Data\nFollowing this graphical exploration, I need to use the LabelEncoder. The LabelEncoder is going to set increasingly large numbers starting from 0 to replace each label. For all other qualitative columns, they are are turned into 1’s and 0’s so that we can run our models on it.\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df) #here is where we set all non-integer values to integers\n  return df, y\n\nX_train, y_train = prepare_data(train)"
  },
  {
    "objectID": "posts/penguins-blog-post/index.html#lets-use-a-more-comprehensive-search-for-choosing-features",
    "href": "posts/penguins-blog-post/index.html#lets-use-a-more-comprehensive-search-for-choosing-features",
    "title": "Penguin Blog",
    "section": "Lets use a more comprehensive search for choosing features!",
    "text": "Lets use a more comprehensive search for choosing features!\nBecause our dataset is relatively smaller, we can use an exhaustive search for all the features contained in the dataset. This means we are going to look at every combination of qualitative columns with quantitative columns and then create a model on this. Following this we calculate an estimated score which we add to a dictionary of ratings.\n\nfrom itertools import combinations\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nimport warnings\nwarnings.filterwarnings('ignore')\n\nLR = LogisticRegression()\n\n# these are not actually all the columns: you'll \n# need to add any of the other ones you want to search for\nall_qual_cols = [\"Clutch Completion\", \"Sex\", \"Island\"]\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)','Body Mass (g)', 'Delta 15 N (o/oo)', 'Delta 13 C (o/oo)']\nratings = {}\nfor qual in all_qual_cols: \n  qual_cols = [col for col in X_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 2):\n    # you could train models and score them here, keeping the list of \n    cols = qual_cols + list(pair) \n    LR.fit(X_train[cols], y_train)\n    #cv_scores_LR = cross_val_score(LR, X_train, y_train, cv=3)\n\n    estimated_score = LR.score(X_train[cols], y_train) \n    #estimated_score = cv_scores_LR.mean()\n    ratings[\" \".join(cols)] = estimated_score \n\nWe now have a dictionary of items that should tell us our highest scored combination\n\ntop_result = max(ratings, key=ratings.get)\ntop_result \n\n'Sex_FEMALE Sex_MALE Culmen Length (mm) Culmen Depth (mm)'\n\n\nWe are going to move our qualitative values to the end of our list because of our decision regions function we have later on\n\n#top_result= [\"Culmen Length (mm)\", \"Culmen Depth (mm)\", \"Clutch Completion_No\", \"Clutch Completion_Yes\"]\ntop_result= [\"Culmen Length (mm)\", \"Culmen Depth (mm)\", \"Sex_FEMALE\", \"Sex_MALE\"]\n\n\nLR.fit(X_train[top_result], y_train)\nLR.score(X_train[top_result], y_train)\n\n0.99609375"
  },
  {
    "objectID": "posts/penguins-blog-post/index.html#plotting-decisions-regions",
    "href": "posts/penguins-blog-post/index.html#plotting-decisions-regions",
    "title": "Penguin Blog",
    "section": "Plotting Decisions Regions",
    "text": "Plotting Decisions Regions\nWow! Our rating is pretty high. Lets visualize what our model is doing by plotting Decisions Regions\n\nfrom matplotlib import pyplot as plt\nimport numpy as np\n\n\n\nfrom matplotlib.patches import Patch\n\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (7, 3))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1], \n            title = qual_features[i])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n      plt.tight_layout()\n\n\nprint(X_train[top_result], y_train)\n\nplot_regions(LR, X_train[top_result], y_train)\n\n     Culmen Length (mm)  Culmen Depth (mm)  Sex_FEMALE  Sex_MALE\n0                  40.9               16.6        True     False\n1                  49.0               19.5       False      True\n2                  50.0               15.2       False      True\n3                  45.8               14.6        True     False\n4                  51.0               18.8       False      True\n..                  ...                ...         ...       ...\n270                51.1               16.5       False      True\n271                35.9               16.6        True     False\n272                39.5               17.8        True     False\n273                36.7               19.3        True     False\n274                42.4               17.3        True     False\n\n[256 rows x 4 columns] [1 1 2 2 1 0 0 1 2 1 0 1 0 1 1 2 0 2 2 2 2 0 0 0 2 1 0 0 0 0 0 0 1 2 0 0 2\n 2 1 1 2 2 1 0 0 2 2 1 2 2 1 2 0 0 2 2 0 1 2 2 1 2 1 2 2 2 0 0 0 2 2 2 0 1\n 2 2 2 0 0 2 0 0 2 0 0 0 1 0 0 1 0 0 0 1 0 0 2 2 0 0 2 0 2 1 0 2 2 1 2 2 2\n 0 2 0 0 0 1 0 2 2 0 2 2 1 2 0 0 1 2 2 1 0 2 0 1 2 0 0 2 0 2 1 0 0 2 1 0 2\n 0 2 0 1 0 0 0 2 2 2 0 0 2 0 2 1 1 0 1 2 0 0 1 1 0 0 1 0 0 2 1 2 1 0 0 0 1\n 0 2 2 2 2 1 1 1 2 2 2 0 1 0 0 1 0 0 0 0 0 1 2 2 2 0 2 2 1 0 2 0 0 2 0 2 0\n 2 0 2 2 2 2 0 2 1 0 2 1 1 0 2 1 0 0 0 1 0 1 0 0 2 1 0 0 0 2 0 0 0 1]"
  },
  {
    "objectID": "posts/penguins-blog-post/index.html#cross-validation",
    "href": "posts/penguins-blog-post/index.html#cross-validation",
    "title": "Penguin Blog",
    "section": "Cross Validation",
    "text": "Cross Validation\nWe want to calculate how our model will do on unseen data, and a useful way to simulate this is by performing cross-validation. We can take parts of the train data from being used, and then testing on those witheld parts, we can determine our accuracy.\n\nfrom sklearn.model_selection import cross_val_score\n\ncv_scores_LR = cross_val_score(LR, X_train, y_train, cv=5)\ncv_scores_LR.mean()\n\n1.0"
  },
  {
    "objectID": "posts/penguins-blog-post/index.html#we-did-it",
    "href": "posts/penguins-blog-post/index.html#we-did-it",
    "title": "Penguin Blog",
    "section": "We did it!",
    "text": "We did it!\n\nNext Steps\nThis would involve testing other models such as Decision Classfier Trees, and we could run the cross validation step within our brute-force attempt instead of after."
  },
  {
    "objectID": "posts/replication-blog-post/index.html#modelingggggg",
    "href": "posts/replication-blog-post/index.html#modelingggggg",
    "title": "Replication Study Blog",
    "section": "MODELINGGGGGG",
    "text": "MODELINGGGGGG\nHere we go! Its important to know what we are modeling here. We are going to attempt to fit a linear regression model with our features. Our model can be described mathematically by this description \\[ \\text{log cost} \\approx w_b \\times \\text{(patient is Black)} \\sum_{i+0}^kw_k \\times \\text{(gagne sum)}^k\\]\nWith our model, we are going to estimate what \\(w_b\\) is, which is how much cost Black Patients incur as a percentage of White Patients. However, we do not know how many polynomial featuers we should use! Don’t worry, we have a plan!\nWe are going to use a function called add_polynomial_featuers to keep adding new columns in our dataframe until we find a successful number of polynomial features.\n\ndef add_polynomial_features(X, degree):\n  X_ = X.copy()\n  for j in range(1, degree):\n    X_[f\"poly_{j}\"] = X_[\"gagne_sum_t\"]**j\n\n  return X_\n\nHere is the meat of our code where we are going to fit our model and cross-validate our scores. We are also going to test all values from 1-10 as dictated by our “degrees” variable\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LinearRegression\nratings = {}\ndegrees = 10\nLR = LinearRegression(fit_intercept = True)\n#run our polynomial features\nnew_X = add_polynomial_features(non_zero_costs, degrees)\nfor i in range(1, degrees):\n    \n    predictor_variables=[\"is_black\", f\"poly_{i}\", \"gagne_sum_t\"]\n    LR.fit(new_X[predictor_variables], new_X[\"log_costs\"])\n    cv_scores_LR = cross_val_score(LR, new_X[predictor_variables], new_X[target_variable], cv=3)\n    estimated_score = cv_scores_LR.mean()\n    ratings[i] = estimated_score\n\nHere we are going to find our top result, which comes out to be 2. Then we create a new model basedon that value and we fit it.\n\n#Here we can take a look at the \n\ntop_result = max(ratings, key=ratings.get)\nnew_predictor_values = [\"is_black\", f\"poly_{top_result}\", \"gagne_sum_t\"]\n#Following our top result, lets now make a model using this\nLR = LinearRegression(fit_intercept = True)\nLR.fit(new_X[predictor_variables], new_X[\"log_costs\"])\n\ntop_result\n\n2\n\n\nFrom here, we now want to compute our \\(e^{w_b}\\) and we access our \\(w_b\\) by looking at our LR.coef. Since it is supposed to be the same order as the variables we put in, we are going to look at the first value.\n\nimport math \nmath.e**LR.coef_[0] \n\n0.7693199308357737\n\n\nThis value shows that the cost incurred by Black patients as a percentage of White patients is about 77%. In more understandable terms, this means that Black patients only spend 77% of what White patients spend. Thats only 77 cents spent by Black Patients for every dollar that White patients are spending!\nThis connects directly both with the graphs previously created in this document as well as a key component of the main argument of Obermeyer et al.(2019), which states that Black patients incur less costs than equally sick White patients."
  },
  {
    "objectID": "posts/replication-blog-post/index.html#references",
    "href": "posts/replication-blog-post/index.html#references",
    "title": "Replication Study Blog",
    "section": "References",
    "text": "References\nBarocas, Solon, Moritz Hardt, and Arvind Narayanan. 2023. Fairness and Machine Learning: Limitations and Opportunities. Cambridge, Massachusetts: The MIT Press.\nObermeyer, Ziad, Brian Powers, Christine Vogeli, and Sendhil Mullainathan. 2019. “Dissecting Racial Bias in an Algorithm Used to Manage the Health of Populations.” Science 366 (6464): 447–53. https://doi.org/10.1126/science.aax2342."
  },
  {
    "objectID": "posts/music-blog-post/index.html",
    "href": "posts/music-blog-post/index.html",
    "title": "Penguin Blog",
    "section": "",
    "text": "Introduction\nThis blog will utilize neural networks, and data collected by Spotify, to determine the genre of a given song. Below we will create three neural networks with Torch and attempt to train them.\n\nimport pandas as pd\n\nurl = \"https://raw.githubusercontent.com/PhilChodrow/PIC16B/master/datasets/tcc_ceds_music.csv\"\ndf = pd.read_csv(url)\n\n\ndf.head()\n\n\n\n\n\n\n\n\nUnnamed: 0\nartist_name\ntrack_name\nrelease_date\ngenre\nlyrics\nlen\ndating\nviolence\nworld/life\n...\nsadness\nfeelings\ndanceability\nloudness\nacousticness\ninstrumentalness\nvalence\nenergy\ntopic\nage\n\n\n\n\n0\n0\nmukesh\nmohabbat bhi jhoothi\n1950\npop\nhold time feel break feel untrue convince spea...\n95\n0.000598\n0.063746\n0.000598\n...\n0.380299\n0.117175\n0.357739\n0.454119\n0.997992\n0.901822\n0.339448\n0.137110\nsadness\n1.0\n\n\n1\n4\nfrankie laine\ni believe\n1950\npop\nbelieve drop rain fall grow believe darkest ni...\n51\n0.035537\n0.096777\n0.443435\n...\n0.001284\n0.001284\n0.331745\n0.647540\n0.954819\n0.000002\n0.325021\n0.263240\nworld/life\n1.0\n\n\n2\n6\njohnnie ray\ncry\n1950\npop\nsweetheart send letter goodbye secret feel bet...\n24\n0.002770\n0.002770\n0.002770\n...\n0.002770\n0.225422\n0.456298\n0.585288\n0.840361\n0.000000\n0.351814\n0.139112\nmusic\n1.0\n\n\n3\n10\npérez prado\npatricia\n1950\npop\nkiss lips want stroll charm mambo chacha merin...\n54\n0.048249\n0.001548\n0.001548\n...\n0.225889\n0.001548\n0.686992\n0.744404\n0.083935\n0.199393\n0.775350\n0.743736\nromantic\n1.0\n\n\n4\n12\ngiorgos papadopoulos\napopse eida oneiro\n1950\npop\ntill darling till matter know till dream live ...\n48\n0.001350\n0.001350\n0.417772\n...\n0.068800\n0.001350\n0.291671\n0.646489\n0.975904\n0.000246\n0.597073\n0.394375\nromantic\n1.0\n\n\n\n\n5 rows × 31 columns\n\n\n\n\nengineered_features = ['dating', 'violence', 'world/life', 'night/time','shake the audience','family/gospel', 'romantic', 'communication','obscene', 'music', 'movement/places', 'light/visual perceptions','family/spiritual', 'like/girls', 'sadness', 'feelings', 'danceability','loudness', 'acousticness', 'instrumentalness', 'valence', 'energy']      \n\nLets look at our different type of genres that we have!\n\ndf.groupby(\"genre\").size()\n\ngenre\nblues      4604\ncountry    5445\nhip hop     904\njazz       3845\npop        7042\nreggae     2498\nrock       4034\ndtype: int64\n\n\nLets change reclassify\n\n\ngenres = {\n    \"blues\"     : 0,\n    \"country\"   : 1,\n    \"hip hop\"   : 2,\n    \"jazz\"      : 3,\n    \"pop\"       : 4,\n    \"reggae\"    : 5,\n    \"rock\"      : 6\n}\ndf = df[df[\"genre\"].apply(lambda x: x in genres.keys())]\ndf[\"genre\"] = df[\"genre\"].apply(genres.get) \n\nNow that we have relabeled all of our categories lets ensure that they have all been converted to integer values\n\ndf.groupby(\"genre\").size()\n\ngenre\n0    4604\n1    5445\n2     904\n3    3845\n4    7042\n5    2498\n6    4034\ndtype: int64\n\n\nNow let’s calculate our base-rate\n\ndf.groupby(\"genre\").size() / len(df)\n\ngenre\n0    0.162273\n1    0.191915\n2    0.031862\n3    0.135521\n4    0.248202\n5    0.088045\n6    0.142182\ndtype: float64\n\n\nIf we were to guess category 4 (pop) we would be right about 24.8% of the time so this our base-rate.\nLets utilize engineered_features\n\nfrom torch import nn\nfrom matplotlib import pyplot as plt\nimport numpy as np \n\n\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.model_selection import train_test_split\nclass LyricsFromDF(Dataset):\n    def __init__(self, df):\n        self.df = df\n    \n    def __getitem__(self, index):\n        return self.df.iloc[index, 1], self.df.iloc[index, 0]\n    \n    def __len__(self):\n        return len(self.df)\n\n\ndf_train, df_val = train_test_split(df,shuffle = True, test_size = 0.2)\ntrain_data = LyricsFromDF(df_train)\nval_data   = LyricsFromDF(df_val)\n\ntrain_data[1000]\n\n('randy rogers band', 34490)\n\n\n\n\n!pipinstall torchtext\n\ntokenizer = get_tokenizer('basic_english')\n\ntokenized = tokenizer(train_data[194][0])\ntokenized\n\n/usr/bin/bash: line 1: pipinstall: command not found\n\n\nNameError: name 'get_tokenizer' is not defined\n\n\n\nclass TextClassificationModel(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, max_len, num_class)\n\n\n\n\nclass EngineeredNetwork(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super().__init__()\n\n\nimport time \n\ndef train(dataloader):\n    epoch_start_time = time.time()\n\n\nfrom matplotlib import pyplot as plt\npop_df = df[df['genre'] == 4]\n\nimport seaborn as sns\nsns.set_theme(palette=\"bright\")\n\nax = sns.regplot(data=pop_df, y=\"danceability\", x=\"release_date\", line_kws={\"color\": \"red\"})\n\n# Set the title\nax.set_title(\"Pop song danceability over time\")\n\n# Show the plot\nplt.show()\n\n\n\n\n\n\n\n\n\nfrom matplotlib import pyplot as plt\npop_df = df[df['genre'] == 4]\n\nimport seaborn as sns\nsns.set_theme(palette=\"bright\")\n\nax = sns.regplot(data=pop_df, x=\"danceability\", y=\"release_date\", line_kws={\"color\": \"red\"})\n\n# Set the title\nax.set_title(\"Pop song danceability over time\")\n\n# Show the plot\nplt.show()\n\n\nfrom matplotlib import pyplot as plt\n\nimport seaborn as sns\nsns.set_theme(palette=\"bright\")\n\nplt.figure(figsize=(12, 8))\nsns.boxplot(data=df, x='genre', y='sadness', palette='Set2')\nplt.title('Sadness Distribution Across Genres')\nplt.xlabel('Genre')\nplt.ylabel('Sadness')\n#plt.xticks(rotation=10)  # Rotate x-axis labels for better visibility\nplt.grid(True)\nplt.show()"
  },
  {
    "objectID": "posts/decision-making-blog-post/index.html",
    "href": "posts/decision-making-blog-post/index.html",
    "title": "Whose Costs?",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/train.csv\"\ndf_train = pd.read_csv(url)\ndf_train.head()\n\n\n\n\n\n\n\n\nperson_age\nperson_income\nperson_home_ownership\nperson_emp_length\nloan_intent\nloan_grade\nloan_amnt\nloan_int_rate\nloan_status\nloan_percent_income\ncb_person_default_on_file\ncb_person_cred_hist_length\n\n\n\n\n0\n25\n43200\nRENT\nNaN\nVENTURE\nB\n1200\n9.91\n0\n0.03\nN\n4\n\n\n1\n27\n98000\nRENT\n3.0\nEDUCATION\nC\n11750\n13.47\n0\n0.12\nY\n6\n\n\n2\n22\n36996\nRENT\n5.0\nEDUCATION\nA\n10000\n7.51\n0\n0.27\nN\n4\n\n\n3\n24\n26000\nRENT\n2.0\nMEDICAL\nC\n1325\n12.87\n1\n0.05\nN\n4\n\n\n4\n29\n53004\nMORTGAGE\n2.0\nHOMEIMPROVEMENT\nA\n15000\n9.63\n0\n0.28\nN\n10"
  },
  {
    "objectID": "posts/decision-making-blog-post/index.html#part-a-grab-the-data",
    "href": "posts/decision-making-blog-post/index.html#part-a-grab-the-data",
    "title": "Whose Costs?",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/train.csv\"\ndf_train = pd.read_csv(url)\ndf_train.head()\n\n\n\n\n\n\n\n\nperson_age\nperson_income\nperson_home_ownership\nperson_emp_length\nloan_intent\nloan_grade\nloan_amnt\nloan_int_rate\nloan_status\nloan_percent_income\ncb_person_default_on_file\ncb_person_cred_hist_length\n\n\n\n\n0\n25\n43200\nRENT\nNaN\nVENTURE\nB\n1200\n9.91\n0\n0.03\nN\n4\n\n\n1\n27\n98000\nRENT\n3.0\nEDUCATION\nC\n11750\n13.47\n0\n0.12\nY\n6\n\n\n2\n22\n36996\nRENT\n5.0\nEDUCATION\nA\n10000\n7.51\n0\n0.27\nN\n4\n\n\n3\n24\n26000\nRENT\n2.0\nMEDICAL\nC\n1325\n12.87\n1\n0.05\nN\n4\n\n\n4\n29\n53004\nMORTGAGE\n2.0\nHOMEIMPROVEMENT\nA\n15000\n9.63\n0\n0.28\nN\n10"
  },
  {
    "objectID": "posts/decision-making-blog-post/index.html#part-b-exploration-through-visualization",
    "href": "posts/decision-making-blog-post/index.html#part-b-exploration-through-visualization",
    "title": "Whose Costs?",
    "section": "Part B: Exploration through visualization",
    "text": "Part B: Exploration through visualization\nI want to explore the data through some categorical variables. Specifically, I am curious about how loan intent correlates with the loan grade people are getting as well as what kind\n\nimport seaborn as sns \nimport matplotlib.pyplot as plt \nsns.set_style(\"darkgrid\")\nsns.set_palette(\"bright\")\nsns.set_context\n\nfigure1 = sns.relplot(data=df_train, x=\"loan_amnt\", y=\"loan_grade\", col=\"loan_intent\", hue =\"loan_status\", col_wrap=3)\n\n\n\n\n\n\n\n\n\nfigure2 = sns.relplot(data=df_train, x=\"loan_amnt\", y=\"loan_grade\", col=\"person_home_ownership\", hue =\"loan_status\", col_wrap=2)\n\n\n\n\n\n\n\n\nExploring new graphs, I want to potentially answer the questions\n\nfigure3=sns.scatterplot(data=df_train, x='person_age', y='person_emp_length', hue='loan_intent')\n\n\n\n\n\n\n\n\n\nOutliers\nI noticed some strong outliers and was curious what these data points looked like\n\ndf_over_100_age = df_train[df_train['person_age']&gt;100]\ndf_over_100_age\n\n\n\n\n\n\n\n\nperson_age\nperson_income\nperson_home_ownership\nperson_emp_length\nloan_intent\nloan_grade\nloan_amnt\nloan_int_rate\nloan_status\nloan_percent_income\ncb_person_default_on_file\ncb_person_cred_hist_length\n\n\n\n\n1561\n144\n250000\nRENT\n4.0\nVENTURE\nC\n4800\n13.57\n0\n0.02\nN\n3\n\n\n2043\n123\n80004\nRENT\n2.0\nEDUCATION\nB\n20400\n10.25\n0\n0.25\nN\n3\n\n\n6493\n144\n200000\nMORTGAGE\n4.0\nEDUCATION\nB\n6000\n11.86\n0\n0.03\nN\n2\n\n\n18067\n144\n6000000\nMORTGAGE\n12.0\nPERSONAL\nC\n5000\n12.73\n0\n0.00\nN\n25\n\n\n\n\n\n\n\nI decide to remove both people under age 100 and employement years under 60 so that my data would be more readable and less filled with outliers\n\ndf_under_100_age_and_under_60_emp_years = df_train[(df_train['person_age']&lt;100) & (df_train['person_emp_length']&lt;60)]\nfigure4=sns.scatterplot(data=df_under_100_age_and_under_60_emp_years, x='person_age', y='person_emp_length', hue='loan_intent', size=4)\n\n\n\n\n\n\n\n\n\n#18-24 #25-34 #35-44 45-64 65+ \n#lets make a summary table\n\n#I want to see the average amount per age group\n#these age bands are defined by the US 2020 census\n#float('inf') is so we take into account all people above this value\ndf_under_100_age = df_train[(df_train['person_age']&lt;100)]\nage_bins = [18, 25, 35, 45, 65, float('inf')]\nage_labels = ['18-24', '25-34', '35-44', '45-64', '65+']\ndf_under_100_age['age_band'] = pd.cut(df_under_100_age['person_age'], bins=age_bins, labels=age_labels, right=False)\n\nmost_common_intent_loan_by_age = df_under_100_age.groupby('age_band')['loan_intent'].agg(lambda x: x.value_counts().index[0])\n\n/tmp/ipykernel_259090/406788686.py:10: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df_under_100_age['age_band'] = pd.cut(df_under_100_age['person_age'], bins=age_bins, labels=age_labels, right=False)\n/tmp/ipykernel_259090/406788686.py:12: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n  most_common_intent_loan_by_age = df_under_100_age.groupby('age_band')['loan_intent'].agg(lambda x: x.value_counts().index[0])\n\n\n\nsummary_loan_intent_by_age = df_under_100_age.pivot_table(index='age_band', columns='loan_intent', aggfunc='size', fill_value=0)\nprint(most_common_intent_loan_by_age)\nsummary_loan_intent_by_age\n\nage_band\n18-24    EDUCATION\n25-34      MEDICAL\n35-44      MEDICAL\n45-64     PERSONAL\n65+        MEDICAL\nName: loan_intent, dtype: object\n\n\n\n\n\n\n\n\nloan_intent\nDEBTCONSOLIDATION\nEDUCATION\nHOMEIMPROVEMENT\nMEDICAL\nPERSONAL\nVENTURE\n\n\nage_band\n\n\n\n\n\n\n\n\n\n\n18-24\n1559\n2560\n591\n1708\n1608\n1811\n\n\n25-34\n2122\n2052\n1885\n2505\n2196\n2224\n\n\n35-44\n413\n434\n355\n508\n424\n491\n\n\n45-64\n81\n76\n71\n100\n169\n86\n\n\n65+\n3\n3\n0\n14\n10\n1\n\n\n\n\n\n\n\n\ndf_under_100_age_under_10_emp = df_under_100_age[df_under_100_age['person_emp_length']&lt;10]\nsummary_loan_intent_by_age_under_10_emp = df_under_100_age_under_10_emp.pivot_table(index='age_band', columns='loan_intent', aggfunc='size', fill_value=0)\nsummary_loan_intent_by_age_under_10_emp\n\n\n\n\n\n\n\nloan_intent\nDEBTCONSOLIDATION\nEDUCATION\nHOMEIMPROVEMENT\nMEDICAL\nPERSONAL\nVENTURE\n\n\nage_band\n\n\n\n\n\n\n\n\n\n\n18-24\n1519\n2489\n574\n1650\n1556\n1752\n\n\n25-34\n1699\n1662\n1504\n2021\n1753\n1767\n\n\n35-44\n319\n356\n265\n396\n327\n368\n\n\n45-64\n65\n68\n60\n76\n129\n59\n\n\n65+\n2\n3\n0\n11\n9\n1"
  },
  {
    "objectID": "posts/decision-making-blog-post/index.html#part-c-fitting-the-model",
    "href": "posts/decision-making-blog-post/index.html#part-c-fitting-the-model",
    "title": "Whose Costs?",
    "section": "Part C: Fitting the model",
    "text": "Part C: Fitting the model\nLets use our data frame df_under_100_age_and_under_60_emp_years as it eliminates outliers in age and employement years\n\ndf_under_100_age_and_under_60_emp_years.head()\n\n\n\n\n\n\n\n\nperson_age\nperson_income\nperson_home_ownership\nperson_emp_length\nloan_intent\nloan_grade\nloan_amnt\nloan_int_rate\nloan_status\nloan_percent_income\ncb_person_default_on_file\ncb_person_cred_hist_length\nage_band\n\n\n\n\n0\n25\n43200\nRENT\nNaN\nVENTURE\nB\n1200\n9.91\n0\n0.03\nN\n4\n25-34\n\n\n1\n27\n98000\nRENT\n3.0\nEDUCATION\nC\n11750\n13.47\n0\n0.12\nY\n6\n25-34\n\n\n2\n22\n36996\nRENT\n5.0\nEDUCATION\nA\n10000\n7.51\n0\n0.27\nN\n4\n18-24\n\n\n3\n24\n26000\nRENT\n2.0\nMEDICAL\nC\n1325\n12.87\n1\n0.05\nN\n4\n18-24\n\n\n4\n29\n53004\nMORTGAGE\n2.0\nHOMEIMPROVEMENT\nA\n15000\n9.63\n0\n0.28\nN\n10\n25-34\n\n\n\n\n\n\n\nHere we can do a bit of preprocessing before we make our model. I noticed that there were missing values in our person_emp_length, and loan_int_rate columns so I removed the associated rows.\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\n\n# Drop columns that we will not use \n# These are our training datasets  \nX_train = df_under_100_age.drop([\"loan_status\", \"loan_grade\"], axis = 1)\ny_train = df_under_100_age['loan_status']\n\n\n# These are the columns we will use in our dataset\nall_qual_cols = ['loan_intent', 'cb_person_default_on_file']\nall_quant_cols = ['person_age', 'person_income', 'person_emp_length', 'loan_amnt', 'loan_int_rate', 'loan_percent_income', 'cb_person_cred_hist_length']\n\n\n# Drop any NA values\nX_train = X.dropna()\n\n# In order to avoid size differences between the two, I make sure y has the same number of \n# rows as X \ny_train = y[X.index]\n\n# Lets change all qual columns to decimal values to use in our Logistic Regression\nfor col in all_qual_cols:\n    X_train[col] = le.fit_transform(X_train[col])\n\n\n\nNow there should be no missing values!\n\nmissing_values_count = X_train.isnull().sum()\nprint(\"Number of missing values in each column:\\n\", missing_values_count)\n\nNumber of missing values in each column:\n person_age                    0\nperson_income                 0\nperson_home_ownership         0\nperson_emp_length             0\nloan_intent                   0\nloan_amnt                     0\nloan_int_rate                 0\nloan_percent_income           0\ncb_person_default_on_file     0\ncb_person_cred_hist_length    0\nage_band                      0\ndtype: int64\n\n\nWe now want to find the best combination of qual and quantitative columns.\n\nfrom itertools import combinations\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nLR = LogisticRegression()\n\n\nratings = {}\n\n# Iterate through all combinations of qualitative and quantitative features\nfor qual in all_qual_cols:\n    qual_cols = [col for col in X_train.columns if qual in col]\n    for pair in combinations(all_quant_cols, 4):\n        \n        cols = qual_cols + list(pair)\n        \n        # Train logistic regression model\n        LR.fit(X_train[cols], y_train)\n        cv_scores_LR = cross_val_score(LR, X_train[cols], y_train, cv =3)\n        # Calculate and store the estimated score\n        #estimated_score = LR.score(X_train[cols], y_train)\n        estimated_score = cv_scores_LR.mean()\n        ratings[\", \".join(cols)] = estimated_score\n\n# Print the rating\n\n/home/manny/anaconda3/envs/ml-0451/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/home/manny/anaconda3/envs/ml-0451/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/home/manny/anaconda3/envs/ml-0451/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/home/manny/anaconda3/envs/ml-0451/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/home/manny/anaconda3/envs/ml-0451/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/home/manny/anaconda3/envs/ml-0451/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/home/manny/anaconda3/envs/ml-0451/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/home/manny/anaconda3/envs/ml-0451/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/home/manny/anaconda3/envs/ml-0451/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/home/manny/anaconda3/envs/ml-0451/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/home/manny/anaconda3/envs/ml-0451/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/home/manny/anaconda3/envs/ml-0451/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/home/manny/anaconda3/envs/ml-0451/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/home/manny/anaconda3/envs/ml-0451/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/home/manny/anaconda3/envs/ml-0451/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/home/manny/anaconda3/envs/ml-0451/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n\n\n\ntop_result = max(ratings, key = ratings.get)\ntop_result_array = top_result.split(\", \")\n\nLR.fit(X_train[top_result_array], y_train)\nLR.score(X_train[top_result_array], y_train)\n\n0.8290616949744575"
  },
  {
    "objectID": "posts/decision-making-blog-post/index.html#find-the-threshold",
    "href": "posts/decision-making-blog-post/index.html#find-the-threshold",
    "title": "Whose Costs?",
    "section": "Find the threshold",
    "text": "Find the threshold\nNow that we have a decent working model, let’s look at the weight vector associated with this\n\nw = LR.coef_\nw\n\narray([[-1.24528351e-01, -4.12340732e-02,  2.91752559e-01,\n         8.46612085e+00, -6.77066619e-03]])\n\n\n\ny_pred = LR.predict(X_train[top_result_array])\n\n\ndef loan_repaid(loan_amnt, loan_int_rate):\n    return loan_amt*(1 + 0.25*loan_int_rate)**10 - loan_amnt \n\ndef loan_defaulted(loan_amnt, loan_int_rate):\n    return loan_amnt*(1 + 0.25*loan_int_rate)**3 - 1.7*loan_amnt \n\n\nloan_amnt = X_train['loan_amnt'].values\nloan_int_rate = X_train['loan_int_rate'].values\n\nloan_profit = loan_repaid(loan_amnt, loan_int_rate)\nloan_loss = loan_defaulted(loan_amnt, loan_int_rate)\n\n#Calculate score \nw.reshape(-1, 1)\nscore = X_train[top_result_array]@w\n\nTaking a brief look at scores, we can see that our a score a little above 4 is the most common score\n\nhist = plt.hist(s)\nplt.title(\"Frequency of Score calculations\")\nlabs = plt.gca().set(xlabel = r\"Score $s$\", ylabel = \"Frequency\") \n\n\n\n\n\n\n\n\nMy score matrix is not the same shape as my y_train so i flatten my score to match. I also normalized my score values so they lie between 0 and 1\n\nscore = np.array(score).flatten()\n\nscore = (score - np.min(score)) / (np.max(score) - np.min(score))\n\nWe can now examine our accuracy as thresholds change\n\nfor t in np.linspace(0, 1, 11):\n    y_pred1 = score &gt;= t\n    acc = (y_pred1 == y_train).mean()\n    print(f\"A threshold of {t:.1f} gives an accuracy of {acc:.2f}.\")\n\nA threshold of 0.0 gives an accuracy of 0.22.\nA threshold of 0.1 gives an accuracy of 0.22.\nA threshold of 0.2 gives an accuracy of 0.22.\nA threshold of 0.3 gives an accuracy of 0.22.\nA threshold of 0.4 gives an accuracy of 0.33.\nA threshold of 0.5 gives an accuracy of 0.60.\nA threshold of 0.6 gives an accuracy of 0.81.\nA threshold of 0.7 gives an accuracy of 0.82.\nA threshold of 0.8 gives an accuracy of 0.79.\nA threshold of 0.9 gives an accuracy of 0.79.\nA threshold of 1.0 gives an accuracy of 0.78.\n\n\nWe see the threshold reach its peak around a threshold of 7 and then fall in accuracy following.\n\nCalculating the ROC curve\nAs we change through our thresholds, we want to know how our True Positive Rates (TPR) and False Positive Rates (FPR) are affected. In order to do so, we can calculate an ROC Curve. This essentially calculates both the TPR and FPR. Through this we can see at each threshold \\(t\\) the relationship between the TPR and FPR.\n\n\nnum_thresholds = 101\n\nFPR = np.zeros(num_thresholds)\nTPR = np.zeros(num_thresholds)\nT = np.linspace(score.min()-0.1, score.max()+0.1, num_thresholds)\n\nfor i in range(num_thresholds):\n    t = T[i]\n    preds    = score &gt;= t\n    FPR[i]   = ((preds == 1) & (y_train == 0)).sum() / (y_train == 0).sum()\n    TPR[i]   = ((preds == 1) & (y_train == 1)).sum() / (y_train == 1).sum()\n\n\nfig, ax = plt.subplots(1, 1, figsize = (6, 4))\nax.plot(FPR, TPR, color = \"black\")\nax.plot([0,1], [0,1], linestyle=\"--\", color = \"grey\")\nax.set_aspect('equal')\n\nlabs = ax.set(xlabel = \"False Positive Rate\", ylabel = \"True Positive Rate\", title = \"ROC Curve\")\n\n\n\n\n\n\n\n\nFrom this, we can also notice that our curve is bending further to the upper left corner, which means our model is predicting a good proportion as True Positive and with low False Positives. More specifically, we can see that with around \\(30\\%\\) FPR we are achieving a TPR of \\(80\\%\\)!\n\n\nCalculating profit\nMost importantly for this use-case, we want to determine what the profit will be at each threshold.\n\n\nprofits = []\n\nfor t in T:\n    predicted_repaid = X['score'] &lt; t \n    total_profit = ((predicted_repaid != y_train) * (y_pred == 0) * (-loan_loss) + (predicted_repaid == y_train) * (y_pred == 0) * loan_profit).mean()\n    profits.append(total_profit)\n    \nplt.plot(T, profits)\nlabs = plt.gca().set(xlabel=r\"Threshold $t$\", ylabel=\"Expected profit per loan\")\n\n\n\n\n\n\n\n\n\nprofits = []\n\nfor t in T:\n    predicted_repaid = y_pred_prob &gt; t  # Predictions based on the threshold\n    total_profit = np.sum(predicted_repaid * loan_profit + (~predicted_repaid) * loan_loss)\n    profits.append(total_profit)\n\nplt.plot(T, profits)\nlabs = plt.gca().set(xlabel=r\"Threshold $t$\", ylabel=\"Expected profit per loan\")"
  },
  {
    "objectID": "posts/phishing-blog-post/index.html",
    "href": "posts/phishing-blog-post/index.html",
    "title": "Test analysis of Phishing and Non-phishing emails",
    "section": "",
    "text": "For this dataset, I want to look at a dataset of phishing and non-phishing emails. I want to be able to determine when whether or not an email is a phishing email.\n\n\nLets load some intial libraries earlier on so that our code below runs smoothly\n\nlibrary(tidyverse)\nlibrary(tidytext)\nlibrary(ggplot2) \nset.seed(1)\n\n\n\n\nThis data is relatively large with about 18k rows and later on when we perform our pivot_wider(), we can’t have too many rows, otherwise our function will crash. I also want an even number of phishing and safe emails, so I am going to randomly sample 500 from each.\n\nemails &lt;- read_csv('databases/Phishing_Email.csv')\ncolnames(emails) &lt;- c(\"Id\", \"Email_Text\", \"Type\")\n\nphishing_emails &lt;- emails |&gt; \n    filter(Type == \"Phishing Email\") %&gt;%\n    sample_n(size = 500, replace = FALSE)\n\nsafe_emails &lt;- emails |&gt; \n    filter(Type == \"Safe Email\") %&gt;% \n    sample_n(size = 500, replace = FALSE)\n\nsampled_emails &lt;- bind_rows(phishing_emails, safe_emails)\n\n\n\n\nThankfully, our data has already been separated into clean columns, so all that we need to do is use the unnest_tokens function to create a token from each of our\n\nemail_words &lt;- sampled_emails |&gt;\n    unnest_tokens(input = \"Email_Text\",\n                output = \"Word\")\n\n\nemail_words$Id &lt;- as.numeric(email_words$Id)\n\nLets make our feature matrix fm1. We also want to keep track of the Id and Type so we can identify a given email and know its type.\n\nfm1 &lt;- email_words |&gt; \n    filter(!(Word %in% stop_words$word)) |&gt;\n    pivot_wider(id_cols = c(Id, Type),\n                names_from = \"Word\", \n                values_from = \"Word\",\n                values_fn = length,\n                values_fill = 0)\n\n\n\n\nIn order to explore our data a bit more, I want to use hclust in order to understand how our emails are grouped.\n\nhc1 &lt;- hclust(fm1 |&gt;\n                select(-c(Id, Type)) |&gt;\n                dist())\n\nI also want to label this diagram with the Id’s so that I can later remove outliers before we run rf_caret\n\nlibrary(dendextend)\nhc1 |&gt; \n    plot(label = fm1$Id)\n\nThere is a huge outlier ID 8074, which I will inspect below.\n\nemail_words |&gt; \n    filter(Id == 8074) |&gt;\n    count(Word) |&gt;\n    arrange(-n)\n\nInterestingly, the character ï which appears 947 times! This is also a phishing email, which is consistent with my previous notion of spam emails which consists of copious amounts of the same character.\n\n\n\nWith pca, my hope is to see specific words or phrases that point to significant trends in our corpora.\n\npca1 &lt;- prcomp(fm1 |&gt; select(-c(Id, Type)))\n\n\npca1$rotation[,1] |&gt;\n    sort(decreasing = TRUE) %&gt;%\n    .[1:5]\n\nemail_words |&gt;\n    filter(Word == \"ï\") |&gt;\n    count(Id, Type)\n\n#Email ID 8074\n\nAs our hclust proved before, Phishing Email 8074 is overwhelmingly contributing to the number of ï in our dataset and pca is backing this up by using it as a major predictor in whether or not an email is a Phishing email. We can also see many other\nWe can continue exploring more of our pca1,\n\npca1$rotation[,2] |&gt;\n    sort(decreasing = FALSE) %&gt;%\n    .[1:5]\n\nemail_words |&gt; \n    filter(Word == \"enron\") |&gt;\n    count(Id, Type, Word) |&gt;\n    arrange(-n)\n\nemail_words |&gt; \n    filter(Word == \"29\") |&gt;\n    count(Id, Type, Word) |&gt;\n    arrange(-n)\n\n# Email ID 17316\n\npca1$rotation[,3] |&gt;\n    sort(decreasing = TRUE) %&gt;%\n    .[1:5]\n\nemail_words |&gt; \n    filter(Word == \"ect\") |&gt;\n    count(Id, Type, Word) |&gt;\n    arrange(-n)\n\n#Email ID 12526\n\nThrough this pca investigation, I found two more strong candidates for outliers: Email 17316 and 12526. Let’s remove those outliers, and clean up our feature matrix. Our fm1 is quite large, so we must also remove columns of lesser importance. In this case, I chose to remove columns that had less than 10 mentions. Thus cut down a substantial amount of columns.\n\nlibrary(MASS) \noutlier_rows &lt;- c(8074, 17316, 12526)\nfm1_no_outliers &lt;- fm1[!(fm1$Id %in% outlier_rows),]\n\nnumeric_columns &lt;- fm1_no_outliers[, !names(fm1_no_outliers) %in% c(\"Id\", \"Type\")]\nword_count_col &lt;- colSums(numeric_columns) \ncolumns_to_keep &lt;- which(word_count_col &gt;= 10)\ncol_cleaned_fm1 &lt;- fm1_no_outliers[, columns_to_keep]\n\nAnd then lets get rid of all rows that no longer have anything in them.\n\nrow_sums &lt;- rowSums(col_cleaned_fm1[, !names(col_cleaned_fm1) %in% c(\"Id\", \"Type\")]) # nolint\nnon_zero_rows &lt;- row_sums != 0\nfull_cleaned_fm1 &lt;- col_cleaned_fm1[non_zero_rows, ]\n\n\n\n\nWith our cleaned fm1 we now can run a tuned supervised learning algorithm in order to predict whether an email is a Phishing Email or Safe Email.\n\nrf_caret &lt;- train(Type ~ .-Id,\n                data = full_cleaned_fm1 |&gt;\n                    na.omit(),\n                method = \"ranger\",\n                importance = \"impurity\")\n\nWow, that took a long time to run! On my machine this took 22 minutes to run and took a lot of ram. Now that we have our model, we can look at the top 30 most important words.\n\nimportance_scores &lt;- varImp(rf_caret)\n\nFrom our top 30 words, I picked out variables from the top 30 most important variables for safe emails\n\ntop_safe_words &lt;- c(\"university\", \"mailman\", \"planning\", \"research\", \"pm\", \"forwarded\")\nemail_words |&gt;\n    count(Type, Word) |&gt;\n    filter(Word %in% top_safe_words)|&gt;\n    arrange(-n)\n\nFrom our top 30 words, I picked out variables from the top 30 most important variables for phishing emails. I also added money as I had a hunch that it was more popular with phishing emails, even though it is not listed in the top 30 by rf_caret.\n\ntop_phish_words &lt;- c(\"guaranteed\", \"viagra\", \"free\", \"online\", \"remove\", \"money\")\nemail_words |&gt;\n    count(Type, Word) |&gt;\n    filter(Word %in% top_phish_words) |&gt;\n    arrange(Type != \"Phishing Email\", desc(n))\n\n\n\n\nFinally, we can plot these importances in a simple plot showing our top most important variables for predicting whether or not an email is phishing.\n\nplot(importance_scores, top = 30)\n\n\n\n\nFrom these corpora, two distinct classes emerge.\nOur Phishing Email dataset seems intent on “selling” or “offering” a product to the given email recipient. This corpus uses words like “free”, “guaranteed”, and “viagra” to high degree. From those examples, we can see Phishing emails are attempting to have you “buy into” an idea. From our PCA Analysis and Clustering Analysis, we also see patterns of heavy use of non-english characters, and repetition of characters.\nOur Safe Email dataset seems more intent on sharing of information. There is a lot of talk about elements of communication such as “forwarding”, “mailman” and “pm” (which could be the time or the messaging abbreviation) From our PCA Analysis and Clustering Analysis, the words in these datasets are a bit “plainer” to be frank. They seem to demand less urgency and attention than the Phishing Emails."
  },
  {
    "objectID": "posts/phishing-blog-post/index.html#loading-in-libraries",
    "href": "posts/phishing-blog-post/index.html#loading-in-libraries",
    "title": "Test analysis of Phishing and Non-phishing emails",
    "section": "",
    "text": "Lets load some intial libraries earlier on so that our code below runs smoothly\n\nlibrary(tidyverse)\nlibrary(tidytext)\nlibrary(ggplot2) \nset.seed(1)"
  },
  {
    "objectID": "posts/phishing-blog-post/index.html#separating-our-data",
    "href": "posts/phishing-blog-post/index.html#separating-our-data",
    "title": "Test analysis of Phishing and Non-phishing emails",
    "section": "",
    "text": "This data is relatively large with about 18k rows and later on when we perform our pivot_wider(), we can’t have too many rows, otherwise our function will crash. I also want an even number of phishing and safe emails, so I am going to randomly sample 500 from each.\n\nemails &lt;- read_csv('databases/Phishing_Email.csv')\ncolnames(emails) &lt;- c(\"Id\", \"Email_Text\", \"Type\")\n\nphishing_emails &lt;- emails |&gt; \n    filter(Type == \"Phishing Email\") %&gt;%\n    sample_n(size = 500, replace = FALSE)\n\nsafe_emails &lt;- emails |&gt; \n    filter(Type == \"Safe Email\") %&gt;% \n    sample_n(size = 500, replace = FALSE)\n\nsampled_emails &lt;- bind_rows(phishing_emails, safe_emails)"
  },
  {
    "objectID": "posts/phishing-blog-post/index.html#lets-tokenize-our-data",
    "href": "posts/phishing-blog-post/index.html#lets-tokenize-our-data",
    "title": "Test analysis of Phishing and Non-phishing emails",
    "section": "",
    "text": "Thankfully, our data has already been separated into clean columns, so all that we need to do is use the unnest_tokens function to create a token from each of our\n\nemail_words &lt;- sampled_emails |&gt;\n    unnest_tokens(input = \"Email_Text\",\n                output = \"Word\")\n\n\nemail_words$Id &lt;- as.numeric(email_words$Id)\n\nLets make our feature matrix fm1. We also want to keep track of the Id and Type so we can identify a given email and know its type.\n\nfm1 &lt;- email_words |&gt; \n    filter(!(Word %in% stop_words$word)) |&gt;\n    pivot_wider(id_cols = c(Id, Type),\n                names_from = \"Word\", \n                values_from = \"Word\",\n                values_fn = length,\n                values_fill = 0)"
  },
  {
    "objectID": "posts/phishing-blog-post/index.html#lets-cluster-with-hclust",
    "href": "posts/phishing-blog-post/index.html#lets-cluster-with-hclust",
    "title": "Test analysis of Phishing and Non-phishing emails",
    "section": "",
    "text": "In order to explore our data a bit more, I want to use hclust in order to understand how our emails are grouped.\n\nhc1 &lt;- hclust(fm1 |&gt;\n                select(-c(Id, Type)) |&gt;\n                dist())\n\nI also want to label this diagram with the Id’s so that I can later remove outliers before we run rf_caret\n\nlibrary(dendextend)\nhc1 |&gt; \n    plot(label = fm1$Id)\n\nThere is a huge outlier ID 8074, which I will inspect below.\n\nemail_words |&gt; \n    filter(Id == 8074) |&gt;\n    count(Word) |&gt;\n    arrange(-n)\n\nInterestingly, the character ï which appears 947 times! This is also a phishing email, which is consistent with my previous notion of spam emails which consists of copious amounts of the same character."
  },
  {
    "objectID": "posts/phishing-blog-post/index.html#lets-run-some-analysis-with-pca",
    "href": "posts/phishing-blog-post/index.html#lets-run-some-analysis-with-pca",
    "title": "Test analysis of Phishing and Non-phishing emails",
    "section": "",
    "text": "With pca, my hope is to see specific words or phrases that point to significant trends in our corpora.\n\npca1 &lt;- prcomp(fm1 |&gt; select(-c(Id, Type)))\n\n\npca1$rotation[,1] |&gt;\n    sort(decreasing = TRUE) %&gt;%\n    .[1:5]\n\nemail_words |&gt;\n    filter(Word == \"ï\") |&gt;\n    count(Id, Type)\n\n#Email ID 8074\n\nAs our hclust proved before, Phishing Email 8074 is overwhelmingly contributing to the number of ï in our dataset and pca is backing this up by using it as a major predictor in whether or not an email is a Phishing email. We can also see many other\nWe can continue exploring more of our pca1,\n\npca1$rotation[,2] |&gt;\n    sort(decreasing = FALSE) %&gt;%\n    .[1:5]\n\nemail_words |&gt; \n    filter(Word == \"enron\") |&gt;\n    count(Id, Type, Word) |&gt;\n    arrange(-n)\n\nemail_words |&gt; \n    filter(Word == \"29\") |&gt;\n    count(Id, Type, Word) |&gt;\n    arrange(-n)\n\n# Email ID 17316\n\npca1$rotation[,3] |&gt;\n    sort(decreasing = TRUE) %&gt;%\n    .[1:5]\n\nemail_words |&gt; \n    filter(Word == \"ect\") |&gt;\n    count(Id, Type, Word) |&gt;\n    arrange(-n)\n\n#Email ID 12526\n\nThrough this pca investigation, I found two more strong candidates for outliers: Email 17316 and 12526. Let’s remove those outliers, and clean up our feature matrix. Our fm1 is quite large, so we must also remove columns of lesser importance. In this case, I chose to remove columns that had less than 10 mentions. Thus cut down a substantial amount of columns.\n\nlibrary(MASS) \noutlier_rows &lt;- c(8074, 17316, 12526)\nfm1_no_outliers &lt;- fm1[!(fm1$Id %in% outlier_rows),]\n\nnumeric_columns &lt;- fm1_no_outliers[, !names(fm1_no_outliers) %in% c(\"Id\", \"Type\")]\nword_count_col &lt;- colSums(numeric_columns) \ncolumns_to_keep &lt;- which(word_count_col &gt;= 10)\ncol_cleaned_fm1 &lt;- fm1_no_outliers[, columns_to_keep]\n\nAnd then lets get rid of all rows that no longer have anything in them.\n\nrow_sums &lt;- rowSums(col_cleaned_fm1[, !names(col_cleaned_fm1) %in% c(\"Id\", \"Type\")]) # nolint\nnon_zero_rows &lt;- row_sums != 0\nfull_cleaned_fm1 &lt;- col_cleaned_fm1[non_zero_rows, ]"
  },
  {
    "objectID": "posts/phishing-blog-post/index.html#supervised-learning-with-ranger-randomforest",
    "href": "posts/phishing-blog-post/index.html#supervised-learning-with-ranger-randomforest",
    "title": "Test analysis of Phishing and Non-phishing emails",
    "section": "",
    "text": "With our cleaned fm1 we now can run a tuned supervised learning algorithm in order to predict whether an email is a Phishing Email or Safe Email.\n\nrf_caret &lt;- train(Type ~ .-Id,\n                data = full_cleaned_fm1 |&gt;\n                    na.omit(),\n                method = \"ranger\",\n                importance = \"impurity\")\n\nWow, that took a long time to run! On my machine this took 22 minutes to run and took a lot of ram. Now that we have our model, we can look at the top 30 most important words.\n\nimportance_scores &lt;- varImp(rf_caret)\n\nFrom our top 30 words, I picked out variables from the top 30 most important variables for safe emails\n\ntop_safe_words &lt;- c(\"university\", \"mailman\", \"planning\", \"research\", \"pm\", \"forwarded\")\nemail_words |&gt;\n    count(Type, Word) |&gt;\n    filter(Word %in% top_safe_words)|&gt;\n    arrange(-n)\n\nFrom our top 30 words, I picked out variables from the top 30 most important variables for phishing emails. I also added money as I had a hunch that it was more popular with phishing emails, even though it is not listed in the top 30 by rf_caret.\n\ntop_phish_words &lt;- c(\"guaranteed\", \"viagra\", \"free\", \"online\", \"remove\", \"money\")\nemail_words |&gt;\n    count(Type, Word) |&gt;\n    filter(Word %in% top_phish_words) |&gt;\n    arrange(Type != \"Phishing Email\", desc(n))"
  },
  {
    "objectID": "posts/phishing-blog-post/index.html#plotting",
    "href": "posts/phishing-blog-post/index.html#plotting",
    "title": "Test analysis of Phishing and Non-phishing emails",
    "section": "",
    "text": "Finally, we can plot these importances in a simple plot showing our top most important variables for predicting whether or not an email is phishing.\n\nplot(importance_scores, top = 30)"
  },
  {
    "objectID": "posts/phishing-blog-post/index.html#conclusion",
    "href": "posts/phishing-blog-post/index.html#conclusion",
    "title": "Test analysis of Phishing and Non-phishing emails",
    "section": "",
    "text": "From these corpora, two distinct classes emerge.\nOur Phishing Email dataset seems intent on “selling” or “offering” a product to the given email recipient. This corpus uses words like “free”, “guaranteed”, and “viagra” to high degree. From those examples, we can see Phishing emails are attempting to have you “buy into” an idea. From our PCA Analysis and Clustering Analysis, we also see patterns of heavy use of non-english characters, and repetition of characters.\nOur Safe Email dataset seems more intent on sharing of information. There is a lot of talk about elements of communication such as “forwarding”, “mailman” and “pm” (which could be the time or the messaging abbreviation) From our PCA Analysis and Clustering Analysis, the words in these datasets are a bit “plainer” to be frank. They seem to demand less urgency and attention than the Phishing Emails."
  }
]