[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This blog is to showcase the work I do in class as well as my thought process behind coding."
  },
  {
    "objectID": "about.html#about-this-blog",
    "href": "about.html#about-this-blog",
    "title": "About",
    "section": "",
    "text": "This blog is to showcase the work I do in class as well as my thought process behind coding."
  },
  {
    "objectID": "about.html#about-me",
    "href": "about.html#about-me",
    "title": "About",
    "section": "About me",
    "text": "About me\nI am a senior Computer Science major at Middlebury College. I have experience in Python, C, OCaml, and Java, as well as the domain-specific language “NetLogo”. I also like linguistics and art! Enjoy reading my blog and thank you for stopping by!"
  },
  {
    "objectID": "posts/WiDs-blog-post/index.html",
    "href": "posts/WiDs-blog-post/index.html",
    "title": "WiDS Conference",
    "section": "",
    "text": "On March 4th, I attended the WiDS conference here in Middlebury. The room was filled with Middlebury Students eager to hear lectures, talk to peers, and eat some delicious snacks. Looking around the room, there is a 60/40 split of women and men. To a passerby, it might seem like Computer Science is a deeply diverse. To Computer Science Majors at Middlebury, one can notice that the class is roughly a 50/50 split of men. I learned from 4 different Professors on how to use data, and what it means to be a women in Data Science. I read the report “Solving the Equation: The Variables for Women’s Success in Engineering and Computing” written by Corbett and Hill (2015) to supplement all the work I did in this essay and from there I learned that women were the original data scientists that were then systematically excluded from the same profession over-time.\n\n\nTo answer this question, let us first reflect on the history of women in Computer Science, Engineering, and Math to understand the situation women face now. Leading up to the 1950s, the “first computing programmer”, Ada lovelace figured out that one could feed a series of instructions to an theoretical machine to perform calculations. Women were among the first programmers of the ENIAC during World War 2. This led traditionally male engineering programs to start allowing the admission of women to these institutions.\n\n\n\n\n\nImage 1. A picture of Grace Hopper, the creator of the first compiler! Courtesy of Getty Images\n\n\nComputing in the 1960s was still viewed as a “low-status clerical” job, which revolved around administrative and “secretary”-style work. However, companies began to use aptitude tests that male stereotypes heavily benefited from. A strong personality began to develop around Computer Science, leading to the stereotype of “antisocial, mathematically inclined males” being the face of Computer Science. While prompting a drop in representation of women in CS, there existed another significant drop in the 1980s.\nThe personal computer became a “toy” for young men to get heavy interaction with in the home. This connected gaming culture to the home, which was a culture that traditionally excluded women. As Gamer-culture has become entwined with those those interested in Computer Science departments, it is important to notice what actually happens at these companies that revolve around gaming. What you can see can be regarded as “frat boy behavior”, in which these large video-game companies, which heavily influence those who get into computer science, are creating unsafe and harmful situations in which women who want to work in industry are forced to face. This feeds into an unfortunate positive feedback loop, where as more women feel uncomfortable and leave these positions, men then fill in the gaps. What are the reprecussions of diversity dropping in tech and related fields?\nWith the growing amount of dependency our world has on the work of tech companies, algorithms continue to be implemented to affect lives in a major way. As can be clearly inferred, missing half the population can lead to drastic consequences, and is as unsustainable as fossil fuel usage. With greater diversity of thought in the workplace, we can make better, more informed decisions. Moreover, it leads to lower quality to life for women in the workforce. Engineering and CS tend to offer job benefits that are rarer in other fields. Workplace flexibility exploded during COVID, but engineering roles have maintained that working from home is a viable path. There are reports that Job satisfaction is higher in engineering jobs, and while I believe often times applies more to men, these companies can most definitely offer this satisfaction for women in these companies, if they fix their current issues that push women away. Perhaps most importantly, the engineering and computing fields have almost entirely closed the gender pay gap, with women being paid 90 cents for every dollar made by a man.\n\n\n\nThese disciplines continue to disenfranchise women with work-life balance issues, sense of belonging, in-group favoritism, and microinequities, so is there any hope? I think that is a perfectly logical question, to which I of course say yes!! It is important to remember that people all around the world are trying to resolve this imbalance in Math, Engineering, and CS. Programs like Girls who code seek to provide opportunities for more women to get access to coding and eventually becoming engineers. Huge conferences like the Grace Hopper Conference try to connect female and non-binary students from around the world to network and connect with female professionals in the Engineering and Data Science fields. However, how does a college or university get to push women towards these disciplines? Fostering a strong bond between engineers-to-be and working professionals at the same university is where events like WiDS comes in.\n\n\n\nIt was exciting to be seated among so many people in STEM at Middlebury. The room was much more diverse than the average Middlebury classroom, and we all munched on small snacks as we waited for the presentations to begin.\n\n\n\n\n\nImage 2. A picture of Professor Amy Yuen\n\n\nThe first presenter was Professor Amy Yuen. She is a political science teacher at Middlebury that focuses her research on the intersection between game theory, large statistical (SC) and asked two questsion: Why do members continue to run for spots and is the SC democratic? Through these questions she wanted to answer whose issues are discussed on the international stage. Her research showed two key things: 1. There are Permanent Members whose main job is to fight crisis and maintain. 2. Elected Members are more likely to fight for current issues. Within these Elected Members, there are some who invest lots of money to get reelected. Ultimately, this answered that members who continue to run are trying to fight for current issues and that the SC is not democratic as votes from certain countries carry more weight i.e. Permanent Members.\n\n\n\n\n\nImage 3. A picture of Professor Sarah Brown\n\n\nThe second presenter was Professor Sarah Brown. She is a Computer Science professor at the University of Rhode Island. Her research focuses on Machine Learning as a multidisciplinary field. As a keynote speaker, Professor Brown did not fail to deliver. With a BS, MS, and PhD, she appears to be an extremely STEM and hard science person. However, her presentation sought to explain what makes up Machine learning. It is made up of three domains: Computer Science, Statistics, and Domain-Expertise. While CS students often also understand statistics, they have to reach out to domain-experts for their expertise. What Professor Brown presented was how Computer Scientists and Data Scientists can add specific “keys” to their toolbox to unlock this knowledge.\nShe presented three keys: “Contextualize Data”, “Disciplines Are Communities”, and “Meet People Where They Are”. Professor Brown explains that data is a primary source. Treat this like Social studies class and use context to supplement the trends you are seeing, and you will see underlying properties emerge. After seeing these patterns, it is important to know when to make and deploy a model. As a community and a discipline, we must define fairness, and know how that influences so that we can properly determine how to use our models. But how do we ensure that people do their due diligence and try to ensure fairness? This is where we meet people where they are. You have to show that unbiased algorithms affect all of us, and that all of us need to fight for fairness in algorithms.\n\n\n\n\n\nImage 4. A picture of Professor Jessica L’Roe\n\n\nThe third presenter was Professor Jessica L’Roe. She is a professor of Geoegraphy at Middlebury that studies the relationship with humans in forest landscape in East Africa and the Amazon. Professor L’Roe brought a refreshing look at how data can be used in non-traditional settings, and more importantly how data is collected. She presented quite a bit of work, but her research on “Landscape Changes around Kibale National Park” was particularly interesting. She harped on the importance of both qualitative and quantitative data. A piece of data that was particularly interesting to me was collecting data about what benefits people felt from the forest near them. Within her talk, her overall question appeared to be “How do you apply stats to work?” and she challenged this by showcasing her various usages of data.\n\n\n\n\n\nImage 5. A picture of Professor Laura Biester\n\n\nThe fourth presenter was Professor Laura Biester. She is a professor in our very own Computer science department at Middlebury. Her research focuses on natural language processing and computational social science. Her talk focused on a pretty impressive technological feat of searching all the comments in reddit and finding examples of people using “identity claims” of depression. These were instances where people stated that they were diagnosed with depression. The question posed was to discover if there were benefits to sharing one’s diagnosis on an anonymous forumn like Reddit. The research focused on the difference in the way they spoke before and following their identity claim. The research ultimately showed that they displayed more positive emotions after their identity claim.\n\n\n\nFrom this blog, I learned how to speak about a very real issue in Computer Science. Women continue to choose other fields not because of some arbitrary disinterest but because of a slew of behavior within the CS world that is both intentionally and unintentionally excludes women. These same causes also disproportionately underrepresented minority women to be significantly less likely to pursue fields related to engineering. I identified with some of the struggles faced by women as a Latino man interested in the CS world, particularly the inherent in-group favoritism and a sense of belonging. I also understood that there were varying degrees with which I identified with the very serious problems that women face in these disciplines. Going to the conference gave me a very unique opportunity to listen to female professionals adjacent to the Engineering disciplines clearly outline how they used data in their every day work. Following this meeting I talked to my partner a bit about this blog post and she told me possibly my biggest takeaway from the reading and essay I have synthesized. Female Professionals, regardless of the field, thread a delicate line between displaying femininity and masculinity. Particularly in the CS world, they are competing in a predominantly male field, and thus “masculine” traits are cherished. However, women are still expected to be somewhat feminine, otherwise they are also seen in a negative light. Having to experience this sort of dysphoria in the workplace was incredibly eye-opening to me, and without this blog post I might not have learned that.\nFrom the presenters, I particularly enjoyed the key-note speaker. Professor Brown did an incredible job connecting soft skills to Computer Science. I resonated heavily with the way she detailed how our past experiences in non-STEM areas should influence how we are thinking about CS. Moreover, she provided a unique viewpoint being an extremely decorated POC female professor and gave liberal arts CS majors a sense of tranquility as we apply to opportunities in the real world.\n\n\n\n\n“https://www.aauw.org/app/uploads/2020/03/Solving-the-Equation-report-nsa.pdf” by Corbett and Hill 2015\n“https://www.middlebury.edu/college/academics/computer-science/faculty-and-staff” (for photos)"
  },
  {
    "objectID": "posts/WiDs-blog-post/index.html#abstract",
    "href": "posts/WiDs-blog-post/index.html#abstract",
    "title": "WiDS Conference",
    "section": "",
    "text": "On March 4th, I attended the WiDS conference here in Middlebury. The room was filled with Middlebury Students eager to hear lectures, talk to peers, and eat some delicious snacks. Looking around the room, there is a 60/40 split of women and men. To a passerby, it might seem like Computer Science is a deeply diverse. To Computer Science Majors at Middlebury, one can notice that the class is roughly a 50/50 split of men. I learned from 4 different Professors on how to use data, and what it means to be a women in Data Science. I read the report “Solving the Equation: The Variables for Women’s Success in Engineering and Computing” written by Corbett and Hill (2015) to supplement all the work I did in this essay and from there I learned that women were the original data scientists that were then systematically excluded from the same profession over-time.\n\n\nTo answer this question, let us first reflect on the history of women in Computer Science, Engineering, and Math to understand the situation women face now. Leading up to the 1950s, the “first computing programmer”, Ada lovelace figured out that one could feed a series of instructions to an theoretical machine to perform calculations. Women were among the first programmers of the ENIAC during World War 2. This led traditionally male engineering programs to start allowing the admission of women to these institutions.\n\n\n\n\n\nImage 1. A picture of Grace Hopper, the creator of the first compiler! Courtesy of Getty Images\n\n\nComputing in the 1960s was still viewed as a “low-status clerical” job, which revolved around administrative and “secretary”-style work. However, companies began to use aptitude tests that male stereotypes heavily benefited from. A strong personality began to develop around Computer Science, leading to the stereotype of “antisocial, mathematically inclined males” being the face of Computer Science. While prompting a drop in representation of women in CS, there existed another significant drop in the 1980s.\nThe personal computer became a “toy” for young men to get heavy interaction with in the home. This connected gaming culture to the home, which was a culture that traditionally excluded women. As Gamer-culture has become entwined with those those interested in Computer Science departments, it is important to notice what actually happens at these companies that revolve around gaming. What you can see can be regarded as “frat boy behavior”, in which these large video-game companies, which heavily influence those who get into computer science, are creating unsafe and harmful situations in which women who want to work in industry are forced to face. This feeds into an unfortunate positive feedback loop, where as more women feel uncomfortable and leave these positions, men then fill in the gaps. What are the reprecussions of diversity dropping in tech and related fields?\nWith the growing amount of dependency our world has on the work of tech companies, algorithms continue to be implemented to affect lives in a major way. As can be clearly inferred, missing half the population can lead to drastic consequences, and is as unsustainable as fossil fuel usage. With greater diversity of thought in the workplace, we can make better, more informed decisions. Moreover, it leads to lower quality to life for women in the workforce. Engineering and CS tend to offer job benefits that are rarer in other fields. Workplace flexibility exploded during COVID, but engineering roles have maintained that working from home is a viable path. There are reports that Job satisfaction is higher in engineering jobs, and while I believe often times applies more to men, these companies can most definitely offer this satisfaction for women in these companies, if they fix their current issues that push women away. Perhaps most importantly, the engineering and computing fields have almost entirely closed the gender pay gap, with women being paid 90 cents for every dollar made by a man.\n\n\n\nThese disciplines continue to disenfranchise women with work-life balance issues, sense of belonging, in-group favoritism, and microinequities, so is there any hope? I think that is a perfectly logical question, to which I of course say yes!! It is important to remember that people all around the world are trying to resolve this imbalance in Math, Engineering, and CS. Programs like Girls who code seek to provide opportunities for more women to get access to coding and eventually becoming engineers. Huge conferences like the Grace Hopper Conference try to connect female and non-binary students from around the world to network and connect with female professionals in the Engineering and Data Science fields. However, how does a college or university get to push women towards these disciplines? Fostering a strong bond between engineers-to-be and working professionals at the same university is where events like WiDS comes in.\n\n\n\nIt was exciting to be seated among so many people in STEM at Middlebury. The room was much more diverse than the average Middlebury classroom, and we all munched on small snacks as we waited for the presentations to begin.\n\n\n\n\n\nImage 2. A picture of Professor Amy Yuen\n\n\nThe first presenter was Professor Amy Yuen. She is a political science teacher at Middlebury that focuses her research on the intersection between game theory, large statistical (SC) and asked two questsion: Why do members continue to run for spots and is the SC democratic? Through these questions she wanted to answer whose issues are discussed on the international stage. Her research showed two key things: 1. There are Permanent Members whose main job is to fight crisis and maintain. 2. Elected Members are more likely to fight for current issues. Within these Elected Members, there are some who invest lots of money to get reelected. Ultimately, this answered that members who continue to run are trying to fight for current issues and that the SC is not democratic as votes from certain countries carry more weight i.e. Permanent Members.\n\n\n\n\n\nImage 3. A picture of Professor Sarah Brown\n\n\nThe second presenter was Professor Sarah Brown. She is a Computer Science professor at the University of Rhode Island. Her research focuses on Machine Learning as a multidisciplinary field. As a keynote speaker, Professor Brown did not fail to deliver. With a BS, MS, and PhD, she appears to be an extremely STEM and hard science person. However, her presentation sought to explain what makes up Machine learning. It is made up of three domains: Computer Science, Statistics, and Domain-Expertise. While CS students often also understand statistics, they have to reach out to domain-experts for their expertise. What Professor Brown presented was how Computer Scientists and Data Scientists can add specific “keys” to their toolbox to unlock this knowledge.\nShe presented three keys: “Contextualize Data”, “Disciplines Are Communities”, and “Meet People Where They Are”. Professor Brown explains that data is a primary source. Treat this like Social studies class and use context to supplement the trends you are seeing, and you will see underlying properties emerge. After seeing these patterns, it is important to know when to make and deploy a model. As a community and a discipline, we must define fairness, and know how that influences so that we can properly determine how to use our models. But how do we ensure that people do their due diligence and try to ensure fairness? This is where we meet people where they are. You have to show that unbiased algorithms affect all of us, and that all of us need to fight for fairness in algorithms.\n\n\n\n\n\nImage 4. A picture of Professor Jessica L’Roe\n\n\nThe third presenter was Professor Jessica L’Roe. She is a professor of Geoegraphy at Middlebury that studies the relationship with humans in forest landscape in East Africa and the Amazon. Professor L’Roe brought a refreshing look at how data can be used in non-traditional settings, and more importantly how data is collected. She presented quite a bit of work, but her research on “Landscape Changes around Kibale National Park” was particularly interesting. She harped on the importance of both qualitative and quantitative data. A piece of data that was particularly interesting to me was collecting data about what benefits people felt from the forest near them. Within her talk, her overall question appeared to be “How do you apply stats to work?” and she challenged this by showcasing her various usages of data.\n\n\n\n\n\nImage 5. A picture of Professor Laura Biester\n\n\nThe fourth presenter was Professor Laura Biester. She is a professor in our very own Computer science department at Middlebury. Her research focuses on natural language processing and computational social science. Her talk focused on a pretty impressive technological feat of searching all the comments in reddit and finding examples of people using “identity claims” of depression. These were instances where people stated that they were diagnosed with depression. The question posed was to discover if there were benefits to sharing one’s diagnosis on an anonymous forumn like Reddit. The research focused on the difference in the way they spoke before and following their identity claim. The research ultimately showed that they displayed more positive emotions after their identity claim.\n\n\n\nFrom this blog, I learned how to speak about a very real issue in Computer Science. Women continue to choose other fields not because of some arbitrary disinterest but because of a slew of behavior within the CS world that is both intentionally and unintentionally excludes women. These same causes also disproportionately underrepresented minority women to be significantly less likely to pursue fields related to engineering. I identified with some of the struggles faced by women as a Latino man interested in the CS world, particularly the inherent in-group favoritism and a sense of belonging. I also understood that there were varying degrees with which I identified with the very serious problems that women face in these disciplines. Going to the conference gave me a very unique opportunity to listen to female professionals adjacent to the Engineering disciplines clearly outline how they used data in their every day work. Following this meeting I talked to my partner a bit about this blog post and she told me possibly my biggest takeaway from the reading and essay I have synthesized. Female Professionals, regardless of the field, thread a delicate line between displaying femininity and masculinity. Particularly in the CS world, they are competing in a predominantly male field, and thus “masculine” traits are cherished. However, women are still expected to be somewhat feminine, otherwise they are also seen in a negative light. Having to experience this sort of dysphoria in the workplace was incredibly eye-opening to me, and without this blog post I might not have learned that.\nFrom the presenters, I particularly enjoyed the key-note speaker. Professor Brown did an incredible job connecting soft skills to Computer Science. I resonated heavily with the way she detailed how our past experiences in non-STEM areas should influence how we are thinking about CS. Moreover, she provided a unique viewpoint being an extremely decorated POC female professor and gave liberal arts CS majors a sense of tranquility as we apply to opportunities in the real world.\n\n\n\n\n“https://www.aauw.org/app/uploads/2020/03/Solving-the-Equation-report-nsa.pdf” by Corbett and Hill 2015\n“https://www.middlebury.edu/college/academics/computer-science/faculty-and-staff” (for photos)"
  },
  {
    "objectID": "posts/perceptron-post/index.html",
    "href": "posts/perceptron-post/index.html",
    "title": "Perceptron Blog",
    "section": "",
    "text": "We are going to load in our algorithm from our perceptron.py!\n\nimport torch \nimport numpy as np\ntorch.manual_seed(1234)\n\ndef perceptron_data(n_points = 300, noise = 0.2, p_dims = 2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n\n    # convert y from {0, 1} to {-1, 1}\n    y = 2*y - 1\n\n    return X, y\n\nX, y = perceptron_data(n_points = 300, noise = 0.2)\n\n\n%load_ext autoreload\n%autoreload 2\n\ntorch.manual_seed(1234)\nX, y = perceptron_data(n_points = 50, noise = 0.3)\n\nfrom perceptron import Perceptron, PerceptronOptimizer\n# instantiate a model and an optimizer\np = Perceptron() \nopt = PerceptronOptimizer(p)\n\nloss = 1.0\n\n# for keeping track of loss values\nloss_vec = []\n\nn = X.size()[0]\n\nwhile loss &gt; 0: # dangerous -- only terminates if data is linearly separable\n    \n    # not part of the update: just for tracking our progress    \n    loss = p.loss(X, y) \n    loss_vec.append(loss)\n    \n    # pick a random data point\n    i = torch.randint(n, size = (1,))\n    x_i = X[[i],:]\n    y_i = y[i]\n    \n    # perform a perceptron update using the random data point\n    opt.step(x_i, y_i)\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n\n\nRuntimeError: mat1 and mat2 shapes cannot be multiplied (50x3 and 1x3)"
  },
  {
    "objectID": "posts/perceptron-post/index.html#starting",
    "href": "posts/perceptron-post/index.html#starting",
    "title": "Perceptron Blog",
    "section": "",
    "text": "We are going to load in our algorithm from our perceptron.py!\n\nimport torch \nimport numpy as np\ntorch.manual_seed(1234)\n\ndef perceptron_data(n_points = 300, noise = 0.2, p_dims = 2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n\n    # convert y from {0, 1} to {-1, 1}\n    y = 2*y - 1\n\n    return X, y\n\nX, y = perceptron_data(n_points = 300, noise = 0.2)\n\n\n%load_ext autoreload\n%autoreload 2\n\ntorch.manual_seed(1234)\nX, y = perceptron_data(n_points = 50, noise = 0.3)\n\nfrom perceptron import Perceptron, PerceptronOptimizer\n# instantiate a model and an optimizer\np = Perceptron() \nopt = PerceptronOptimizer(p)\n\nloss = 1.0\n\n# for keeping track of loss values\nloss_vec = []\n\nn = X.size()[0]\n\nwhile loss &gt; 0: # dangerous -- only terminates if data is linearly separable\n    \n    # not part of the update: just for tracking our progress    \n    loss = p.loss(X, y) \n    loss_vec.append(loss)\n    \n    # pick a random data point\n    i = torch.randint(n, size = (1,))\n    x_i = X[[i],:]\n    y_i = y[i]\n    \n    # perform a perceptron update using the random data point\n    opt.step(x_i, y_i)\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n\n\nRuntimeError: mat1 and mat2 shapes cannot be multiplied (50x3 and 1x3)"
  },
  {
    "objectID": "example_posts/new-test-post/index.html",
    "href": "example_posts/new-test-post/index.html",
    "title": "Second Post",
    "section": "",
    "text": "from source import Perceptron\np = Perceptron()\n\nI did it!!\nnot implemented\nThis is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "example_posts/new-test-post/index.html#math",
    "href": "example_posts/new-test-post/index.html#math",
    "title": "Second Post",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "WiDS Conference\n\n\n\n\n\n\ncsci415\n\n\n\nEssay on attending WiDS at Middlebury\n\n\n\n\n\nApr 3, 2024\n\n\nManuel Fors\n\n\n\n\n\n\n\n\n\n\n\n\nPerceptron Blog\n\n\n\n\n\n\ncsci415\n\n\n\nThis is an implementation of the Perceptron Algorithm\n\n\n\n\n\nMar 22, 2024\n\n\nManuel Fors\n\n\n\n\n\n\n\n\n\n\n\n\nReplication Study Blog\n\n\n\n\n\n\ncsci415\n\n\n\nThis focuses on a study done on medical algorithms that assign scores to individuals and contains a racial bias.\n\n\n\n\n\nMar 21, 2024\n\n\nManuel Fors\n\n\n\n\n\n\n\n\n\n\n\n\nPenguin Blog\n\n\n\n\n\n\ncsci415\n\n\n\nPenguin classifier\n\n\n\n\n\nFeb 16, 2024\n\n\nManuel Fors\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "example_posts/example-blog-post/index.html",
    "href": "example_posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "from source import Perceptron\np = Perceptron()\n\nI did it!!\nnot implemented\nThis is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "example_posts/example-blog-post/index.html#math",
    "href": "example_posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/penguins-blog-post/index.html",
    "href": "posts/penguins-blog-post/index.html",
    "title": "Penguin Blog",
    "section": "",
    "text": "Adelie Penguin ready to engage in his final semester"
  },
  {
    "objectID": "posts/penguins-blog-post/index.html#loading-in-data",
    "href": "posts/penguins-blog-post/index.html#loading-in-data",
    "title": "Penguin Blog",
    "section": "Loading in Data",
    "text": "Loading in Data\nWe are going to load in our training data for this project from a github url\n\nimport pandas as pd \ntrain_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\n\nNow we are going to take a look at our data and look for some potentially interesting features. My goal hear is to do some exploration through graphs and a bit of intuition to see if there are any key features that stick out to me.\n\ntrain.head()\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0809\n31\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN63A1\nYes\n11/24/08\n40.9\n16.6\n187.0\n3200.0\nFEMALE\n9.08458\n-24.54903\nNaN\n\n\n1\nPAL0809\n41\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN74A1\nYes\n11/24/08\n49.0\n19.5\n210.0\n3950.0\nMALE\n9.53262\n-24.66867\nNaN\n\n\n2\nPAL0708\n4\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN32A2\nYes\n11/27/07\n50.0\n15.2\n218.0\n5700.0\nMALE\n8.25540\n-25.40075\nNaN\n\n\n3\nPAL0708\n15\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN38A1\nYes\n12/3/07\n45.8\n14.6\n210.0\n4200.0\nFEMALE\n7.79958\n-25.62618\nNaN\n\n\n4\nPAL0809\n34\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN65A2\nYes\n11/24/08\n51.0\n18.8\n203.0\n4100.0\nMALE\n9.23196\n-24.17282\nNaN\n\n\n\n\n\n\n\nFollowing this, I made a rather arbitrary guess to take a look at the culmen depth and culmen length. I combined this with a hue to see the species as well as whether clutches were completed just to have a large range of variables potentially explored by this graph.\n\nfrom matplotlib import pyplot as plt \nimport seaborn as sns \nsns.set_theme(palette=\"bright\")\n\n#sns.scatterplot(data=train, x=\"Flipper Length (mm)\", y=\"Body Mass (g)\", hue=\"Culmen Depth (mm)\", style=\"Clutch Completion\")\n\nsns.scatterplot(data=train, x=\"Culmen Depth (mm)\", y=\"Culmen Length (mm)\", hue=\"Species\", style=\"Clutch Completion\").set_title(\"Culmen Length and Culmen Depth of Chinstrap, Gentoo, and Adelie Penguins\")\n\nText(0.5, 1.0, 'Culmen Length and Culmen Depth of Chinstrap, Gentoo, and Adelie Penguins')\n\n\n\n\n\n\n\n\n\nFollowing this, I created a graph that showed the variation in Delta 13 temperature across the three different islands. I had an intuition that Islands might be used in the key features for my function and thuse wanted to explore this graphically.\n\nsns.displot(train, x=\"Delta 13 C (o/oo)\", col=\"Island\", binwidth=.5, height=3)\n\n/home/manny/anaconda3/envs/ml-0451/lib/python3.11/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n\n\n\n\n\n\n\n\n\n\nbeautiful_table = train.groupby([\"Species\", \"Island\"]).aggregate({\"Body Mass (g)\" : ('mean') ,\"Culmen Depth (mm)\" : ('mean'), \"Flipper Length (mm)\": ('mean'), \"Delta 15 N (o/oo)\" :('mean')})\nbeautiful_table\n\n\n\n\n\n\n\n\n\nBody Mass (g)\nCulmen Depth (mm)\nFlipper Length (mm)\nDelta 15 N (o/oo)\n\n\nSpecies\nIsland\n\n\n\n\n\n\n\n\nAdelie Penguin (Pygoscelis adeliae)\nBiscoe\n3711.363636\n18.475758\n188.636364\n8.788643\n\n\nDream\n3728.888889\n18.306667\n190.133333\n8.933945\n\n\nTorgersen\n3712.804878\n18.468293\n191.195122\n8.846768\n\n\nChinstrap penguin (Pygoscelis antarctica)\nDream\n3743.421053\n18.366667\n196.000000\n9.331004\n\n\nGentoo penguin (Pygoscelis papua)\nBiscoe\n5039.948454\n14.914433\n216.752577\n8.247341\n\n\n\n\n\n\n\nAfter exploration, I can see that average Flipper Length and Delta 15 N differentiate between the species. I can also see how both the Chinstrap and Gentoo penguin only appear on one island each. While I believe these features might be key features, I am going to take a brute-force approach at finding the correct features by trying every possible combination of approaches."
  },
  {
    "objectID": "posts/penguins-blog-post/index.html#organizing-data",
    "href": "posts/penguins-blog-post/index.html#organizing-data",
    "title": "Penguin Blog",
    "section": "Organizing Data",
    "text": "Organizing Data\nFollowing this graphical exploration, I need to use the LabelEncoder. The LabelEncoder is going to set increasingly large numbers starting from 0 to replace each label. For all other qualitative columns, they are are turned into 1’s and 0’s so that we can run our models on it.\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df) #here is where we set all non-integer values to integers\n  return df, y\n\nX_train, y_train = prepare_data(train)"
  },
  {
    "objectID": "posts/penguins-blog-post/index.html#lets-use-a-more-comprehensive-search-for-choosing-features",
    "href": "posts/penguins-blog-post/index.html#lets-use-a-more-comprehensive-search-for-choosing-features",
    "title": "Penguin Blog",
    "section": "Lets use a more comprehensive search for choosing features!",
    "text": "Lets use a more comprehensive search for choosing features!\nBecause our dataset is relatively smaller, we can use an exhaustive search for all the features contained in the dataset. This means we are going to look at every combination of qualitative columns with quantitative columns and then create a model on this. Following this we calculate an estimated score which we add to a dictionary of ratings.\n\nfrom itertools import combinations\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nimport warnings\nwarnings.filterwarnings('ignore')\n\nLR = LogisticRegression()\n\n# these are not actually all the columns: you'll \n# need to add any of the other ones you want to search for\nall_qual_cols = [\"Clutch Completion\", \"Sex\", \"Island\"]\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)','Body Mass (g)', 'Delta 15 N (o/oo)', 'Delta 13 C (o/oo)']\nratings = {}\nfor qual in all_qual_cols: \n  qual_cols = [col for col in X_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 2):\n    # you could train models and score them here, keeping the list of \n    cols = qual_cols + list(pair) \n    LR.fit(X_train[cols], y_train)\n    #cv_scores_LR = cross_val_score(LR, X_train, y_train, cv=3)\n\n    estimated_score = LR.score(X_train[cols], y_train) \n    #estimated_score = cv_scores_LR.mean()\n    ratings[\" \".join(cols)] = estimated_score \n\nWe now have a dictionary of items that should tell us our highest scored combination\n\ntop_result = max(ratings, key=ratings.get)\ntop_result \n\n'Sex_FEMALE Sex_MALE Culmen Length (mm) Culmen Depth (mm)'\n\n\nWe are going to move our qualitative values to the end of our list because of our decision regions function we have later on\n\n#top_result= [\"Culmen Length (mm)\", \"Culmen Depth (mm)\", \"Clutch Completion_No\", \"Clutch Completion_Yes\"]\ntop_result= [\"Culmen Length (mm)\", \"Culmen Depth (mm)\", \"Sex_FEMALE\", \"Sex_MALE\"]\n\n\nLR.fit(X_train[top_result], y_train)\nLR.score(X_train[top_result], y_train)\n\n0.99609375"
  },
  {
    "objectID": "posts/penguins-blog-post/index.html#plotting-decisions-regions",
    "href": "posts/penguins-blog-post/index.html#plotting-decisions-regions",
    "title": "Penguin Blog",
    "section": "Plotting Decisions Regions",
    "text": "Plotting Decisions Regions\nWow! Our rating is pretty high. Lets visualize what our model is doing by plotting Decisions Regions\n\nfrom matplotlib import pyplot as plt\nimport numpy as np\n\n\n\nfrom matplotlib.patches import Patch\n\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (7, 3))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1], \n            title = qual_features[i])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n      plt.tight_layout()\n\n\nprint(X_train[top_result], y_train)\n\nplot_regions(LR, X_train[top_result], y_train)\n\n     Culmen Length (mm)  Culmen Depth (mm)  Sex_FEMALE  Sex_MALE\n0                  40.9               16.6        True     False\n1                  49.0               19.5       False      True\n2                  50.0               15.2       False      True\n3                  45.8               14.6        True     False\n4                  51.0               18.8       False      True\n..                  ...                ...         ...       ...\n270                51.1               16.5       False      True\n271                35.9               16.6        True     False\n272                39.5               17.8        True     False\n273                36.7               19.3        True     False\n274                42.4               17.3        True     False\n\n[256 rows x 4 columns] [1 1 2 2 1 0 0 1 2 1 0 1 0 1 1 2 0 2 2 2 2 0 0 0 2 1 0 0 0 0 0 0 1 2 0 0 2\n 2 1 1 2 2 1 0 0 2 2 1 2 2 1 2 0 0 2 2 0 1 2 2 1 2 1 2 2 2 0 0 0 2 2 2 0 1\n 2 2 2 0 0 2 0 0 2 0 0 0 1 0 0 1 0 0 0 1 0 0 2 2 0 0 2 0 2 1 0 2 2 1 2 2 2\n 0 2 0 0 0 1 0 2 2 0 2 2 1 2 0 0 1 2 2 1 0 2 0 1 2 0 0 2 0 2 1 0 0 2 1 0 2\n 0 2 0 1 0 0 0 2 2 2 0 0 2 0 2 1 1 0 1 2 0 0 1 1 0 0 1 0 0 2 1 2 1 0 0 0 1\n 0 2 2 2 2 1 1 1 2 2 2 0 1 0 0 1 0 0 0 0 0 1 2 2 2 0 2 2 1 0 2 0 0 2 0 2 0\n 2 0 2 2 2 2 0 2 1 0 2 1 1 0 2 1 0 0 0 1 0 1 0 0 2 1 0 0 0 2 0 0 0 1]"
  },
  {
    "objectID": "posts/penguins-blog-post/index.html#cross-validation",
    "href": "posts/penguins-blog-post/index.html#cross-validation",
    "title": "Penguin Blog",
    "section": "Cross Validation",
    "text": "Cross Validation\nWe want to calculate how our model will do on unseen data, and a useful way to simulate this is by performing cross-validation. We can take parts of the train data from being used, and then testing on those witheld parts, we can determine our accuracy.\n\nfrom sklearn.model_selection import cross_val_score\n\ncv_scores_LR = cross_val_score(LR, X_train, y_train, cv=5)\ncv_scores_LR.mean()\n\n1.0"
  },
  {
    "objectID": "posts/penguins-blog-post/index.html#we-did-it",
    "href": "posts/penguins-blog-post/index.html#we-did-it",
    "title": "Penguin Blog",
    "section": "We did it!",
    "text": "We did it!\n\nNext Steps\nThis would involve testing other models such as Decision Classfier Trees, and we could run the cross validation step within our brute-force attempt instead of after."
  },
  {
    "objectID": "posts/replication-blog-post/index.html#modelingggggg",
    "href": "posts/replication-blog-post/index.html#modelingggggg",
    "title": "Replication Study Blog",
    "section": "MODELINGGGGGG",
    "text": "MODELINGGGGGG\nHere we go! Its important to know what we are modeling here. We are going to attempt to fit a linear regression model with our features. Our model can be described mathematically by this description \\[ \\text{log cost} \\approx w_b \\times \\text{(patient is Black)} \\sum_{i+0}^kw_k \\times \\text{(gagne sum)}^k\\]\nWith our model, we are going to estimate what \\(w_b\\) is, which is how much cost Black Patients incur as a percentage of White Patients. However, we do not know how many polynomial featuers we should use! Don’t worry, we have a plan!\nWe are going to use a function called add_polynomial_featuers to keep adding new columns in our dataframe until we find a successful number of polynomial features.\n\ndef add_polynomial_features(X, degree):\n  X_ = X.copy()\n  for j in range(1, degree):\n    X_[f\"poly_{j}\"] = X_[\"gagne_sum_t\"]**j\n\n  return X_\n\nHere is the meat of our code where we are going to fit our model and cross-validate our scores. We are also going to test all values from 1-10 as dictated by our “degrees” variable\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LinearRegression\nratings = {}\ndegrees = 10\nLR = LinearRegression(fit_intercept = True)\n#run our polynomial features\nnew_X = add_polynomial_features(non_zero_costs, degrees)\nfor i in range(1, degrees):\n    \n    predictor_variables=[\"is_black\", f\"poly_{i}\", \"gagne_sum_t\"]\n    LR.fit(new_X[predictor_variables], new_X[\"log_costs\"])\n    cv_scores_LR = cross_val_score(LR, new_X[predictor_variables], new_X[target_variable], cv=3)\n    estimated_score = cv_scores_LR.mean()\n    ratings[i] = estimated_score\n\nHere we are going to find our top result, which comes out to be 2. Then we create a new model basedon that value and we fit it.\n\n#Here we can take a look at the \n\ntop_result = max(ratings, key=ratings.get)\nnew_predictor_values = [\"is_black\", f\"poly_{top_result}\", \"gagne_sum_t\"]\n#Following our top result, lets now make a model using this\nLR = LinearRegression(fit_intercept = True)\nLR.fit(new_X[predictor_variables], new_X[\"log_costs\"])\n\ntop_result\n\n2\n\n\nFrom here, we now want to compute our \\(e^{w_b}\\) and we access our \\(w_b\\) by looking at our LR.coef. Since it is supposed to be the same order as the variables we put in, we are going to look at the first value.\n\nimport math \nmath.e**LR.coef_[0] \n\n0.7693199308357737\n\n\nThis value shows that the cost incurred by Black patients as a percentage of White patients is about 77%. In more understandable terms, this means that Black patients only spend 77% of what White patients spend. Thats only 77 cents spent by Black Patients for every dollar that White patients are spending!\nThis connects directly both with the graphs previously created in this document as well as a key component of the main argument of Obermeyer et al.(2019), which states that Black patients incur less costs than equally sick White patients."
  },
  {
    "objectID": "posts/replication-blog-post/index.html#references",
    "href": "posts/replication-blog-post/index.html#references",
    "title": "Replication Study Blog",
    "section": "References",
    "text": "References\nBarocas, Solon, Moritz Hardt, and Arvind Narayanan. 2023. Fairness and Machine Learning: Limitations and Opportunities. Cambridge, Massachusetts: The MIT Press.\nObermeyer, Ziad, Brian Powers, Christine Vogeli, and Sendhil Mullainathan. 2019. “Dissecting Racial Bias in an Algorithm Used to Manage the Health of Populations.” Science 366 (6464): 447–53. https://doi.org/10.1126/science.aax2342."
  }
]