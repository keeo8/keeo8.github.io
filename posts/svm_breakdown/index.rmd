---
title: "Explaining Support Vector Machines"
author: "Manuel Fors"
date: '2024-05-28'
categories: [R]
description: "I give a break introduction of how Support Vector Machines (SVM) can be used in machine learning"
output: 
  html_document: default
---


# Determining Fraudulent Dollar Bills 
As I handle less and less cash in my day to day life, I have been wondering how many fake bills I have potentially held over my lifetime. 
Would I have been able to tell what makes them different? Surely if this cash had the wrong president on the front or a weird font I might 
have been able to tell, but are there more minute distinctions between really good forgeries and normal bills? 
Today, I will be analyzing the various measurements of a bill: height, length, and margins. Through these variables, I am going to determine 
whether or not a dollar is real fake. In order to accomplish this, I will use Support Vector Machines and the dataset of [fake and real money](https://www.kaggle.com/datasets/alexandrepetit881234/fake-bills)

## Algorithm of Choice: Support Vector Machines
Support Vector Machines (SVM) is an algorithm which can be used for both regression and classification. This approach divides points into classes using a *hyperplane*. 
This line is calculated based on how far away points would lie from the line. On both sides of the line, the distance of each point is maximized. From there, we arrive to
the term "Support Vector". Support Vectors are the closest points to the hyperplane, and their distances are maximized. 


When trying to separate the data into two classes, one might think that SVM will fail when there are outliers in the data. 
However, a built-in feature in SVM is a "slack variable". This is the distance between where the outlier lies and where it should be. 
From this, SVM can safely ignore the outlier, creating its line (hyperplane). 

Data must separable by a line for vanilla SVM to work. However, if it is not linearly separable, we can still do SVM. In order to do so, 
we would add an extra dimension to our data via some function.  

## Why SVM? 
That dataset has two defined classes: real and fake. This means that with supervised learning, we can train a model to make predictions
using our classes as labels. I chose SVM because of its strength in classification problems as well as its ability to limit outliers. 
With a dataset as small as a few thousand rows, this means we avoid overfitting to some observed noise or outliers.


## Methods
We are going to clean up our data, graphically explore our data, divide into a training and test split, train our SVM model, 
and then finally, make predictions and gauge our accuracy. 



## Code 
```{r setup, include=FALSE}
set.seed(31445)
knitr::opts_chunk$set(echo = TRUE, cache = TRUE)
```


### Load in dependencies and dataset
```{r}
library(caret)
library(tidyverse)
library(e1071)
library(ggplot2)

bills <- read_csv2("databases/fake_bills.csv")
```

An interesting graph emerges:
```{r}
bills |>
  ggplot() + 
  ggtitle("Relationship between Margin Heights and class of Bill") +
  geom_point(aes(x = margin_up, y = margin_low, color = is_genuine))

```
Each of these clusters of data appears separable by a line!


### Preprocess 
```{r}
clean_bills <- bills |>
                drop_na()
table(clean_bills$is_genuine)
```


For the variable we are classifying, `is_genuine`, I noticed that there are only 492 false bills, but 971 true bills. Thus, I select a random 500 true bills so that
there is the same amount for both. I was able to find this out because of the `table()` function.

I also decide to organize our data into a 80/20 training-test split.

```{r}

# Organize data into subsets of true and false
real_bills <- clean_bills |> 
    filter(is_genuine == TRUE) %>%
    sample_n(size = 500, replace = FALSE)

fake_bills <- bills |>
    filter(is_genuine == FALSE)   

# Create test data 
train_fake_bills <- fake_bills |> 
    sample_n(size = 393, replace = FALSE)

train_real_bills <- real_bills |> 
    sample_n(size = 393, replace = FALSE)

# Create training data
test_real_bills <- anti_join(real_bills, train_real_bills)
test_fake_bills <- anti_join(fake_bills, train_fake_bills)

# Combine our sets to make the full sets of each
train_set <- bind_rows(train_fake_bills, train_real_bills)
test_set <- bind_rows(test_fake_bills, test_real_bills)

#Drop na values 
train_set <- train_set |> 
            drop_na()

test_set <- test_set |> 
            drop_na()
```
### Creating our model
With our training and testing sets created, we can now start using SVM. 
The most common package is [`e1071`](https://cran.r-project.org/web/packages/e1071/e1071.pdf) which comes with a host of other useful functions.
We begin with training our svm model on our data.

```{r}
model <- svm(is_genuine ~ .,
             data = train_set, kernel = "linear", cost = .1, scale = TRUE, 
             type = "C-classification")
```

Let's break down this function:

* `kernel` This decides how the data should be separated. If there are linear patterns, use linear; if there are polynomial pattnerns, 
use polynomial. There are also other kernels such as `radial bias` and `sigmoid` with their own specific use functions. 

* `cost`: This determines how large the margin is from our hyperplane we make, to the the support vectors. The larger the cost, the smaller the margin allowed. 
The smaller the C, the larger the margin is. 

* `type`: Because SVM can be used for regression and classification, we specify that we want to use C-classification. This classification itself is the default 
setting in R. 


### Predict with our model 
After training, it is now finally time to predict on our test_set 

```{r}
  y_pred <- predict(model, test_set)
  confusionMatrix(table(y_pred, test_set$is_genuine)) 
```

Our accuracy is about 84% which is quite good! We also see our true positive were 78% correct and that our true negatives were 92%. 


### Interpreting accuracy 
Depending on our usecase, this accuracy could be quite good. For example, if a cashier has to scan cash to check for forgery every time they are handed money, 
it would beneficial (and hopefully a good outlook on humans) to calculate with high accuracy that a dollar is not a fake. However, with a true positive of 78%, 
this could conversely negatively affect those dealing with money, as 22% of the time, the model would be unfairly assuming that the money is fake. 